{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15b66f1a-394d-4b5b-ab06-ea2774295e3c",
   "metadata": {},
   "source": [
    "# U-Net 4 Multiple Sclerosis Lesion Segmentation - ICPR Challenge\n",
    "### Authors: Andrew R. Darnall, Giovanni Spadaro @ UniCT\n",
    "---\n",
    "\n",
    "## üéØ Competition Objective: MS Lesion Segmentation\n",
    "\n",
    "The central goal of this competition is the **automatic segmentation of Multiple Sclerosis (MS) lesions** using **multi-modal MRI data** and **deep learning algorithms**.\n",
    "\n",
    "### üß™ Provided Data\n",
    "Participants were given:\n",
    "- **MRI scans** in three modalities:\n",
    "  - **FLAIR**\n",
    "  - **T1-weighted (T1-w)**\n",
    "  - **T2-weighted (T2-w)**\n",
    "- **Ground-truth segmentation masks**, which are:\n",
    "  - **Binary masks**:  \n",
    "    - **White pixels** ‚Üí MS lesion regions  \n",
    "    - **Black pixels** ‚Üí Background\n",
    "\n",
    "### üß† Task Description\n",
    "- Participants could use **any or all modalities**, along with the ground-truth labels, to:\n",
    "  - Develop **deep learning-based models** for **automatic lesion segmentation**\n",
    "- MS lesions appear as **irregular clusters of pixels** with **high variability in size and shape**\n",
    "- These lesions are often **difficult to detect** via visual inspection, requiring **expert-level interpretation**\n",
    "\n",
    "The ultimate goal is to create **fully automated segmentation pipelines** that can robustly identify and delineate MS lesions from raw MRI data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919db79e-a3bd-4896-89a1-1d9e0017be10",
   "metadata": {},
   "source": [
    "## üß† MSLesSeg Dataset Overview\n",
    "\n",
    "As part of this competition, participants were provided with the **MSLesSeg Dataset** ‚Äî a **comprehensively annotated, multi-modal MRI dataset** designed for advancing **lesion segmentation** research in medical imaging.\n",
    "\n",
    "### üìä Dataset Composition\n",
    "- **Total Patients:** 75 (48 women, 27 men)  \n",
    "- **Age Range:** 18‚Äì59 years (Mean: 37 ¬± 10.3 years)  \n",
    "- **Longitudinal Timepoints:**  \n",
    "  - 50 patients with 1 timepoint  \n",
    "  - 15 patients with 2 timepoints  \n",
    "  - 5 patients with 3 timepoints  \n",
    "  - 5 patients with 4 timepoints  \n",
    "- **Time Interval Between Scans:** ~1.27 ¬± 0.62 years  \n",
    "- **Total MRI Series:** 115\n",
    "\n",
    "### üß¨ Imaging Modalities\n",
    "Each timepoint includes **three core MRI modalities**:\n",
    "- **T1-weighted (T1-w)**\n",
    "- **T2-weighted (T2-w)**\n",
    "- **FLAIR (Fluid-Attenuated Inversion Recovery)**\n",
    "\n",
    "### üßë‚Äç‚öïÔ∏è Expert Annotation\n",
    "- Lesions were **manually annotated** by clinical experts.\n",
    "- **FLAIR sequences** were the primary reference for lesion labeling.\n",
    "- **T1-w and T2-w** scans supported **multi-contrast lesion characterization**.\n",
    "\n",
    "### üß™ Dataset Splits\n",
    "- **Training Set:** 53 scans  \n",
    "- **Test Set:** 22 scans  \n",
    "\n",
    "### ‚úÖ Ethical Compliance\n",
    "- **Ethical approval** was obtained from the corresponding Hospital Ethics Committee.\n",
    "- **Informed consent** was acquired from all participating patients.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b3a0a-2f43-44f9-a898-28f1fa46b75a",
   "metadata": {},
   "source": [
    "# The Experiment\n",
    "\n",
    "Below is the code used for the:\n",
    "\n",
    "1) Preprocessing of the ***Brain MRI*** scans\n",
    "2) Definition of Dataset, Dataloader and LihgtningDataModule classes\n",
    "3) ***U-Net*** architecture\n",
    "4) ***PyTorch Lightning*** Trainer\n",
    "5) Training & Evaluation\n",
    "6) Model Exaplainability with the post-hoc method ***GradCam++***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13613113-a191-4399-9b42-9f6df49bc6e9",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Preprocessing & Annotation Workflow\n",
    "\n",
    "The MSLesSeg dataset underwent a **comprehensive preprocessing pipeline** and **expert-driven manual annotation** to ensure **standardization** and **label quality** for downstream MS lesion segmentation tasks.\n",
    "\n",
    "### üßº Preprocessing Pipeline\n",
    "1. **Anonymization** of all MRI scans to protect patient privacy.\n",
    "2. **DICOM to NIfTI conversion**, leveraging NIfTI's wide adoption in neuroimaging.\n",
    "3. **Co-registration to the MNI152 1mm¬≥ isotropic template** using **FLIRT** (FMRIB‚Äôs Linear Image Registration Tool), ensuring all scans are aligned to a **common anatomical space**.\n",
    "4. **Brain extraction** via **BET** (Brain Extraction Tool) to remove non-brain tissues and isolate relevant structures.\n",
    "\n",
    "This pipeline guarantees that all images are **standardized** and **aligned**, which is critical for **automated MS lesion segmentation algorithms**.\n",
    "\n",
    "---\n",
    "\n",
    "### üñãÔ∏è Ground-Truth Annotation Protocol\n",
    "- Lesions were **manually segmented** on the **FLAIR modality** for each patient and timepoint.\n",
    "- **T1-w and T2-w** modalities were used to **cross-validate ambiguous cases**.\n",
    "- Annotation was conducted by a **trained junior rater**, under supervision of:\n",
    "  - A **senior neuroradiologist**\n",
    "  - A **senior neurologist**\n",
    "- Annotation sessions included:\n",
    "  - Multiple **training meetings** to establish a **consistent segmentation strategy**\n",
    "  - Use of **JIM9** ‚Äî a high-end tool for **medical image segmentation and analysis**\n",
    "  - Regular **expert validation checkpoints** to ensure consistency and accuracy\n",
    "\n",
    "The final masks, reviewed and approved by senior experts, are considered the **gold-standard ground truth**.\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ Key Annotation Highlights\n",
    "- **Independent segmentation** for each patient/timepoint to avoid bias\n",
    "- Conducted on **FLAIR scans registered to MNI space**\n",
    "- **Validated ground-truth masks** ready for training and evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293bc032-fddb-44a7-8db3-cd8bdf0315e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from skimage.transform import resize\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2027badd-35e2-42ac-b423-e85e9e99d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load the nifti files\n",
    "def load_nifti(file_path):\n",
    "    return nib.load(file_path).get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3515529-ff9f-4f47-935b-e09a357f22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_case(input_dir, output_dir, case_id):\n",
    "    flair = load_nifti(input_dir / f\"{case_id}_flair.nii.gz\")\n",
    "    t1 = load_nifti(input_dir / f\"{case_id}_t1.nii.gz\")\n",
    "    t2 = load_nifti(input_dir / f\"{case_id}_t2.nii.gz\")\n",
    "    seg = load_nifti(input_dir / f\"{case_id}_seg.nii.gz\").astype(np.uint8)\n",
    "\n",
    "    # Stack input modalities into a tensor (C, D, H, W)\n",
    "    stacked = np.stack([flair, t1, t2], axis=0)\n",
    "    # Use CPU Tensors instead\n",
    "    input_tensor = torch.tensor(stacked, dtype=torch.float32)\n",
    "    \n",
    "    # Load it back into a PyTorch Tensor for storing\n",
    "    seg_tensor = torch.tensor(seg, dtype=torch.uint8)\n",
    "    # Add the batch dimension in order to make it compatible with the other Tensor sizes\n",
    "    seg_tensor = seg_tensor.unsqueeze(0)\n",
    "\n",
    "    \n",
    "    # Save all\n",
    "    output_case_dir = output_dir / case_id\n",
    "    output_case_dir.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(input_tensor, output_case_dir / \"input_tensor.pt\")\n",
    "    torch.save(seg_tensor, output_case_dir / \"seg_mask.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac6e5ec-ad39-42e9-8a62-c1140398e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_preprocessing(root_path, output_path):\n",
    "    input_path = Path(root_path)\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    # Get all case directories\n",
    "    all_case_dirs = [d for d in input_path.iterdir() if d.is_dir()]\n",
    "    print(f\"Found {len(all_case_dirs)} cases.\")\n",
    "\n",
    "    for case_dir in tqdm(all_case_dirs):\n",
    "        case_id = case_dir.name  # e.g., MSLS_000\n",
    "        output_case_path = output_path / case_id  # Define output path for each case\n",
    "\n",
    "        # Check if the case has already been processed (e.g., output directory or file exists)\n",
    "        if output_case_path.exists():\n",
    "            print(f\"‚úÖ Skipping {case_id}, already processed.\")\n",
    "            continue  # Skip processing if the case already exists\n",
    "\n",
    "        try:\n",
    "            preprocess_case(case_dir, output_path, case_id)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed on {case_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e6a0eb7-7da6-400d-9446-97773e286c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 93 cases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:00<00:00, 22907.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Skipping MSLS_019, already processed.\n",
      "‚úÖ Skipping MSLS_039, already processed.\n",
      "‚úÖ Skipping MSLS_059, already processed.\n",
      "‚úÖ Skipping MSLS_000, already processed.\n",
      "‚úÖ Skipping MSLS_001, already processed.\n",
      "‚úÖ Skipping MSLS_002, already processed.\n",
      "‚úÖ Skipping MSLS_003, already processed.\n",
      "‚úÖ Skipping MSLS_004, already processed.\n",
      "‚úÖ Skipping MSLS_005, already processed.\n",
      "‚úÖ Skipping MSLS_006, already processed.\n",
      "‚úÖ Skipping MSLS_007, already processed.\n",
      "‚úÖ Skipping MSLS_008, already processed.\n",
      "‚úÖ Skipping MSLS_009, already processed.\n",
      "‚úÖ Skipping MSLS_010, already processed.\n",
      "‚úÖ Skipping MSLS_011, already processed.\n",
      "‚úÖ Skipping MSLS_012, already processed.\n",
      "‚úÖ Skipping MSLS_013, already processed.\n",
      "‚úÖ Skipping MSLS_014, already processed.\n",
      "‚úÖ Skipping MSLS_015, already processed.\n",
      "‚úÖ Skipping MSLS_016, already processed.\n",
      "‚úÖ Skipping MSLS_017, already processed.\n",
      "‚úÖ Skipping MSLS_018, already processed.\n",
      "‚úÖ Skipping MSLS_020, already processed.\n",
      "‚úÖ Skipping MSLS_021, already processed.\n",
      "‚úÖ Skipping MSLS_022, already processed.\n",
      "‚úÖ Skipping MSLS_023, already processed.\n",
      "‚úÖ Skipping MSLS_024, already processed.\n",
      "‚úÖ Skipping MSLS_025, already processed.\n",
      "‚úÖ Skipping MSLS_026, already processed.\n",
      "‚úÖ Skipping MSLS_027, already processed.\n",
      "‚úÖ Skipping MSLS_028, already processed.\n",
      "‚úÖ Skipping MSLS_029, already processed.\n",
      "‚úÖ Skipping MSLS_030, already processed.\n",
      "‚úÖ Skipping MSLS_031, already processed.\n",
      "‚úÖ Skipping MSLS_032, already processed.\n",
      "‚úÖ Skipping MSLS_033, already processed.\n",
      "‚úÖ Skipping MSLS_034, already processed.\n",
      "‚úÖ Skipping MSLS_035, already processed.\n",
      "‚úÖ Skipping MSLS_036, already processed.\n",
      "‚úÖ Skipping MSLS_037, already processed.\n",
      "‚úÖ Skipping MSLS_038, already processed.\n",
      "‚úÖ Skipping MSLS_040, already processed.\n",
      "‚úÖ Skipping MSLS_041, already processed.\n",
      "‚úÖ Skipping MSLS_042, already processed.\n",
      "‚úÖ Skipping MSLS_043, already processed.\n",
      "‚úÖ Skipping MSLS_044, already processed.\n",
      "‚úÖ Skipping MSLS_045, already processed.\n",
      "‚úÖ Skipping MSLS_046, already processed.\n",
      "‚úÖ Skipping MSLS_047, already processed.\n",
      "‚úÖ Skipping MSLS_048, already processed.\n",
      "‚úÖ Skipping MSLS_049, already processed.\n",
      "‚úÖ Skipping MSLS_050, already processed.\n",
      "‚úÖ Skipping MSLS_051, already processed.\n",
      "‚úÖ Skipping MSLS_052, already processed.\n",
      "‚úÖ Skipping MSLS_053, already processed.\n",
      "‚úÖ Skipping MSLS_054, already processed.\n",
      "‚úÖ Skipping MSLS_055, already processed.\n",
      "‚úÖ Skipping MSLS_056, already processed.\n",
      "‚úÖ Skipping MSLS_057, already processed.\n",
      "‚úÖ Skipping MSLS_058, already processed.\n",
      "‚úÖ Skipping MSLS_060, already processed.\n",
      "‚úÖ Skipping MSLS_061, already processed.\n",
      "‚úÖ Skipping MSLS_062, already processed.\n",
      "‚úÖ Skipping MSLS_063, already processed.\n",
      "‚úÖ Skipping MSLS_064, already processed.\n",
      "‚úÖ Skipping MSLS_065, already processed.\n",
      "‚úÖ Skipping MSLS_066, already processed.\n",
      "‚úÖ Skipping MSLS_067, already processed.\n",
      "‚úÖ Skipping MSLS_068, already processed.\n",
      "‚úÖ Skipping MSLS_069, already processed.\n",
      "‚úÖ Skipping MSLS_070, already processed.\n",
      "‚úÖ Skipping MSLS_071, already processed.\n",
      "‚úÖ Skipping MSLS_072, already processed.\n",
      "‚úÖ Skipping MSLS_073, already processed.\n",
      "‚úÖ Skipping MSLS_074, already processed.\n",
      "‚úÖ Skipping MSLS_075, already processed.\n",
      "‚úÖ Skipping MSLS_076, already processed.\n",
      "‚úÖ Skipping MSLS_077, already processed.\n",
      "‚úÖ Skipping MSLS_078, already processed.\n",
      "‚úÖ Skipping MSLS_079, already processed.\n",
      "‚úÖ Skipping MSLS_080, already processed.\n",
      "‚úÖ Skipping MSLS_081, already processed.\n",
      "‚úÖ Skipping MSLS_082, already processed.\n",
      "‚úÖ Skipping MSLS_083, already processed.\n",
      "‚úÖ Skipping MSLS_084, already processed.\n",
      "‚úÖ Skipping MSLS_085, already processed.\n",
      "‚úÖ Skipping MSLS_086, already processed.\n",
      "‚úÖ Skipping MSLS_087, already processed.\n",
      "‚úÖ Skipping MSLS_088, already processed.\n",
      "‚úÖ Skipping MSLS_089, already processed.\n",
      "‚úÖ Skipping MSLS_090, already processed.\n",
      "‚úÖ Skipping MSLS_091, already processed.\n",
      "‚úÖ Skipping MSLS_092, already processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the preprocessing on the training set\n",
    "RAW_DATA_PATH = \"../data/01-Pre-Processed-Data/train\"\n",
    "OUTPUT_PATH = \"../data/02-Tensor-Data/train\"\n",
    "\n",
    "run_preprocessing(RAW_DATA_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc06f78a-1ed2-4e4b-90f7-391b10748569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 cases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:00<00:00, 17273.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Skipping MSLS_093, already processed.\n",
      "‚úÖ Skipping MSLS_094, already processed.\n",
      "‚úÖ Skipping MSLS_095, already processed.\n",
      "‚úÖ Skipping MSLS_096, already processed.\n",
      "‚úÖ Skipping MSLS_097, already processed.\n",
      "‚úÖ Skipping MSLS_098, already processed.\n",
      "‚úÖ Skipping MSLS_099, already processed.\n",
      "‚úÖ Skipping MSLS_100, already processed.\n",
      "‚úÖ Skipping MSLS_101, already processed.\n",
      "‚úÖ Skipping MSLS_102, already processed.\n",
      "‚úÖ Skipping MSLS_103, already processed.\n",
      "‚úÖ Skipping MSLS_104, already processed.\n",
      "‚úÖ Skipping MSLS_105, already processed.\n",
      "‚úÖ Skipping MSLS_106, already processed.\n",
      "‚úÖ Skipping MSLS_107, already processed.\n",
      "‚úÖ Skipping MSLS_108, already processed.\n",
      "‚úÖ Skipping MSLS_109, already processed.\n",
      "‚úÖ Skipping MSLS_110, already processed.\n",
      "‚úÖ Skipping MSLS_111, already processed.\n",
      "‚úÖ Skipping MSLS_112, already processed.\n",
      "‚úÖ Skipping MSLS_113, already processed.\n",
      "‚úÖ Skipping MSLS_114, already processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the preprocessing on the test set\n",
    "RAW_DATA_PATH = \"../data/01-Pre-Processed-Data/test/test_MASK\"\n",
    "OUTPUT_PATH = \"../data/02-Tensor-Data/test\"\n",
    "\n",
    "run_preprocessing(RAW_DATA_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2f3830-e80c-47b9-9f3f-3b1268619f14",
   "metadata": {},
   "source": [
    "## Build the Dataset and Dataloaders for the MSLesSeg preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "813cbfe2-346f-48f1-b350-c0ef69e9ae0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Check the initial memory consuption (GPU) of the project\n",
    "import torch\n",
    "\n",
    "# Check initial GPU memory usage\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59da4863-42f3-4d2c-a675-c8cdc1eeeace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSLesSeg Tensor Dataset class\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MSLesSegDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.patient_dirs = self._get_patient_dirs()\n",
    "\n",
    "    def _get_patient_dirs(self):\n",
    "        \"\"\"\n",
    "        Helper function to search for all patient directories within train/test directories.\n",
    "        \"\"\"\n",
    "        patient_dirs = []\n",
    "        \n",
    "        # Get all patient directories within the root_dir (either train or test)\n",
    "        for patient_dir in os.listdir(self.root_dir):\n",
    "            patient_path = os.path.join(self.root_dir, patient_dir)\n",
    "            \n",
    "            # Make sure it's a directory\n",
    "            if os.path.isdir(patient_path):\n",
    "                # Check if both 'input_tensor.pt' and 'seg_mask.pt' exist\n",
    "                input_tensor_path = os.path.join(patient_path, 'input_tensor.pt')\n",
    "                seg_mask_path = os.path.join(patient_path, 'seg_mask.pt')\n",
    "                \n",
    "                if os.path.exists(input_tensor_path) and os.path.exists(seg_mask_path):\n",
    "                    patient_dirs.append(patient_path)\n",
    "        \n",
    "        return patient_dirs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "        \n",
    "        Returns:\n",
    "            dict: {'input': tensor, 'mask': tensor} for the requested sample.\n",
    "        \"\"\"\n",
    "        patient_dir = self.patient_dirs[idx]\n",
    "        \n",
    "        # Load the input tensor and segmentation mask\n",
    "        input_tensor = torch.load(os.path.join(patient_dir, 'input_tensor.pt'))  # Shape: [3, 182, 218, 182]\n",
    "        seg_mask = torch.load(os.path.join(patient_dir, 'seg_mask.pt'))  # Shape: [1, 182, 218, 182]\n",
    "        \n",
    "        return input_tensor, seg_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f0311-9b80-4bf0-b80d-996692a83815",
   "metadata": {},
   "source": [
    "### PyTorch Lightning DataModule\n",
    "\n",
    "This particular version of PyTorch Lightning, and in general from version 2.x onward require a ***LightningDataModule*** instead of passing the dataloaders directly to the ***.fit()*** method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03106a16-273a-4298-b82f-e15990258754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSLesSeg (PyTorch) LightningDataModule definition\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class MSLesSegDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, root_data_dir, batch_size, val_split, num_workers):\n",
    "        \"\"\"\n",
    "        root_data_dir: Is the path to the train and test data\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_dir = root_data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.val_split = val_split\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Load full training dataset\n",
    "        full_dataset = MSLesSegDataset(root_dir=os.path.join(self.data_dir, 'train'))\n",
    "\n",
    "        # Split into train and val\n",
    "        val_size = int(len(full_dataset) * self.val_split)\n",
    "        train_size = len(full_dataset) - val_size\n",
    "        self.train_dataset, self.val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "        # Load test dataset (if it exists)\n",
    "        test_dir = os.path.join(self.data_dir, 'test')\n",
    "        if os.path.exists(test_dir):\n",
    "            self.test_dataset = MSLesSegDataset(root_dir=test_dir)\n",
    "        else:\n",
    "            self.test_dataset = None\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if self.test_dataset:\n",
    "            return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b08f0-2188-400b-b30e-88997983f659",
   "metadata": {},
   "source": [
    "## The Model's Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "726efd51-8791-4abc-9204-b5571ed1a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Standard double convolution block (with GeLU instead of ReLU)\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.GELU(),\n",
    "        nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.GELU()\n",
    "    )\n",
    "\n",
    "# Final output layer: Conv3D + Sigmoid for probability map\n",
    "def final_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv3d(in_channels, out_channels, kernel_size=1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "# Center cropping\n",
    "def center_crop(tensor, target_tensor):\n",
    "    _, _, h, w, d = target_tensor.shape\n",
    "    tensor = tensor[:, :, :h, :w, :d]\n",
    "    return tensor\n",
    "\n",
    "# Pad the two tensors to match the shapes\n",
    "def pad_to_match_shape(tensor1, tensor2):\n",
    "    \"\"\"\n",
    "    Pads the smaller tensor to match the shape of the larger tensor along all spatial dimensions (H, W, D),\n",
    "    but leaves batch (B) and channel (C) dimensions untouched.\n",
    "    \n",
    "    Args:\n",
    "        tensor1 (Tensor): The first tensor.\n",
    "        tensor2 (Tensor): The second tensor.\n",
    "        \n",
    "    Returns:\n",
    "        Tensor, Tensor: Both tensors padded to the same shape.\n",
    "    \"\"\"\n",
    "    # Get the shapes of both tensors\n",
    "    shape1 = tensor1.shape\n",
    "    shape2 = tensor2.shape\n",
    "    \n",
    "    # Initialize padding lists for tensor1 and tensor2\n",
    "    padding1 = []\n",
    "    padding2 = []\n",
    "    \n",
    "    # Compare shapes and calculate the padding for each tensor\n",
    "    for dim1, dim2 in zip(shape1[2:], shape2[2:]):  # Start from dimension 2 (H, W, D)\n",
    "        if dim1 < dim2:\n",
    "            # Calculate padding for tensor1\n",
    "            pad_left = (dim2 - dim1) // 2\n",
    "            pad_right = dim2 - dim1 - pad_left\n",
    "            padding1.extend([pad_left, pad_right])\n",
    "            padding2.extend([0, 0])  # No padding needed for tensor2 in this dimension\n",
    "        elif dim1 > dim2:\n",
    "            # Calculate padding for tensor2\n",
    "            pad_left = (dim1 - dim2) // 2\n",
    "            pad_right = dim1 - dim2 - pad_left\n",
    "            padding1.extend([0, 0])  # No padding needed for tensor1 in this dimension\n",
    "            padding2.extend([pad_left, pad_right])\n",
    "        else:\n",
    "            # No padding needed for this dimension, keep it symmetric\n",
    "            padding1.extend([0, 0])\n",
    "            padding2.extend([0, 0])\n",
    "\n",
    "    # Pad tensor1 and tensor2 with the calculated padding\n",
    "    padded_tensor1 = F.pad(tensor1, padding1)\n",
    "    padded_tensor2 = F.pad(tensor2, padding2)\n",
    "    \n",
    "    return padded_tensor1, padded_tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c67520ee-53db-45f6-a184-3ff3761bb80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D U-Net Architecture\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    The U-Net model will accept input Tensors of shape: [B, C, H, W, D]\n",
    "    Which based on the used MSLesSeg dataset will be [B, 3, 182, 218, 182]\n",
    "    The input Tensor is nothing more than the stacked [FLAIR, T1w, T2w] modalities\n",
    "\n",
    "    The output will be the segmentation mask of shape [B, 1, 182, 218, 182]\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, in_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.max_pool3d = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "\n",
    "        # == Contracting Path == #\n",
    "        self.down_conv_1 = double_conv(in_channels, 64)\n",
    "        self.down_conv_2 = double_conv(64, 128)\n",
    "        self.down_conv_3 = double_conv(128, 256)\n",
    "        self.down_conv_4 = double_conv(256, 512)\n",
    "        self.down_conv_5 = double_conv(512, 1024)\n",
    "\n",
    "        # == Expanding Path == #\n",
    "        self.up_transpose_1 = nn.ConvTranspose3d(\n",
    "            in_channels=1024,\n",
    "            out_channels=512,\n",
    "            kernel_size=2,\n",
    "            stride=2,\n",
    "            output_padding=1\n",
    "        )\n",
    "        self.up_conv_1 = double_conv(1024, 512)\n",
    "        \n",
    "        self.up_transpose_2 = nn.ConvTranspose3d(\n",
    "            in_channels=512,\n",
    "            out_channels=256,\n",
    "            kernel_size=2,\n",
    "            stride=2,\n",
    "            output_padding=1\n",
    "        )\n",
    "        self.up_conv_2 = double_conv(512, 256)\n",
    "\n",
    "        self.up_transpose_3 = nn.ConvTranspose3d(\n",
    "            in_channels=256,\n",
    "            out_channels=128,\n",
    "            kernel_size=2,\n",
    "            stride=2,\n",
    "            output_padding=1\n",
    "        )\n",
    "        self.up_conv_3 = double_conv(256, 128)\n",
    "\n",
    "        self.up_transpose_4 = nn.ConvTranspose3d(\n",
    "            in_channels=128,\n",
    "            out_channels=64,\n",
    "            kernel_size=2,\n",
    "            stride=2,\n",
    "            output_padding=1\n",
    "        )\n",
    "        self.up_conv_4 = double_conv(128, 64)\n",
    "\n",
    "        self.prob_out = final_conv(64, num_classes)\n",
    "\n",
    "        self.out = nn.Conv3d(\n",
    "            in_channels=64,\n",
    "            out_channels=num_classes,\n",
    "            kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        down_1 = self.down_conv_1(x)\n",
    "        down_2 = self.max_pool3d(down_1)\n",
    "        down_3 = self.down_conv_2(down_2)\n",
    "        down_4 = self.max_pool3d(down_3)\n",
    "        down_5 = self.down_conv_3(down_4)\n",
    "        down_6 = self.max_pool3d(down_5)\n",
    "        down_7 = self.down_conv_4(down_6)\n",
    "        down_8 = self.max_pool3d(down_7)\n",
    "        down_9 = self.down_conv_5(down_8)\n",
    "\n",
    "        up_1 = self.up_transpose_1(down_9)\n",
    "        up_1, down_7 = pad_to_match_shape(up_1, down_7)\n",
    "        x = self.up_conv_1(torch.cat([down_7, up_1], dim=1))\n",
    "        \n",
    "        up_2 = self.up_transpose_2(x)\n",
    "        up_2, down_5 = pad_to_match_shape(up_2, down_5)\n",
    "        x = self.up_conv_2(torch.cat([down_5, up_2], dim=1))\n",
    "        \n",
    "        up_3 = self.up_transpose_3(x)\n",
    "        up_3, down_3 = pad_to_match_shape(up_3, down_3)\n",
    "        x = self.up_conv_3(torch.cat([down_3, up_3], dim=1))\n",
    "        \n",
    "        up_4 = self.up_transpose_4(x)\n",
    "        up_4, down_1 = pad_to_match_shape(up_4, down_1)\n",
    "        x = self.up_conv_4(torch.cat([down_1, up_4], dim=1))\n",
    "\n",
    "        \n",
    "        out = self.out(x)\n",
    "        prob_out = self.prob_out(x)\n",
    "        \n",
    "        return out, prob_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fd1805d-2f4e-4057-9d61-2ac0755cbe69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "UNet                                     [1, 1, 191, 223, 191]     --\n",
       "‚îú‚îÄSequential: 1-1                        [1, 64, 182, 218, 182]    --\n",
       "‚îÇ    ‚îî‚îÄConv3d: 2-1                       [1, 64, 182, 218, 182]    5,248\n",
       "‚îÇ    ‚îî‚îÄGELU: 2-2                         [1, 64, 182, 218, 182]    --\n",
       "‚îÇ    ‚îî‚îÄConv3d: 2-3                       [1, 64, 182, 218, 182]    110,656\n",
       "‚îÇ    ‚îî‚îÄGELU: 2-4                         [1, 64, 182, 218, 182]    --\n",
       "‚îú‚îÄMaxPool3d: 1-2                         [1, 64, 91, 109, 91]      --\n",
       "‚îú‚îÄSequential: 1-3                        [1, 128, 91, 109, 91]     --\n",
       "‚îÇ    ‚îî‚îÄConv3d: 2-5                       [1, 128, 91, 109, 91]     221,312\n",
       "‚îÇ    ‚îî‚îÄGELU: 2-6                         [1, 128, 91, 109, 91]     --\n",
       "‚îÇ    ‚îî‚îÄConv3d: 2-7                       [1, 128, 91, 109, 91]     442,496\n",
       "‚îÇ    ‚îî‚îÄGELU: 2-8                         [1, 128, 91, 109, 91]     --\n",
       "‚îú‚îÄMaxPool3d: 1-4                         [1, 128, 45, 54, 45]      --\n",
       "‚îú‚îÄSequential: 1-5                        [1, 256, 45, 54, 45]      --\n",
       "‚îÇ    ‚îî‚îÄConv3d: 2-9                       [1, 256, 45, 54, 45]      884,992\n",
       "‚îÇ    ‚îî‚îÄGELU: 2-10                        [1, 256, 45, 54, 45]      --\n",
       "‚îÇ    ‚îî‚îÄConv3d: 2-11                      [1, 256, 45, 54, 45]      1,769,728\n",
       "‚îÇ    ‚îî‚îÄGELU: 2-12                        [1, 256, 45, 54, 45]      --\n",
       "‚îú‚îÄMaxPool3d: 1-6                         [1, 256, 22, 27, 22]      --\n",
       "‚îú‚îÄSequential: 1-7                        [1, 512, 22, 27, 22]      --\n",
       "‚îÇ    ‚îî‚îÄConv3d: 2-13                      [1, 512, 22, 27, 22]      3,539,456\n",
       "‚îÇ    ‚îî‚îÄGELU: 2-14                        [1, 512, 22, 27, 22]      --\n",
       "‚îÇ    ‚îî‚îÄConv3d: 2-15                      [1, 512, 22, 27, 22]      7,078,400\n",
       "‚îÇ    ‚îî‚îÄGELU: 2-16                        [1, 512, 22, 27, 22]      --\n",
       "‚îú‚îÄMaxPool3d: 1-8                         [1, 512, 11, 13, 11]      --\n",
       "‚îú‚îÄSequential: 1-9                        [1, 1024, 11, 13, 11]     --\n",
       "‚îÇ    ‚îî‚îÄConv3d: 2-17                      [1, 1024, 11, 13, 11]     14,156,800\n",
       "‚îÇ    ‚îî‚îÄGELU: 2-18                        [1, 1024, 11, 13, 11]     --\n",
       "‚îÇ    ‚îî‚îÄConv3d: 2-19                      [1, 1024, 11, 13, 11]     28,312,576\n",
       "‚îÇ    ‚îî‚îÄGELU: 2-20                        [1, 1024, 11, 13, 11]     --\n",
       "‚îú‚îÄConvTranspose3d: 1-10                  [1, 512, 23, 27, 23]      4,194,816\n",
       "‚îú‚îÄSequential: 1-11                       [1, 512, 23, 27, 23]      --\n",
       "‚îÇ    ‚îî‚îÄConv3d: 2-21                      [1, 512, 23, 27, 23]      14,156,288\n",
       "‚îÇ    ‚îî‚îÄGELU: 2-22                        [1, 512, 23, 27, 23]      --\n",
       "‚îÇ    ‚îî‚îÄConv3d: 2-23                      [1, 512, 23, 27, 23]      7,078,400\n",
       "‚îÇ    ‚îî‚îÄGELU: 2-24                        [1, 512, 23, 27, 23]      --\n",
       "‚îú‚îÄConvTranspose3d: 1-12                  [1, 256, 47, 55, 47]      1,048,832\n",
       "‚îú‚îÄSequential: 1-13                       [1, 256, 47, 55, 47]      --\n",
       "‚îÇ    ‚îî‚îÄConv3d: 2-25                      [1, 256, 47, 55, 47]      3,539,200\n",
       "‚îÇ    ‚îî‚îÄGELU: 2-26                        [1, 256, 47, 55, 47]      --\n",
       "‚îÇ    ‚îî‚îÄConv3d: 2-27                      [1, 256, 47, 55, 47]      1,769,728\n",
       "‚îÇ    ‚îî‚îÄGELU: 2-28                        [1, 256, 47, 55, 47]      --\n",
       "‚îú‚îÄConvTranspose3d: 1-14                  [1, 128, 95, 111, 95]     262,272\n",
       "‚îú‚îÄSequential: 1-15                       [1, 128, 95, 111, 95]     --\n",
       "‚îÇ    ‚îî‚îÄConv3d: 2-29                      [1, 128, 95, 111, 95]     884,864\n",
       "‚îÇ    ‚îî‚îÄGELU: 2-30                        [1, 128, 95, 111, 95]     --\n",
       "‚îÇ    ‚îî‚îÄConv3d: 2-31                      [1, 128, 95, 111, 95]     442,496\n",
       "‚îÇ    ‚îî‚îÄGELU: 2-32                        [1, 128, 95, 111, 95]     --\n",
       "‚îú‚îÄConvTranspose3d: 1-16                  [1, 64, 191, 223, 191]    65,600\n",
       "‚îú‚îÄSequential: 1-17                       [1, 64, 191, 223, 191]    --\n",
       "‚îÇ    ‚îî‚îÄConv3d: 2-33                      [1, 64, 191, 223, 191]    221,248\n",
       "‚îÇ    ‚îî‚îÄGELU: 2-34                        [1, 64, 191, 223, 191]    --\n",
       "‚îÇ    ‚îî‚îÄConv3d: 2-35                      [1, 64, 191, 223, 191]    110,656\n",
       "‚îÇ    ‚îî‚îÄGELU: 2-36                        [1, 64, 191, 223, 191]    --\n",
       "‚îú‚îÄConv3d: 1-18                           [1, 1, 191, 223, 191]     65\n",
       "‚îú‚îÄSequential: 1-19                       [1, 1, 191, 223, 191]     --\n",
       "‚îÇ    ‚îî‚îÄConv3d: 2-37                      [1, 1, 191, 223, 191]     65\n",
       "‚îÇ    ‚îî‚îÄSigmoid: 2-38                     [1, 1, 191, 223, 191]     --\n",
       "==========================================================================================\n",
       "Total params: 90,296,194\n",
       "Trainable params: 90,296,194\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.TERABYTES): 7.89\n",
       "==========================================================================================\n",
       "Input size (MB): 86.65\n",
       "Forward/backward pass size (MB): 26449.00\n",
       "Params size (MB): 361.18\n",
       "Estimated Total Size (MB): 26896.84\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the model summary\n",
    "from torchinfo import summary\n",
    "\n",
    "# Assuming your model is already defined (UNet or similar)\n",
    "model = UNet(num_classes=1, in_channels=3)\n",
    "\n",
    "# Example input size: (Batch Size, Channels, Height, Width, Depth)\n",
    "input_tensor = torch.randn(1, 3, 182, 218, 182)  # Modify based on your use case\n",
    "\n",
    "# Use summary from torchinfo to display the model summary\n",
    "summary(model, input_data=input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60727e14-353b-4fd3-9e3e-938472763e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be907f2a-2600-407f-8649-42965a1fb3d1",
   "metadata": {},
   "source": [
    "## The Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec1451e1-9756-49f3-a595-7fda87796588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we implement the DiceLoss\n",
    "def dice_loss_3d(pred, target, smooth=1):\n",
    "    \"\"\"\n",
    "    Computes Dice Loss for 3D segmentation tasks.\n",
    "    Args:\n",
    "    pred: Tensor of predictions (batch_size, C, D, H, W).\n",
    "    target: Tensor of ground truth (batch_size, C, D, H, W).\n",
    "    smooth: Smoothing factor.\n",
    "    Returns:\n",
    "    Scalar Dice Loss.\n",
    "    \"\"\"\n",
    "    pred = F.softmax(pred, dim=1)\n",
    "    num_classes = pred.shape[1]\n",
    "    dice = 0\n",
    "    for c in range(num_classes):\n",
    "        pred_c = pred[:, c]\n",
    "        target_c = target[:, c]\n",
    "        intersection = (pred_c * target_c).sum(dim=(2, 3, 4))\n",
    "        union = pred_c.sum(dim=(2, 3, 4)) + target_c.sum(dim=(2, 3, 4))\n",
    "        dice += (2. * intersection + smooth) / (union + smooth)\n",
    "        \n",
    "    return 1 - dice.mean() / num_classes\n",
    "\n",
    "# Then we implement the Mean Dice\n",
    "def mean_dice(pred, target, smooth=1):\n",
    "    \"\"\"\n",
    "    Computes Dice Loss for 3D segmentation tasks.\n",
    "    Args:\n",
    "    pred: Tensor of predictions (batch_size, C, D, H, W).\n",
    "    target: Tensor of ground truth (batch_size, C, D, H, W).\n",
    "    smooth: Smoothing factor.\n",
    "    Returns:\n",
    "    Scalar Dice Loss.\n",
    "    \"\"\"\n",
    "    pred = F.softmax(pred, dim=1)\n",
    "    num_classes = pred.shape[1]\n",
    "    dice = 0\n",
    "    for c in range(num_classes):\n",
    "        pred_c = pred[:, c]\n",
    "        target_c = target[:, c]\n",
    "        intersection = (pred_c * target_c).sum(dim=(2, 3, 4))\n",
    "        union = pred_c.sum(dim=(2, 3, 4)) + target_c.sum(dim=(2, 3, 4))\n",
    "        dice += (2. * intersection + smooth) / (union + smooth)\n",
    "        \n",
    "    return dice.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b2d3471-ee79-41ff-b176-929864412593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning Trainer for the U-Net model\n",
    "import torch.optim as optim\n",
    "\n",
    "class MSLesionSegmentationModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, model, checkpoint_dir, lr=1e-4):\n",
    "        super(MSLesionSegmentationModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, prob_out = self.model(x)\n",
    "        return out, prob_out \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Separate the input Tensor from the Segmentation Mask\n",
    "        input_tensor, gt = batch\n",
    "        # Forward Pass on the model\n",
    "        y_pred = self(input_tensor)[0]\n",
    "        # Compute DiceLoss\n",
    "        loss = dice_loss_3d(y_pred, gt)\n",
    "\n",
    "        # log the train loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Separate the input Tensor from the Segmentation Mask\n",
    "        input_tensor, gt = batch\n",
    "        # Forward pass on the model\n",
    "        y_pred = self(input_tensor)[0]\n",
    "        # Compute DiceLoss\n",
    "        loss = dice_loss_3d(y_pred, gt)\n",
    "        # Compute the Mean Dice Score\n",
    "        dice_score = mean_dice(y_pred, gt)\n",
    "        # log the val loss\n",
    "        self.log(\"val_loss\", loss)\n",
    "        # log the Mean Dice Score\n",
    "        self.log(\"dice_score\", dice_score)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Separate the input Tensor from the Segmentation Mask\n",
    "        input_tensor, gt = batch\n",
    "        # Forward pass on the model\n",
    "        y_pred = self(input_tensor)[0]\n",
    "        # Compute DiceLoss\n",
    "        loss = dice_loss_3d(y_pred, gt)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "        \n",
    "    def on_train_epoch_end(self):\n",
    "        # Save the model's checkpoint at the end of the epoch\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "        save_path = os.path.join(self.save_dir, \"unet_best_model.pth\")\n",
    "        \n",
    "        # Prepare the checkpoint dict\n",
    "        checkpoint = {\n",
    "            'epoch': self.current_epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,\n",
    "            'learning_rate': self.lr,\n",
    "        }\n",
    "\n",
    "        # Save and flush to disk\n",
    "        with open(save_path, 'wb') as f:\n",
    "            torch.save(checkpoint, f)\n",
    "            f.flush()\n",
    "            os.fsync(f.fileno())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b884f-771e-4852-be17-23973479e5a5",
   "metadata": {},
   "source": [
    "## The Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "330d4d97-5326-4dcf-b27e-2f3a57de10f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdrnnrw00m10c351s\u001b[0m (\u001b[33mfpv-perceivelab-unict\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "# Register a free account with Weights and Biases, and create a new project in order to obtain an API Key for the training\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the env variable\n",
    "api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "# Login to wandb\n",
    "if api_key:\n",
    "    os.environ[\"WANDB_API_KEY\"] = api_key\n",
    "    wandb.login()\n",
    "else:\n",
    "    print(\"‚ùå WANDB_API_KEY not found in .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd78050f-09dd-4251-a8b1-468756f2f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Weights and Biases logger\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project='MSLesSeg-4-ICPR',     # Change to your actual project name\n",
    "    name='UNet_run_1', # A specific run name\n",
    "    log_model=True          # Optional: log model checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1804c28e-27c2-490d-bf6f-83be924e2674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the constant for the maximum number of epochs for the training\n",
    "MAX_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe02bc36-efab-409d-9ff4-780ebf605e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of Dataloader hyperparameters (Batch size, seed and num workers)\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "TRAIN_NUM_WORKERS = 0\n",
    "TEST_NUM_WORKERS = 0\n",
    "\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "VAL_BATCH_SIZE = 1\n",
    "TEST_BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "094c3a1c-11bf-456a-a77b-8a0eb8698b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drew/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Create the Trainer object\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    accelerator=\"gpu\",\n",
    "    precision=16,\n",
    "    logger=wandb_logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05a9d033-88de-417f-ad1a-1be7b3b3c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data module\n",
    "data_module = MSLesSegDataModule(\n",
    "    root_data_dir=\"../data/02-Tensor-Data/\",\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    val_split=VAL_SPLIT,\n",
    "    num_workers=TRAIN_NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80715b4e-731b-496e-85f4-1f3f31f7c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the PyTorch Lightning Module (wrapper for the PyTorch nn.Module ~ Architecture)\n",
    "lightning_model = MSLesionSegmentationModel(\n",
    "    model=UNet(in_channels=3, num_classes=2),\n",
    "    checkpoint_dir=\"./model_checkpoints\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "038c95be-0282-431c-b17f-75932d56e760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Check the initial memory consuption (GPU) of the project\n",
    "import torch\n",
    "\n",
    "# Check initial GPU memory usage\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cf24dbb-10f4-4a5a-955d-84f46cf566f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250507_234107-2scckzoe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR/runs/2scckzoe' target=\"_blank\">UNet_run_1</a></strong> to <a href='https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR' target=\"_blank\">https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR/runs/2scckzoe' target=\"_blank\">https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR/runs/2scckzoe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params | Mode \n",
      "---------------------------------------\n",
      "0 | model | UNet | 90.3 M | train\n",
      "---------------------------------------\n",
      "90.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "90.3 M    Total params\n",
      "361.185   Total estimated model params size (MB)\n",
      "55        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4323281720074e968b5e5725f300ad08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drew/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (191) must match the size of tensor b (182) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train (fit) the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlightning_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     51\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    592\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1017\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1054\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1053\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m   1056\u001b[39m         \u001b[38;5;28mself\u001b[39m.fit_loop.run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1083\u001b[39m, in \u001b[36mTrainer._run_sanity_check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1080\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_start\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1082\u001b[39m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m \u001b[43mval_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1085\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_end\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1087\u001b[39m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    177\u001b[39m     context_manager = torch.no_grad\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:145\u001b[39m, in \u001b[36m_EvaluationLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28mself\u001b[39m.batch_progress.is_last_batch = data_fetcher.done\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    147\u001b[39m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:437\u001b[39m, in \u001b[36m_EvaluationLoop._evaluation_step\u001b[39m\u001b[34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[39m\n\u001b[32m    431\u001b[39m hook_name = \u001b[33m\"\u001b[39m\u001b[33mtest_step\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.testing \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvalidation_step\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    432\u001b[39m step_args = (\n\u001b[32m    433\u001b[39m     \u001b[38;5;28mself\u001b[39m._build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[32m    436\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.batch_progress.increment_processed()\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[32m    442\u001b[39m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:328\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    331\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:412\u001b[39m, in \u001b[36mStrategy.validation_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mvalidation_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mMSLesionSegmentationModel.validation_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m     33\u001b[39m y_pred = \u001b[38;5;28mself\u001b[39m(input_tensor)[\u001b[32m0\u001b[39m]\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Compute DiceLoss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m loss = \u001b[43mdice_loss_3d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Compute the Mean Dice Score\u001b[39;00m\n\u001b[32m     37\u001b[39m dice_score = mean_dice(y_pred, gt)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mdice_loss_3d\u001b[39m\u001b[34m(pred, target, smooth)\u001b[39m\n\u001b[32m     16\u001b[39m pred_c = pred[:, c]\n\u001b[32m     17\u001b[39m target_c = target[:, c]\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m intersection = (\u001b[43mpred_c\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_c\u001b[49m).sum(dim=(\u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m))\n\u001b[32m     19\u001b[39m union = pred_c.sum(dim=(\u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m)) + target_c.sum(dim=(\u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m))\n\u001b[32m     20\u001b[39m dice += (\u001b[32m2.\u001b[39m * intersection + smooth) / (union + smooth)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (191) must match the size of tensor b (182) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "# Train (fit) the model\n",
    "trainer.fit(lightning_model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3886c494-2f08-4fb4-868e-ce9e4866b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model's checkpoint and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee0a5d-61d4-4561-a731-b24483798f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "trainer.test(model, test_dataloader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6833331b-df4e-485b-8b26-e617ad583746",
   "metadata": {},
   "source": [
    "## Visualizing the Learned Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2febf704-6408-4a85-8d9c-19b88747f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to extract features from the model\n",
    "def extract_features(model, dataloader, device='cuda'):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, target = data['image'].to(device), data['mask'].to(device)  # Replace 'image' and 'mask' keys as per your dataset\n",
    "            \n",
    "            # Forward pass through the model\n",
    "            output = model(inputs)  # This is where we extract the feature; adjust according to your model's architecture\n",
    "            \n",
    "            # Extract features from MoE (or any other layer you want to visualize)\n",
    "            # Here I assume output is the final feature map after the MoE layer\n",
    "            feature_map = output.view(output.size(0), -1)  # Flatten the features to (batch_size, features)\n",
    "            features.append(feature_map.cpu().numpy())\n",
    "            labels.append(target.cpu().numpy())  # Add the target (or ground truth) labels for color coding in t-SNE\n",
    "\n",
    "    features = np.concatenate(features, axis=0)  # Combine all the feature maps\n",
    "    labels = np.concatenate(labels, axis=0)  # Combine all the labels\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "# Function to apply t-SNE on features\n",
    "def plot_tsne(features, labels):\n",
    "    # Standardize the features (optional but recommended for t-SNE)\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(features)\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=labels, cmap='jet', alpha=0.5)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('t-SNE Visualization of Learned Representations')\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage\n",
    "train_dataloader = # Your train DataLoader here\n",
    "\n",
    "# Extract features and labels from the model\n",
    "features, labels = extract_features(model, train_dataloader, device='cuda')\n",
    "\n",
    "# Plot t-SNE visualization\n",
    "plot_tsne(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4089cdf9-90ee-4700-8ec5-2c246a69d848",
   "metadata": {},
   "source": [
    "## Post Hoc Model Explainability - GradCAM++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee0469-f55a-4c46-9c72-a7ba80ad41b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from captum.attr import LayerGradCam, LayerGradCamPlusPlus\n",
    "from captum.attr import GuidedBackprop\n",
    "\n",
    "# Function to visualize GradCAM++ output\n",
    "def visualize_gradcam_plus_plus(model, input_tensor, target_class=None, layer_name='decoder', device='cuda'):\n",
    "    # Move input tensor to the appropriate device\n",
    "    input_tensor = input_tensor.to(device)\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Create GradCAM++ object for the specified layer\n",
    "    gradcam_pp = LayerGradCamPlusPlus(model, model.decoder[2])  # Assuming 'decoder' is your final layer, adjust accordingly.\n",
    "    \n",
    "    # Apply GradCAM++ to input tensor\n",
    "    attributions = gradcam_pp.attribute(input_tensor, target=target_class)\n",
    "    \n",
    "    # Convert attributions to numpy\n",
    "    attributions = attributions.cpu().detach().numpy()\n",
    "    \n",
    "    # Normalize the heatmap\n",
    "    heatmap = np.sum(attributions[0], axis=0)  # Summing over channels for a single-channel heatmap\n",
    "    heatmap = np.maximum(heatmap, 0)  # ReLU to ignore negative values\n",
    "    heatmap = cv2.resize(heatmap, (input_tensor.shape[2], input_tensor.shape[3]))  # Resize to input size\n",
    "    heatmap = cv2.normalize(heatmap, None, 0, 1, cv2.NORM_MINMAX)  # Normalize the heatmap to [0, 1]\n",
    "    \n",
    "    # Convert the original image to numpy and rescale\n",
    "    input_image = input_tensor[0].cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    input_image = cv2.resize(input_image, (input_tensor.shape[2], input_tensor.shape[3]))  # Resize to input size\n",
    "    \n",
    "    # Create a colormap for the heatmap\n",
    "    colormap = plt.get_cmap('jet')\n",
    "    colored_heatmap = colormap(heatmap)  # Apply colormap\n",
    "    \n",
    "    # Overlay the heatmap on top of the original image\n",
    "    superimposed_img = np.uint8(input_image * 255)  # Convert original image back to [0, 255] range\n",
    "    superimposed_img = cv2.addWeighted(superimposed_img, 0.7, np.uint8(colored_heatmap[:, :, :3] * 255), 0.3, 0)\n",
    "    \n",
    "    # Plot the result\n",
    "    plt.imshow(superimposed_img)\n",
    "    plt.title('GradCAM++ Heatmap')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage (assuming `model` is your trained model and `input_tensor` is an input image)\n",
    "input_tensor = torch.randn(1, 3, 128, 128).to(device)  # Example input, use your actual input tensor\n",
    "target_class = None  # You can provide a target class or leave as None for model's top predicted class\n",
    "\n",
    "visualize_gradcam_plus_plus(model, input_tensor, target_class=target_class, layer_name='decoder', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb9090-3260-49ce-be05-a03c2d122e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size of the stored (pre-processed) Tensors\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def inspect_pt_file(file_path):\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(f\"File does not exist: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    data = torch.load(file_path, map_location='cpu')\n",
    "\n",
    "    print(f\"\\nLoaded file: {file_path}\")\n",
    "    \n",
    "    if isinstance(data, torch.Tensor):\n",
    "        print(f\"Single Tensor - Shape: {data.shape}\")\n",
    "    \n",
    "    elif isinstance(data, dict):\n",
    "        print(\"Dictionary of tensors:\")\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                print(f\"  {key}: shape = {value.shape}\")\n",
    "            else:\n",
    "                print(f\"  {key}: type = {type(value)}\")\n",
    "    \n",
    "    elif isinstance(data, (list, tuple)):\n",
    "        print(f\"{type(data).__name__} of tensors:\")\n",
    "        for idx, item in enumerate(data):\n",
    "            if isinstance(item, torch.Tensor):\n",
    "                print(f\"  [{idx}]: shape = {item.shape}\")\n",
    "            else:\n",
    "                print(f\"  [{idx}]: type = {type(item)}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Unknown type loaded: {type(data)}\")\n",
    "        print(f\"Unkown type shape: {data.shape}\")\n",
    "\n",
    "INPUT_PATH_PREFIX = \"../data/02-Tensor-Data/train/MSLS_000/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af84775-fb63-4bfe-9fda-a1b238a7a20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Tensor Shape (Stacked Modalities)\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"input_tensor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feeedd4-a2d9-4e3d-900d-d3ca5a68d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation Mask\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"seg_mask.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858f58bf-be14-45b3-9f56-44f6d8719a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

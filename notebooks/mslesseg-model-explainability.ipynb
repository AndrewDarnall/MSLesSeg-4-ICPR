{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7cd63f2-7fd0-4452-a4ed-0951a065ff88",
   "metadata": {},
   "source": [
    "# MSLesSeg Model Explainability\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cf734c-5a6d-418b-a4c4-31d1799806f7",
   "metadata": {},
   "source": [
    "## SegFormer3D Model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6146d5f-628c-4a8e-81df-a26d9a279e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "\n",
    "from functools import partial\n",
    "import torch\n",
    "from torch import nn\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class SegFormer3D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,\n",
    "        sr_ratios: list = [4, 2, 1, 1],  # [2, 2, 1, 1] for potentially more accurate results\n",
    "        embed_dims: list = [32, 64, 160, 256], # [64, 128, 320, 512] for a VERY aggressive model\n",
    "        patch_kernel_size: list = [7, 3, 3, 3],\n",
    "        patch_stride: list = [4, 2, 2, 2],  # [2, 2, 2, 2] for more detail\n",
    "        patch_padding: list = [3, 1, 1, 1],\n",
    "        mlp_ratios: list = [4, 4, 4, 4],  # Increase to experiment with efficiency if the GPU allows for it\n",
    "        num_heads: list = [1, 2, 5, 8],  # Increase only as a last resort\n",
    "        depths: list = [2, 2, 2, 2],  # Controls the depth of the model\n",
    "        decoder_head_embedding_dim: int = 256, # 128 for a smaller model\n",
    "        num_classes: int = 1,\n",
    "        decoder_dropout: float = 0.0, # increase if overfitting is observed\n",
    "    ):\n",
    "        \"\"\"\n",
    "        in_channels: number of the input channels\n",
    "        img_volume_dim: spatial resolution of the image volume (Depth, Width, Height)\n",
    "        sr_ratios: the rates at which to down sample the sequence length of the embedded patch\n",
    "        embed_dims: hidden size of the PatchEmbedded input\n",
    "        patch_kernel_size: kernel size for the convolution in the patch embedding module\n",
    "        patch_stride: stride for the convolution in the patch embedding module\n",
    "        patch_padding: padding for the convolution in the patch embedding module\n",
    "        mlp_ratios: at which rate increases the projection dim of the hidden_state in the mlp\n",
    "        num_heads: number of attention heads\n",
    "        depths: number of attention layers\n",
    "        decoder_head_embedding_dim: projection dimension of the mlp layer in the all-mlp-decoder module\n",
    "        num_classes: number of the output channel of the network\n",
    "        decoder_dropout: dropout rate of the concatenated feature maps\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.segformer_encoder = MixVisionTransformer(\n",
    "            in_channels=in_channels,\n",
    "            sr_ratios=sr_ratios,\n",
    "            embed_dims=embed_dims,\n",
    "            patch_kernel_size=patch_kernel_size,\n",
    "            patch_stride=patch_stride,\n",
    "            patch_padding=patch_padding,\n",
    "            mlp_ratios=mlp_ratios,\n",
    "            num_heads=num_heads,\n",
    "            depths=depths,\n",
    "        )\n",
    "        # decoder takes in the feature maps in the reversed order\n",
    "        reversed_embed_dims = embed_dims[::-1]\n",
    "        self.segformer_decoder = SegFormerDecoderHead(\n",
    "            input_feature_dims=reversed_embed_dims,\n",
    "            decoder_head_embedding_dim=decoder_head_embedding_dim,\n",
    "            num_classes=num_classes,\n",
    "            dropout=decoder_dropout,\n",
    "        )\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.BatchNorm3d):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Conv3d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.kernel_size[2] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # embedding the input\n",
    "        x = self.segformer_encoder(x)\n",
    "        # # unpacking the embedded features generated by the transformer\n",
    "        c1 = x[0]\n",
    "        c2 = x[1]\n",
    "        c3 = x[2]\n",
    "        c4 = x[3]\n",
    "        # decoding the embedded features\n",
    "        x = self.segformer_decoder(c1, c2, c3, c4)\n",
    "\n",
    "\n",
    "        return x\n",
    "    \n",
    "# ----------------------------------------------------- encoder -----------------------------------------------------\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel: int = 4,\n",
    "        embed_dim: int = 768,\n",
    "        kernel_size: int = 7,\n",
    "        stride: int = 4,\n",
    "        padding: int = 3,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        in_channels: number of the channels in the input volume\n",
    "        embed_dim: embedding dimmesion of the patch\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.patch_embeddings = nn.Conv3d(\n",
    "            in_channel,\n",
    "            embed_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # standard embedding patch\n",
    "        patches = self.patch_embeddings(x)\n",
    "        patches = patches.flatten(2).transpose(1, 2)\n",
    "        patches = self.norm(patches)\n",
    "        return patches\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 768,\n",
    "        num_heads: int = 8,\n",
    "        sr_ratio: int = 2,\n",
    "        qkv_bias: bool = False,\n",
    "        attn_dropout: float = 0.0,\n",
    "        proj_dropout: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        embed_dim : hidden size of the PatchEmbedded input\n",
    "        num_heads: number of attention heads\n",
    "        sr_ratio: the rate at which to down sample the sequence length of the embedded patch\n",
    "        qkv_bias: whether or not the linear projection has bias\n",
    "        attn_dropout: the dropout rate of the attention component\n",
    "        proj_dropout: the dropout rate of the final linear projection\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert (\n",
    "            embed_dim % num_heads == 0\n",
    "        ), \"Embedding dim should be divisible by number of heads!\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        # embedding dimesion of each attention head\n",
    "        self.attention_head_dim = embed_dim // num_heads\n",
    "\n",
    "        # The same input is used to generate the query, key, and value,\n",
    "        # (batch_size, num_patches, hidden_size) -> (batch_size, num_patches, attention_head_size)\n",
    "        self.query = nn.Linear(embed_dim, embed_dim, bias=qkv_bias)\n",
    "        self.key_value = nn.Linear(embed_dim, 2 * embed_dim, bias=qkv_bias)\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_dropout = nn.Dropout(proj_dropout)\n",
    "\n",
    "        self.sr_ratio = sr_ratio\n",
    "        if sr_ratio > 1:\n",
    "            self.sr = nn.Conv3d(\n",
    "                embed_dim, embed_dim, kernel_size=sr_ratio, stride=sr_ratio\n",
    "            )\n",
    "            self.sr_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, num_patches, hidden_size)\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # (batch_size, num_head, sequence_length, embed_dim)\n",
    "        q = (\n",
    "            self.query(x)\n",
    "            .reshape(B, N, self.num_heads, self.attention_head_dim)\n",
    "            .permute(0, 2, 1, 3)\n",
    "        )\n",
    "\n",
    "        if self.sr_ratio > 1:\n",
    "            n = cube_root(N)\n",
    "            # (batch_size, sequence_length, embed_dim) -> (batch_size, embed_dim, patch_D, patch_H, patch_W)\n",
    "            x_ = x.permute(0, 2, 1).reshape(B, C, n, n, n)\n",
    "            # (batch_size, embed_dim, patch_D, patch_H, patch_W) -> (batch_size, embed_dim, patch_D/sr_ratio, patch_H/sr_ratio, patch_W/sr_ratio)\n",
    "            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)\n",
    "            # (batch_size, embed_dim, patch_D/sr_ratio, patch_H/sr_ratio, patch_W/sr_ratio) -> (batch_size, sequence_length, embed_dim)\n",
    "            # normalizing the layer\n",
    "            x_ = self.sr_norm(x_)\n",
    "            # (batch_size, num_patches, hidden_size)\n",
    "            kv = (\n",
    "                self.key_value(x_)\n",
    "                .reshape(B, -1, 2, self.num_heads, self.attention_head_dim)\n",
    "                .permute(2, 0, 3, 1, 4)\n",
    "            )\n",
    "            # (2, batch_size, num_heads, num_sequence, attention_head_dim)\n",
    "        else:\n",
    "            # (batch_size, num_patches, hidden_size)\n",
    "            kv = (\n",
    "                self.key_value(x)\n",
    "                .reshape(B, -1, 2, self.num_heads, self.attention_head_dim)\n",
    "                .permute(2, 0, 3, 1, 4)\n",
    "            )\n",
    "            # (2, batch_size, num_heads, num_sequence, attention_head_dim)\n",
    "\n",
    "        k, v = kv[0], kv[1]\n",
    "\n",
    "        attention_score = (q @ k.transpose(-2, -1)) / math.sqrt(self.num_heads)\n",
    "        attnention_prob = attention_score.softmax(dim=-1)\n",
    "        attnention_prob = self.attn_dropout(attnention_prob)\n",
    "        out = (attnention_prob @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 768,\n",
    "        mlp_ratio: int = 2,\n",
    "        num_heads: int = 8,\n",
    "        sr_ratio: int = 2,\n",
    "        qkv_bias: bool = False,\n",
    "        attn_dropout: float = 0.0,\n",
    "        proj_dropout: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        embed_dim : hidden size of the PatchEmbedded input\n",
    "        mlp_ratio: at which rate increasse the projection dim of the embedded patch in the _MLP component\n",
    "        num_heads: number of attention heads\n",
    "        sr_ratio: the rate at which to down sample the sequence length of the embedded patch\n",
    "        qkv_bias: whether or not the linear projection has bias\n",
    "        attn_dropout: the dropout rate of the attention component\n",
    "        proj_dropout: the dropout rate of the final linear projection\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = SelfAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            sr_ratio=sr_ratio,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_dropout=attn_dropout,\n",
    "            proj_dropout=proj_dropout,\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = _MLP(in_feature=embed_dim, mlp_ratio=mlp_ratio, dropout=0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MixVisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 4,\n",
    "        sr_ratios: list = [8, 4, 2, 1],\n",
    "        embed_dims: list = [64, 128, 320, 512],\n",
    "        patch_kernel_size: list = [7, 3, 3, 3],\n",
    "        patch_stride: list = [4, 2, 2, 2],\n",
    "        patch_padding: list = [3, 1, 1, 1],\n",
    "        mlp_ratios: list = [2, 2, 2, 2],\n",
    "        num_heads: list = [1, 2, 5, 8],\n",
    "        depths: list = [2, 2, 2, 2],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        in_channels: number of the input channels\n",
    "        img_volume_dim: spatial resolution of the image volume (Depth, Width, Height)\n",
    "        sr_ratios: the rates at which to down sample the sequence length of the embedded patch\n",
    "        embed_dims: hidden size of the PatchEmbedded input\n",
    "        patch_kernel_size: kernel size for the convolution in the patch embedding module\n",
    "        patch_stride: stride for the convolution in the patch embedding module\n",
    "        patch_padding: padding for the convolution in the patch embedding module\n",
    "        mlp_ratio: at which rate increasse the projection dim of the hidden_state in the mlp\n",
    "        num_heads: number of attenion heads\n",
    "        depth: number of attention layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # patch embedding at different Pyramid level\n",
    "        self.embed_1 = PatchEmbedding(\n",
    "            in_channel=in_channels,\n",
    "            embed_dim=embed_dims[0],\n",
    "            kernel_size=patch_kernel_size[0],\n",
    "            stride=patch_stride[0],\n",
    "            padding=patch_padding[0],\n",
    "        )\n",
    "        self.embed_2 = PatchEmbedding(\n",
    "            in_channel=embed_dims[0],\n",
    "            embed_dim=embed_dims[1],\n",
    "            kernel_size=patch_kernel_size[1],\n",
    "            stride=patch_stride[1],\n",
    "            padding=patch_padding[1],\n",
    "        )\n",
    "        self.embed_3 = PatchEmbedding(\n",
    "            in_channel=embed_dims[1],\n",
    "            embed_dim=embed_dims[2],\n",
    "            kernel_size=patch_kernel_size[2],\n",
    "            stride=patch_stride[2],\n",
    "            padding=patch_padding[2],\n",
    "        )\n",
    "        self.embed_4 = PatchEmbedding(\n",
    "            in_channel=embed_dims[2],\n",
    "            embed_dim=embed_dims[3],\n",
    "            kernel_size=patch_kernel_size[3],\n",
    "            stride=patch_stride[3],\n",
    "            padding=patch_padding[3],\n",
    "        )\n",
    "\n",
    "        # block 1\n",
    "        self.tf_block1 = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_dim=embed_dims[0],\n",
    "                    num_heads=num_heads[0],\n",
    "                    mlp_ratio=mlp_ratios[0],\n",
    "                    sr_ratio=sr_ratios[0],\n",
    "                    qkv_bias=True,\n",
    "                )\n",
    "                for _ in range(depths[0])\n",
    "            ]\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dims[0])\n",
    "\n",
    "        # block 2\n",
    "        self.tf_block2 = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_dim=embed_dims[1],\n",
    "                    num_heads=num_heads[1],\n",
    "                    mlp_ratio=mlp_ratios[1],\n",
    "                    sr_ratio=sr_ratios[1],\n",
    "                    qkv_bias=True,\n",
    "                )\n",
    "                for _ in range(depths[1])\n",
    "            ]\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dims[1])\n",
    "\n",
    "        # block 3\n",
    "        self.tf_block3 = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_dim=embed_dims[2],\n",
    "                    num_heads=num_heads[2],\n",
    "                    mlp_ratio=mlp_ratios[2],\n",
    "                    sr_ratio=sr_ratios[2],\n",
    "                    qkv_bias=True,\n",
    "                )\n",
    "                for _ in range(depths[2])\n",
    "            ]\n",
    "        )\n",
    "        self.norm3 = nn.LayerNorm(embed_dims[2])\n",
    "\n",
    "        # block 4\n",
    "        self.tf_block4 = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_dim=embed_dims[3],\n",
    "                    num_heads=num_heads[3],\n",
    "                    mlp_ratio=mlp_ratios[3],\n",
    "                    sr_ratio=sr_ratios[3],\n",
    "                    qkv_bias=True,\n",
    "                )\n",
    "                for _ in range(depths[3])\n",
    "            ]\n",
    "        )\n",
    "        self.norm4 = nn.LayerNorm(embed_dims[3])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        # at each stage these are the following mappings:\n",
    "        # (batch_size, num_patches, hidden_state)\n",
    "        # (num_patches,) -> (D, H, W)\n",
    "        # (batch_size, num_patches, hidden_state) -> (batch_size, hidden_state, D, H, W)\n",
    "\n",
    "        # stage 1\n",
    "        x = self.embed_1(x)\n",
    "        B, N, C = x.shape\n",
    "        n = cube_root(N)\n",
    "        for i, blk in enumerate(self.tf_block1):\n",
    "            x = blk(x)\n",
    "        x = self.norm1(x)\n",
    "        # (B, N, C) -> (B, D, H, W, C) -> (B, C, D, H, W)\n",
    "        x = x.reshape(B, n, n, n, -1).permute(0, 4, 1, 2, 3).contiguous()\n",
    "        out.append(x)\n",
    "\n",
    "        # stage 2\n",
    "        x = self.embed_2(x)\n",
    "        B, N, C = x.shape\n",
    "        n = cube_root(N)\n",
    "        for i, blk in enumerate(self.tf_block2):\n",
    "            x = blk(x)\n",
    "        x = self.norm2(x)\n",
    "        # (B, N, C) -> (B, D, H, W, C) -> (B, C, D, H, W)\n",
    "        x = x.reshape(B, n, n, n, -1).permute(0, 4, 1, 2, 3).contiguous()\n",
    "        out.append(x)\n",
    "\n",
    "        # stage 3\n",
    "        x = self.embed_3(x)\n",
    "        B, N, C = x.shape\n",
    "        n = cube_root(N)\n",
    "        for i, blk in enumerate(self.tf_block3):\n",
    "            x = blk(x)\n",
    "        x = self.norm3(x)\n",
    "        # (B, N, C) -> (B, D, H, W, C) -> (B, C, D, H, W)\n",
    "        x = x.reshape(B, n, n, n, -1).permute(0, 4, 1, 2, 3).contiguous()\n",
    "        out.append(x)\n",
    "\n",
    "        # stage 4\n",
    "        x = self.embed_4(x)\n",
    "        B, N, C = x.shape\n",
    "        n = cube_root(N)\n",
    "        for i, blk in enumerate(self.tf_block4):\n",
    "            x = blk(x)\n",
    "        x = self.norm4(x)\n",
    "        # (B, N, C) -> (B, D, H, W, C) -> (B, C, D, H, W)\n",
    "        x = x.reshape(B, n, n, n, -1).permute(0, 4, 1, 2, 3).contiguous()\n",
    "        out.append(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class _MLP(nn.Module):\n",
    "    def __init__(self, in_feature, mlp_ratio=2, dropout=0.0):\n",
    "        super().__init__()\n",
    "        out_feature = mlp_ratio * in_feature\n",
    "        self.fc1 = nn.Linear(in_feature, out_feature)\n",
    "        self.dwconv = DWConv(dim=out_feature)\n",
    "        self.fc2 = nn.Linear(out_feature, in_feature)\n",
    "        self.act_fn = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.dwconv(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class DWConv(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv3d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
    "        # added batchnorm (remove it ?)\n",
    "        self.bn = nn.BatchNorm3d(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        # (batch, patch_cube, hidden_size) -> (batch, hidden_size, D, H, W)\n",
    "        # assuming D = H = W, i.e. cube root of the patch is an integer number!\n",
    "        n = cube_root(N)\n",
    "        x = x.transpose(1, 2).view(B, C, n, n, n)\n",
    "        x = self.dwconv(x)\n",
    "        # added batchnorm (remove it ?)\n",
    "        x = self.bn(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def cube_root(n):\n",
    "    return round(math.pow(n, (1 / 3)))\n",
    "    \n",
    "\n",
    "\n",
    "# ----------------------------------------------------- decoder -------------------\n",
    "class MLP_(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear Embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=2048, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(input_dim, embed_dim)\n",
    "        self.bn = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(2).transpose(1, 2).contiguous()\n",
    "        x = self.proj(x)\n",
    "        # added batchnorm (remove it ?)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class SegFormerDecoderHead(nn.Module):\n",
    "    \"\"\"\n",
    "    SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_feature_dims: list = [512, 320, 128, 64],\n",
    "        decoder_head_embedding_dim: int = 256,\n",
    "        num_classes: int = 3,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        input_feature_dims: list of the output features channels generated by the transformer encoder\n",
    "        decoder_head_embedding_dim: projection dimension of the mlp layer in the all-mlp-decoder module\n",
    "        num_classes: number of the output channels\n",
    "        dropout: dropout rate of the concatenated feature maps\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear_c4 = MLP_(\n",
    "            input_dim=input_feature_dims[0],\n",
    "            embed_dim=decoder_head_embedding_dim,\n",
    "        )\n",
    "        self.linear_c3 = MLP_(\n",
    "            input_dim=input_feature_dims[1],\n",
    "            embed_dim=decoder_head_embedding_dim,\n",
    "        )\n",
    "        self.linear_c2 = MLP_(\n",
    "            input_dim=input_feature_dims[2],\n",
    "            embed_dim=decoder_head_embedding_dim,\n",
    "        )\n",
    "        self.linear_c1 = MLP_(\n",
    "            input_dim=input_feature_dims[3],\n",
    "            embed_dim=decoder_head_embedding_dim,\n",
    "        )\n",
    "        # convolution module to combine feature maps generated by the mlps\n",
    "        self.linear_fuse = nn.Sequential(\n",
    "            nn.Conv3d(\n",
    "                in_channels=4 * decoder_head_embedding_dim,\n",
    "                out_channels=decoder_head_embedding_dim,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm3d(decoder_head_embedding_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # final linear projection layer\n",
    "        self.linear_pred = nn.Conv3d(\n",
    "            decoder_head_embedding_dim, num_classes, kernel_size=1\n",
    "        )\n",
    "\n",
    "        # segformer decoder generates the final decoded feature map size at 1/4 of the original input volume size\n",
    "        self.upsample_volume = nn.Upsample(\n",
    "            scale_factor=4.0, mode=\"trilinear\", align_corners=False\n",
    "        )\n",
    "\n",
    "    def forward(self, c1, c2, c3, c4):\n",
    "       ############## _MLP decoder on C1-C4 ###########\n",
    "        n, _, _, _, _ = c4.shape\n",
    "\n",
    "        _c4 = (\n",
    "            self.linear_c4(c4)\n",
    "            .permute(0, 2, 1)\n",
    "            .reshape(n, -1, c4.shape[2], c4.shape[3], c4.shape[4])\n",
    "            .contiguous()\n",
    "        )\n",
    "        _c4 = torch.nn.functional.interpolate(\n",
    "            _c4,\n",
    "            size=c1.size()[2:],\n",
    "            mode=\"trilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "\n",
    "        _c3 = (\n",
    "            self.linear_c3(c3)\n",
    "            .permute(0, 2, 1)\n",
    "            .reshape(n, -1, c3.shape[2], c3.shape[3], c3.shape[4])\n",
    "            .contiguous()\n",
    "        )\n",
    "        _c3 = torch.nn.functional.interpolate(\n",
    "            _c3,\n",
    "            size=c1.size()[2:],\n",
    "            mode=\"trilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "\n",
    "        _c2 = (\n",
    "            self.linear_c2(c2)\n",
    "            .permute(0, 2, 1)\n",
    "            .reshape(n, -1, c2.shape[2], c2.shape[3], c2.shape[4])\n",
    "            .contiguous()\n",
    "        )\n",
    "        _c2 = torch.nn.functional.interpolate(\n",
    "            _c2,\n",
    "            size=c1.size()[2:],\n",
    "            mode=\"trilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "\n",
    "        _c1 = (\n",
    "            self.linear_c1(c1)\n",
    "            .permute(0, 2, 1)\n",
    "            .reshape(n, -1, c1.shape[2], c1.shape[3], c1.shape[4])\n",
    "            .contiguous()\n",
    "        )\n",
    "\n",
    "        _c = self.linear_fuse(torch.cat([_c4, _c3, _c2, _c1], dim=1))\n",
    "\n",
    "        x = self.dropout(_c)\n",
    "        x = self.linear_pred(x)\n",
    "        x = self.upsample_volume(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b07e101-1967-4ba3-a19a-96f4effeba1b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SegFormer3DMoE Model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a52bb0ef-025d-4816-b31c-b25f46719b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "\n",
    "from functools import partial\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, reduce\n",
    "\n",
    "\n",
    "class SegFormer3DMoE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,\n",
    "        sr_ratios: list = [4, 2, 1, 1],\n",
    "        embed_dims: list = [32, 64, 160, 256],\n",
    "        patch_kernel_size: list = [7, 3, 3, 3],\n",
    "        patch_stride: list = [4, 2, 2, 2],\n",
    "        patch_padding: list = [3, 1, 1, 1],\n",
    "        mlp_ratios: list = [4, 4, 4, 4],\n",
    "        num_experts_list: list = [4, 4, 6, 8],\n",
    "        experts_selected_list: list = [2, 2, 3, 4],\n",
    "        num_heads: list = [1, 2, 5, 8],\n",
    "        depths: list = [2, 2, 2, 2],\n",
    "        decoder_head_embedding_dim: int = 256,\n",
    "        num_classes: int = 1,\n",
    "        encoder_dropout: float = 0.0,\n",
    "        decoder_dropout: float = 0.0,\n",
    "        moe_dropout_encoder: float = 0.0,\n",
    "        moe_dropout_decoder: float = 0.0\n",
    "\n",
    "    ):\n",
    "        \"\"\"\n",
    "        in_channels: number of the input channels\n",
    "        img_volume_dim: spatial resolution of the image volume (Depth, Width, Height)\n",
    "        sr_ratios: the rates at which to down sample the sequence length of the embedded patch\n",
    "        embed_dims: hidden size of the PatchEmbedded input\n",
    "        patch_kernel_size: kernel size for the convolution in the patch embedding module\n",
    "        patch_stride: stride for the convolution in the patch embedding module\n",
    "        patch_padding: padding for the convolution in the patch embedding module\n",
    "        mlp_ratios: at which rate increases the projection dim of the hidden_state in the mlp\n",
    "        num_heads: number of attention heads\n",
    "        depths: number of attention layers\n",
    "        decoder_head_embedding_dim: projection dimension of the mlp layer in the all-mlp-decoder module\n",
    "        num_classes: number of the output channel of the network\n",
    "        decoder_dropout: dropout rate of the concatenated feature maps\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # MoE Specific Losses to avoid unpacking issues in the code\n",
    "        self.total_balance_loss = 0.0\n",
    "        self.total_router_z_loss = 0.0\n",
    "\n",
    "        # -- SegFormer3D Encoder -- #\n",
    "        self.segformer_encoder = MixVisionTransformerMoE(\n",
    "            in_channels=in_channels,\n",
    "            sr_ratios=sr_ratios,\n",
    "            embed_dims=embed_dims,\n",
    "            patch_kernel_size=patch_kernel_size,\n",
    "            patch_stride=patch_stride,\n",
    "            patch_padding=patch_padding,\n",
    "            mlp_ratios=mlp_ratios,\n",
    "            num_experts_list=num_experts_list,\n",
    "            experts_selected_list=experts_selected_list,\n",
    "            num_heads=num_heads,\n",
    "            depths=depths,\n",
    "            encoder_dropout=encoder_dropout,\n",
    "            moe_encoder_dropout=moe_dropout_encoder\n",
    "        )\n",
    "\n",
    "        # decoder takes in the feature maps in the reversed order\n",
    "        reversed_embed_dims = embed_dims[::-1]\n",
    "\n",
    "        # -- SegFormer3D Decoder -- #\n",
    "        self.segformer_decoder = SegFormerDecoderHeadMoE(\n",
    "            input_feature_dims=reversed_embed_dims,\n",
    "            decoder_head_embedding_dim=decoder_head_embedding_dim,\n",
    "            num_classes=num_classes,\n",
    "            dropout=decoder_dropout,\n",
    "            moe_decoder_dropout=moe_dropout_decoder\n",
    "            # TODO ---> There is a Static number of experts in the decoder!\n",
    "        )\n",
    "\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.BatchNorm3d):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Conv3d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.kernel_size[2] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # embedding the input\n",
    "        \n",
    "        x, total_encoder_balance_loss, total_encoder_router_z_loss = self.segformer_encoder(x)\n",
    "        \n",
    "        \n",
    "        # # unpacking the embedded features generated by the transformer\n",
    "        c1 = x[0]\n",
    "        c2 = x[1]\n",
    "        c3 = x[2]\n",
    "        c4 = x[3]\n",
    "        \n",
    "        # decoding the embedded features\n",
    "        x, total_decoder_balance_loss, total_decoder_router_z_loss = self.segformer_decoder(c1, c2, c3, c4)\n",
    "\n",
    "        # Internally store the MoE specific losses ot avoid unpacking errors\n",
    "        self.total_encoder_balance_loss = total_encoder_balance_loss\n",
    "        self.total_encoder_router_z_loss = total_encoder_router_z_loss \n",
    "        \n",
    "        self.total_decoder_balance_loss = total_decoder_balance_loss\n",
    "        self.total_decoder_router_z_loss = total_decoder_router_z_loss\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "# ----------------------------------------------------- encoder -----------------------------------------------------\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel: int = 4,\n",
    "        embed_dim: int = 768,\n",
    "        kernel_size: int = 7,\n",
    "        stride: int = 4,\n",
    "        padding: int = 3,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        in_channels: number of the channels in the input volume\n",
    "        embed_dim: embedding dimmesion of the patch\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.patch_embeddings = nn.Conv3d(\n",
    "            in_channel,\n",
    "            embed_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # standard embedding patch\n",
    "        patches = self.patch_embeddings(x)\n",
    "        patches = patches.flatten(2).transpose(1, 2)\n",
    "        patches = self.norm(patches)\n",
    "        return patches\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 768,\n",
    "        num_heads: int = 8,\n",
    "        sr_ratio: int = 2,\n",
    "        qkv_bias: bool = False,\n",
    "        attn_dropout: float = 0.0,\n",
    "        proj_dropout: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        embed_dim : hidden size of the PatchEmbedded input\n",
    "        num_heads: number of attention heads\n",
    "        sr_ratio: the rate at which to down sample the sequence length of the embedded patch\n",
    "        qkv_bias: whether or not the linear projection has bias\n",
    "        attn_dropout: the dropout rate of the attention component\n",
    "        proj_dropout: the dropout rate of the final linear projection\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert (\n",
    "            embed_dim % num_heads == 0\n",
    "        ), \"Embedding dim should be divisible by number of heads!\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        # embedding dimesion of each attention head\n",
    "        self.attention_head_dim = embed_dim // num_heads\n",
    "\n",
    "        # The same input is used to generate the query, key, and value,\n",
    "        # (batch_size, num_patches, hidden_size) -> (batch_size, num_patches, attention_head_size)\n",
    "        self.query = nn.Linear(embed_dim, embed_dim, bias=qkv_bias)\n",
    "        self.key_value = nn.Linear(embed_dim, 2 * embed_dim, bias=qkv_bias)\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_dropout = nn.Dropout(proj_dropout)\n",
    "\n",
    "        self.sr_ratio = sr_ratio\n",
    "        if sr_ratio > 1:\n",
    "            self.sr = nn.Conv3d(\n",
    "                embed_dim, embed_dim, kernel_size=sr_ratio, stride=sr_ratio\n",
    "            )\n",
    "            self.sr_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, num_patches, hidden_size)\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # (batch_size, num_head, sequence_length, embed_dim)\n",
    "        q = (\n",
    "            self.query(x)\n",
    "            .reshape(B, N, self.num_heads, self.attention_head_dim)\n",
    "            .permute(0, 2, 1, 3)\n",
    "        )\n",
    "\n",
    "        if self.sr_ratio > 1:\n",
    "            n = cube_root(N)\n",
    "            # (batch_size, sequence_length, embed_dim) -> (batch_size, embed_dim, patch_D, patch_H, patch_W)\n",
    "            x_ = x.permute(0, 2, 1).reshape(B, C, n, n, n)\n",
    "            # (batch_size, embed_dim, patch_D, patch_H, patch_W) -> (batch_size, embed_dim, patch_D/sr_ratio, patch_H/sr_ratio, patch_W/sr_ratio)\n",
    "            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)\n",
    "            # (batch_size, embed_dim, patch_D/sr_ratio, patch_H/sr_ratio, patch_W/sr_ratio) -> (batch_size, sequence_length, embed_dim)\n",
    "            # normalizing the layer\n",
    "            x_ = self.sr_norm(x_)\n",
    "            # (batch_size, num_patches, hidden_size)\n",
    "            kv = (\n",
    "                self.key_value(x_)\n",
    "                .reshape(B, -1, 2, self.num_heads, self.attention_head_dim)\n",
    "                .permute(2, 0, 3, 1, 4)\n",
    "            )\n",
    "            # (2, batch_size, num_heads, num_sequence, attention_head_dim)\n",
    "        else:\n",
    "            # (batch_size, num_patches, hidden_size)\n",
    "            kv = (\n",
    "                self.key_value(x)\n",
    "                .reshape(B, -1, 2, self.num_heads, self.attention_head_dim)\n",
    "                .permute(2, 0, 3, 1, 4)\n",
    "            )\n",
    "            # (2, batch_size, num_heads, num_sequence, attention_head_dim)\n",
    "\n",
    "        k, v = kv[0], kv[1]\n",
    "\n",
    "        attention_score = (q @ k.transpose(-2, -1)) / math.sqrt(self.num_heads)\n",
    "        attnention_prob = attention_score.softmax(dim=-1)\n",
    "        attnention_prob = self.attn_dropout(attnention_prob)\n",
    "        out = (attnention_prob @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerBlockMoE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 768,\n",
    "        mlp_ratio: int = 2,\n",
    "        num_experts: int = 8, \n",
    "        num_selected: int = 4,\n",
    "        num_heads: int = 8,\n",
    "        sr_ratio: int = 2,\n",
    "        qkv_bias: bool = False,\n",
    "        attn_dropout: float = 0.0,\n",
    "        proj_dropout: float = 0.0,\n",
    "        moe_encoder_dropout: float = 0.0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        embed_dim : hidden size of the PatchEmbedded input\n",
    "        mlp_ratio: at which rate increasse the projection dim of the embedded patch in the _MLP component\n",
    "        num_heads: number of attention heads\n",
    "        sr_ratio: the rate at which to down sample the sequence length of the embedded patch\n",
    "        qkv_bias: whether or not the linear projection has bias\n",
    "        attn_dropout: the dropout rate of the attention component\n",
    "        proj_dropout: the dropout rate of the final linear projection\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = SelfAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            sr_ratio=sr_ratio,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_dropout=attn_dropout,\n",
    "            proj_dropout=proj_dropout,\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = _MLPMoE(in_feature=embed_dim, mlp_ratio=mlp_ratio, dropout=moe_encoder_dropout, num_experts=num_experts, num_selected=num_selected)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        processed_x, balance_loss, router_z_loss = self.mlp(self.norm2(x))\n",
    "        x = x + processed_x\n",
    "        return x, balance_loss, router_z_loss\n",
    "\n",
    "class MixVisionTransformerMoE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 4,\n",
    "        sr_ratios: list = [8, 4, 2, 1],\n",
    "        embed_dims: list = [64, 128, 320, 512],\n",
    "        patch_kernel_size: list = [7, 3, 3, 3],\n",
    "        patch_stride: list = [4, 2, 2, 2],\n",
    "        patch_padding: list = [3, 1, 1, 1],\n",
    "        mlp_ratios: list = [2, 2, 2, 2],\n",
    "        num_experts_list: list = [4, 4, 6, 8],\n",
    "        experts_selected_list: list = [2, 2, 3, 4],\n",
    "        num_heads: list = [1, 2, 5, 8],\n",
    "        depths: list = [2, 2, 2, 2],\n",
    "        encoder_dropout: float = 0.0,\n",
    "        moe_encoder_dropout: float = 0.0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        in_channels: number of the input channels\n",
    "        img_volume_dim: spatial resolution of the image volume (Depth, Width, Height)\n",
    "        sr_ratios: the rates at which to down sample the sequence length of the embedded patch\n",
    "        embed_dims: hidden size of the PatchEmbedded input\n",
    "        patch_kernel_size: kernel size for the convolution in the patch embedding module\n",
    "        patch_stride: stride for the convolution in the patch embedding module\n",
    "        patch_padding: padding for the convolution in the patch embedding module\n",
    "        mlp_ratio: at which rate increasse the projection dim of the hidden_state in the mlp\n",
    "        num_heads: number of attenion heads\n",
    "        depth: number of attention layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # patch embedding at different Pyramid level\n",
    "        self.embed_1 = PatchEmbedding(\n",
    "            in_channel=in_channels,\n",
    "            embed_dim=embed_dims[0],\n",
    "            kernel_size=patch_kernel_size[0],\n",
    "            stride=patch_stride[0],\n",
    "            padding=patch_padding[0],\n",
    "        )\n",
    "        self.embed_2 = PatchEmbedding(\n",
    "            in_channel=embed_dims[0],\n",
    "            embed_dim=embed_dims[1],\n",
    "            kernel_size=patch_kernel_size[1],\n",
    "            stride=patch_stride[1],\n",
    "            padding=patch_padding[1],\n",
    "        )\n",
    "        self.embed_3 = PatchEmbedding(\n",
    "            in_channel=embed_dims[1],\n",
    "            embed_dim=embed_dims[2],\n",
    "            kernel_size=patch_kernel_size[2],\n",
    "            stride=patch_stride[2],\n",
    "            padding=patch_padding[2],# -- SegFormer3D Encoder -- #\n",
    "        )\n",
    "        self.embed_4 = PatchEmbedding(\n",
    "            in_channel=embed_dims[2],\n",
    "            embed_dim=embed_dims[3],\n",
    "            kernel_size=patch_kernel_size[3],\n",
    "            stride=patch_stride[3],\n",
    "            padding=patch_padding[3],\n",
    "        )\n",
    "\n",
    "        # block 1\n",
    "        self.tf_block1 = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlockMoE(\n",
    "                    embed_dim=embed_dims[0],\n",
    "                    num_heads=num_heads[0],\n",
    "                    mlp_ratio=mlp_ratios[0],\n",
    "                    num_experts=num_experts_list[0],\n",
    "                    num_selected=experts_selected_list[0],\n",
    "                    sr_ratio=sr_ratios[0],\n",
    "                    qkv_bias=True,\n",
    "                    moe_encoder_dropout=moe_encoder_dropout\n",
    "                )\n",
    "                for _ in range(depths[0])\n",
    "            ]\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dims[0])\n",
    "\n",
    "        # block 2\n",
    "        self.tf_block2 = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlockMoE(\n",
    "                    embed_dim=embed_dims[1],\n",
    "                    num_heads=num_heads[1],\n",
    "                    mlp_ratio=mlp_ratios[1],\n",
    "                    num_experts=num_experts_list[1],\n",
    "                    num_selected=experts_selected_list[1],\n",
    "                    sr_ratio=sr_ratios[1],\n",
    "                    qkv_bias=True,\n",
    "                    moe_encoder_dropout=moe_encoder_dropout\n",
    "                )\n",
    "                for _ in range(depths[1])\n",
    "            ]\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dims[1])\n",
    "\n",
    "        # block 3\n",
    "        self.tf_block3 = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlockMoE(\n",
    "                    embed_dim=embed_dims[2],\n",
    "                    num_heads=num_heads[2],\n",
    "                    mlp_ratio=mlp_ratios[2],\n",
    "                    num_experts=num_experts_list[2],\n",
    "                    num_selected=experts_selected_list[2],\n",
    "                    sr_ratio=sr_ratios[2],\n",
    "                    qkv_bias=True,\n",
    "                    moe_encoder_dropout=moe_encoder_dropout\n",
    "                )\n",
    "                for _ in range(depths[2])\n",
    "            ]\n",
    "        )\n",
    "        self.norm3 = nn.LayerNorm(embed_dims[2])\n",
    "\n",
    "        # block 4\n",
    "        self.tf_block4 = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlockMoE(\n",
    "                    embed_dim=embed_dims[3],\n",
    "                    num_heads=num_heads[3],\n",
    "                    mlp_ratio=mlp_ratios[3],\n",
    "                    num_experts=num_experts_list[3],\n",
    "                    num_selected=experts_selected_list[3],\n",
    "                    sr_ratio=sr_ratios[3],\n",
    "                    qkv_bias=True,\n",
    "                    moe_encoder_dropout=moe_encoder_dropout\n",
    "                )\n",
    "                for _ in range(depths[3])\n",
    "            ]\n",
    "        )\n",
    "        self.norm4 = nn.LayerNorm(embed_dims[3])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        # at each stage these are the following mappings:\n",
    "        # (batch_size, num_patches, hidden_state)\n",
    "        # (num_patches,) -> (D, H, W)\n",
    "        # (batch_size, num_patches, hidden_state) -> (batch_size, hidden_state, D, H, W)\n",
    "\n",
    "        # Declare the initial values of the accumulated losses for every transformer block #\n",
    "\n",
    "        # Balance Loss #\n",
    "        total_balance_loss_blk_1 = 0.0\n",
    "        total_balance_loss_blk_2 = 0.0\n",
    "        total_balance_loss_blk_3 = 0.0\n",
    "        total_balance_loss_blk_4 = 0.0\n",
    "\n",
    "        # Router Z Loss #\n",
    "        total_router_z_loss_blk_1 = 0.0\n",
    "        total_router_z_loss_blk_2 = 0.0\n",
    "        total_router_z_loss_blk_3 = 0.0\n",
    "        total_router_z_loss_blk_4 = 0.0\n",
    "\n",
    "\n",
    "        # stage 1\n",
    "        x = self.embed_1(x)\n",
    "        B, N, C = x.shape\n",
    "        n = cube_root(N)\n",
    "        for i, blk in enumerate(self.tf_block1):\n",
    "            x, balance_loss_blk_1, router_z_loss_blk_1 = blk(x)\n",
    "            total_balance_loss_blk_1 += balance_loss_blk_1\n",
    "            total_router_z_loss_blk_1 += router_z_loss_blk_1\n",
    "\n",
    "        x = self.norm1(x)\n",
    "        # (B, N, C) -> (B, D, H, W, C) -> (B, C, D, H, W)\n",
    "        x = x.reshape(B, n, n, n, -1).permute(0, 4, 1, 2, 3).contiguous()\n",
    "        out.append(x)\n",
    "\n",
    "        # stage 2\n",
    "        x = self.embed_2(x)\n",
    "        B, N, C = x.shape\n",
    "        n = cube_root(N)\n",
    "        for i, blk in enumerate(self.tf_block2):\n",
    "            x, balance_loss_blk_2, router_z_loss_blk_2 = blk(x)\n",
    "            total_balance_loss_blk_2 += balance_loss_blk_2\n",
    "            total_router_z_loss_blk_2 += router_z_loss_blk_2\n",
    "        x = self.norm2(x)\n",
    "        # (B, N, C) -> (B, D, H, W, C) -> (B, C, D, H, W)\n",
    "        x = x.reshape(B, n, n, n, -1).permute(0, 4, 1, 2, 3).contiguous()\n",
    "        out.append(x)\n",
    "\n",
    "        # stage 3\n",
    "        x = self.embed_3(x)\n",
    "        B, N, C = x.shape\n",
    "        n = cube_root(N)\n",
    "        for i, blk in enumerate(self.tf_block3):\n",
    "            x, balance_loss_blk_3, router_z_loss_blk_3 = blk(x)\n",
    "            total_balance_loss_blk_3 += balance_loss_blk_3\n",
    "            total_router_z_loss_blk_3 += router_z_loss_blk_3\n",
    "        x = self.norm3(x)\n",
    "        # (B, N, C) -> (B, D, H, W, C) -> (B, C, D, H, W)\n",
    "        x = x.reshape(B, n, n, n, -1).permute(0, 4, 1, 2, 3).contiguous()\n",
    "        out.append(x)\n",
    "\n",
    "        # stage 4\n",
    "        x = self.embed_4(x)\n",
    "        B, N, C = x.shape\n",
    "        n = cube_root(N)\n",
    "        for i, blk in enumerate(self.tf_block4):\n",
    "            x, balance_loss_blk_4, router_z_loss_blk_4 = blk(x)\n",
    "            total_balance_loss_blk_4 += balance_loss_blk_4\n",
    "            total_router_z_loss_blk_4 += router_z_loss_blk_4\n",
    "        x = self.norm4(x)\n",
    "        # (B, N, C) -> (B, D, H, W, C) -> (B, C, D, H, W)\n",
    "        x = x.reshape(B, n, n, n, -1).permute(0, 4, 1, 2, 3).contiguous()\n",
    "        out.append(x)\n",
    "\n",
    "\n",
    "        # Accumulate the various balance and router_z losses from each transformer block\n",
    "        total_balance_loss = total_balance_loss_blk_1 + total_balance_loss_blk_2 + total_balance_loss_blk_3 + total_balance_loss_blk_4\n",
    "        total_router_z_loss = total_router_z_loss_blk_1 + total_router_z_loss_blk_2 + total_router_z_loss_blk_3 + total_router_z_loss_blk_4\n",
    "\n",
    "        # Normalize the accumulated losses to avoid numerical instability and incorrectness\n",
    "        normalized_total_balance_loss = total_balance_loss / 4\n",
    "        normalized_total_router_z_loss = total_router_z_loss / 4\n",
    "\n",
    "\n",
    "        # Attempt at changing the outputs in order to see if I don't break the sliding window inference\n",
    "        return out, normalized_total_balance_loss, normalized_total_router_z_loss\n",
    "\n",
    "\n",
    "\n",
    "class _MLPMoE(nn.Module):\n",
    "    def __init__(self, in_feature, mlp_ratio=2, dropout=0.4, num_experts=8, num_selected=4):\n",
    "        super().__init__()\n",
    "        out_feature = mlp_ratio * in_feature\n",
    "        self.num_experts = num_experts\n",
    "        self.num_selected = num_selected\n",
    "\n",
    "        # Gating network\n",
    "        self.gate = nn.Linear(in_feature, num_experts, bias=False)\n",
    "\n",
    "        # Expert networks (each one replicates your original MLP structure)\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_feature, out_feature),\n",
    "                DWConv(dim=out_feature),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(out_feature, in_feature),\n",
    "                nn.Dropout(dropout),\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, C = x.shape  # (Batch, Sequence Length, Channels)\n",
    "\n",
    "        # Gating logic\n",
    "        gate_logits = self.gate(x)  # (B, L, E)\n",
    "\n",
    "        # Router z-loss\n",
    "        router_z_loss = torch.logsumexp(gate_logits, dim=-1)\n",
    "        router_z_loss = torch.square(router_z_loss).mean()\n",
    "\n",
    "        # Softmax on last dim to get gating probabilities\n",
    "        gate_softmax = F.softmax(gate_logits, dim=-1, dtype=torch.float).to(x.dtype)\n",
    "\n",
    "        # Balance proxy\n",
    "        density_1_proxy = reduce(gate_softmax, '... e -> e', 'mean')\n",
    "\n",
    "        # Top-k expert selection\n",
    "        weights, selected_experts = torch.topk(gate_softmax, self.num_selected, dim=-1)  # (B, L, k)\n",
    "\n",
    "        # One-hot encoding to track expert usage\n",
    "        one_hot_gate_indices = F.one_hot(rearrange(selected_experts, '... k -> k ...'), self.num_experts).float()[0]\n",
    "        density_1 = reduce(one_hot_gate_indices, '... e -> e', 'mean')\n",
    "\n",
    "        # Balance loss for encouraging uniform expert use\n",
    "        balance_loss = (density_1_proxy * density_1).mean() * float(self.num_experts ** 2)\n",
    "\n",
    "        # Normalize weights\n",
    "        weights = weights / torch.sum(weights, dim=-1, keepdim=True).to(x.dtype)\n",
    "\n",
    "        # Prepare output tensor\n",
    "        results = torch.zeros_like(x)\n",
    "\n",
    "        # Route each token to its selected experts\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            for b in range(B):  # batch loop\n",
    "                token_idx, nth_expert = torch.where(selected_experts[b] == i)\n",
    "\n",
    "                if token_idx.numel() == 0:\n",
    "                    continue\n",
    "\n",
    "                input_to_expert = x[b][token_idx]  # shape: (N, C)\n",
    "\n",
    "                # Reshape to match expert input expectation\n",
    "                if input_to_expert.dim() == 1:\n",
    "                    input_to_expert = input_to_expert.unsqueeze(0).unsqueeze(0)\n",
    "                elif input_to_expert.dim() == 2:\n",
    "                    input_to_expert = input_to_expert.unsqueeze(0)  # shape: (1, N, C)\n",
    "\n",
    "                output = expert(input_to_expert)  # shape: (1, N, C)\n",
    "                output = output.squeeze(0)  # shape: (N, C)\n",
    "\n",
    "                # Weighted addition to output\n",
    "                results[b][token_idx] += weights[b][token_idx, nth_expert, None] * output\n",
    "    \n",
    "        return results, balance_loss, router_z_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DWConv(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super().__init__()\n",
    "        self.dwconv_1d = nn.Conv1d(dim, dim, kernel_size=3, stride=1, padding=1, groups=dim)\n",
    "        self.dwconv_3d = nn.Conv3d(dim, dim, kernel_size=3, stride=1, padding=1, groups=dim)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        n = round(N ** (1 / 3))\n",
    "\n",
    "        if n ** 3 == N:\n",
    "            x = x.transpose(1, 2).view(B, C, n, n, n)\n",
    "            x = self.dwconv_3d(x)\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "        else:\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.dwconv_1d(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def cube_root(n):\n",
    "    return round(math.pow(n, (1 / 3)))\n",
    "    \n",
    "\n",
    "\n",
    "# TODO ---> Adapt the MLPMoE layer to replace the MLP_ class !!!!! <--------\n",
    "\n",
    "# ----------------------------------------------------- decoder -------------------\n",
    "class MLPMoE(nn.Module):\n",
    "    def __init__(self, num_experts, num_selected, mm_channels, channels, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.num_selected = num_selected\n",
    "        self.mm_channels = mm_channels\n",
    "        self.channels = channels\n",
    "\n",
    "        self.gate = nn.Linear(mm_channels, num_experts, bias=False)\n",
    "        self.num_selected = num_selected\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(mm_channels, channels),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(channels, channels)\n",
    "                    ) for _ in range(num_experts)\n",
    "                    ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape [B, N, C] — same as output from MLP_ after flattening\n",
    "        Returns: Tensor of shape [B, N, channels]\n",
    "        \"\"\"\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        gate_logits = self.gate(x)  # [B, N, num_experts]\n",
    "        router_z_loss = torch.logsumexp(gate_logits, dim=-1)\n",
    "        router_z_loss = torch.square(router_z_loss).mean()\n",
    "\n",
    "        gate_softmax = F.softmax(gate_logits, dim=-1, dtype=torch.float).to(x.dtype)\n",
    "\n",
    "        density_1_proxy = gate_softmax.mean(dim=(0, 1))  # [num_experts]\n",
    "        weights, selected_experts = torch.topk(gate_softmax, self.num_selected, dim=-1)  # [B, N, K]\n",
    "\n",
    "        one_hot_gate_indices = F.one_hot(selected_experts, self.num_experts).float()  # [B, N, K, E]\n",
    "        density_1 = one_hot_gate_indices.mean(dim=(0, 1))  # [num_experts]\n",
    "        balance_loss = (density_1_proxy * density_1).mean() * float(self.num_experts ** 2)\n",
    "\n",
    "        # [B, N, C] → process each token independently\n",
    "        output = torch.zeros(B, N, self.channels, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            # Get mask of tokens that routed to expert i (anywhere in top-k)\n",
    "            mask = (selected_experts == i)  # [B, N, K]\n",
    "            token_mask = mask.any(dim=-1)  # [B, N]\n",
    "\n",
    "            for b in range(B):\n",
    "                selected = token_mask[b]  # [N], bool\n",
    "                if selected.any():\n",
    "                    expert_input = x[b][selected]  # [T_i, C]\n",
    "                    expert_output = expert(expert_input)  # [T_i, channels]\n",
    "\n",
    "                    # distribute outputs by averaging across k if multiple\n",
    "                    weight_sum = weights[b][selected]  # [T_i, K]\n",
    "                    weight_sum = weight_sum[mask[b][selected]].view(-1, 1)  # [T_i*K_used, 1]\n",
    "\n",
    "                    output[b][selected] += expert_output * weight_sum.mean(dim=1, keepdim=True)\n",
    "\n",
    "        return output, balance_loss, router_z_loss\n",
    "\n",
    "\n",
    "\n",
    "class SegFormerDecoderHeadMoE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_feature_dims: list = [512, 320, 128, 64],\n",
    "        decoder_head_embedding_dim: int = 256,\n",
    "        num_classes: int = 3,\n",
    "        dropout: float = 0.0,\n",
    "        num_experts: int = 4,\n",
    "        num_selected: int = 2,\n",
    "        moe_decoder_dropout: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_c4 = MLPMoE(num_experts, num_selected, input_feature_dims[0], decoder_head_embedding_dim, num_layers=2, dropout=moe_decoder_dropout)\n",
    "        self.linear_c3 = MLPMoE(num_experts, num_selected, input_feature_dims[1], decoder_head_embedding_dim, num_layers=2, dropout=moe_decoder_dropout)\n",
    "        self.linear_c2 = MLPMoE(num_experts, num_selected, input_feature_dims[2], decoder_head_embedding_dim, num_layers=2, dropout=moe_decoder_dropout)\n",
    "        self.linear_c1 = MLPMoE(num_experts, num_selected, input_feature_dims[3], decoder_head_embedding_dim, num_layers=2, dropout=moe_decoder_dropout)\n",
    "\n",
    "        self.linear_fuse = nn.Sequential(\n",
    "            nn.Conv3d(4 * decoder_head_embedding_dim, decoder_head_embedding_dim, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm3d(decoder_head_embedding_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_pred = nn.Conv3d(decoder_head_embedding_dim, num_classes, kernel_size=1)\n",
    "        self.upsample_volume = nn.Upsample(scale_factor=4.0, mode=\"trilinear\", align_corners=False)\n",
    "\n",
    "    def forward(self, c1, c2, c3, c4):\n",
    "        n, _, d, h, w = c4.shape\n",
    "\n",
    "        moe_outputs = []\n",
    "        moe_losses = []\n",
    "\n",
    "        def process_moe_input(cX, moe_layer):\n",
    "            B, C, D, H, W = cX.shape\n",
    "            x = cX.flatten(2).transpose(1, 2)  # → [B, N, C]\n",
    "            out, bal_loss, z_loss = moe_layer(x)\n",
    "            out = out.transpose(1, 2).reshape(B, -1, D, H, W)  # back to [B, C, D, H, W]\n",
    "            return out, bal_loss, z_loss\n",
    "\n",
    "\n",
    "        _c4, bal4, z4 = process_moe_input(c4, self.linear_c4)\n",
    "        _c4 = F.interpolate(_c4, size=c1.shape[2:], mode=\"trilinear\", align_corners=False)\n",
    "\n",
    "        _c3, bal3, z3 = process_moe_input(c3, self.linear_c3)\n",
    "        _c3 = F.interpolate(_c3, size=c1.shape[2:], mode=\"trilinear\", align_corners=False)\n",
    "\n",
    "        _c2, bal2, z2 = process_moe_input(c2, self.linear_c2)\n",
    "        _c2 = F.interpolate(_c2, size=c1.shape[2:], mode=\"trilinear\", align_corners=False)\n",
    "\n",
    "        _c1, bal1, z1 = process_moe_input(c1, self.linear_c1)\n",
    "\n",
    "        _c = self.linear_fuse(torch.cat([_c4, _c3, _c2, _c1], dim=1))\n",
    "\n",
    "        x = self.dropout(_c)\n",
    "        x = self.linear_pred(x)\n",
    "        x = self.upsample_volume(x)\n",
    "\n",
    "        total_balance_loss = bal1 + bal2 + bal3 + bal4\n",
    "        total_router_z_loss = z1 + z2 + z3 + z4\n",
    "\n",
    "        total_balance_loss = total_balance_loss / 4\n",
    "        total_router_z_loss = total_router_z_loss / 4\n",
    "\n",
    "        return x, total_balance_loss, total_router_z_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dd8523-4247-4a72-b92c-08b2dbe56856",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Instantiation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed6e537-5624-423a-8645-e10b1d269c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "segformer3d = SegFormer3D(\n",
    "    in_channels=3,\n",
    "    sr_ratios=[4, 2, 1, 1],\n",
    "    embed_dims=[32, 64, 160, 256],\n",
    "    patch_kernel_size=[7, 3, 3, 3],\n",
    "    patch_stride=[4, 2, 2, 2],\n",
    "    patch_padding=[3, 1, 1, 1],\n",
    "    mlp_ratios=[4, 4, 4, 4],\n",
    "    num_heads=[1, 2, 5, 8],\n",
    "    depths=[2, 2, 2, 2],\n",
    "    decoder_head_embedding_dim=256,\n",
    "    num_classes=1,\n",
    "    decoder_dropout=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebf1babf-da8a-489f-96ec-4c84293f1ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "segformer3dmoe = SegFormer3DMoE(\n",
    "    in_channels = 3,\n",
    "    sr_ratios = [4, 2, 1, 1],\n",
    "    embed_dims = [32, 64, 160, 256],\n",
    "    patch_kernel_size = [7, 3, 3, 3],\n",
    "    patch_stride = [4, 2, 2, 2],\n",
    "    patch_padding = [3, 1, 1, 1],\n",
    "    mlp_ratios = [4, 4, 4, 4],\n",
    "    num_experts_list = [2, 2, 4, 4],\n",
    "    experts_selected_list = [1, 1, 2, 2],\n",
    "    num_heads = [1, 2, 5, 8],\n",
    "    depths = [2, 2, 2, 2],\n",
    "    decoder_head_embedding_dim = 256,\n",
    "    num_classes = 1,\n",
    "    encoder_dropout = 0.05,\n",
    "    decoder_dropout = 0.05,\n",
    "    moe_dropout_encoder = 0.05,\n",
    "    moe_dropout_decoder = 0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda25242-80f9-4635-889c-142ecf4ff9d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load the Trained Models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9ae0dfd-9001-450c-abdb-b68d77a73f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For better display\n",
    "import numpy as np\n",
    "\n",
    "def load_trained_model(model_class, checkpoint_path):\n",
    "    model = model_class\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fac18286-ca29-4928-9cda-0a21984f6c49",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SegFormer3D:\n\tMissing key(s) in state_dict: \"segformer_encoder.tf_block1.0.mlp.dwconv.dwconv_1d.weight\", \"segformer_encoder.tf_block1.0.mlp.dwconv.dwconv_1d.bias\", \"segformer_encoder.tf_block1.0.mlp.dwconv.dwconv_3d.weight\", \"segformer_encoder.tf_block1.0.mlp.dwconv.dwconv_3d.bias\", \"segformer_encoder.tf_block1.0.mlp.dwconv.norm.weight\", \"segformer_encoder.tf_block1.0.mlp.dwconv.norm.bias\", \"segformer_encoder.tf_block1.1.mlp.dwconv.dwconv_1d.weight\", \"segformer_encoder.tf_block1.1.mlp.dwconv.dwconv_1d.bias\", \"segformer_encoder.tf_block1.1.mlp.dwconv.dwconv_3d.weight\", \"segformer_encoder.tf_block1.1.mlp.dwconv.dwconv_3d.bias\", \"segformer_encoder.tf_block1.1.mlp.dwconv.norm.weight\", \"segformer_encoder.tf_block1.1.mlp.dwconv.norm.bias\", \"segformer_encoder.tf_block2.0.mlp.dwconv.dwconv_1d.weight\", \"segformer_encoder.tf_block2.0.mlp.dwconv.dwconv_1d.bias\", \"segformer_encoder.tf_block2.0.mlp.dwconv.dwconv_3d.weight\", \"segformer_encoder.tf_block2.0.mlp.dwconv.dwconv_3d.bias\", \"segformer_encoder.tf_block2.0.mlp.dwconv.norm.weight\", \"segformer_encoder.tf_block2.0.mlp.dwconv.norm.bias\", \"segformer_encoder.tf_block2.1.mlp.dwconv.dwconv_1d.weight\", \"segformer_encoder.tf_block2.1.mlp.dwconv.dwconv_1d.bias\", \"segformer_encoder.tf_block2.1.mlp.dwconv.dwconv_3d.weight\", \"segformer_encoder.tf_block2.1.mlp.dwconv.dwconv_3d.bias\", \"segformer_encoder.tf_block2.1.mlp.dwconv.norm.weight\", \"segformer_encoder.tf_block2.1.mlp.dwconv.norm.bias\", \"segformer_encoder.tf_block3.0.mlp.dwconv.dwconv_1d.weight\", \"segformer_encoder.tf_block3.0.mlp.dwconv.dwconv_1d.bias\", \"segformer_encoder.tf_block3.0.mlp.dwconv.dwconv_3d.weight\", \"segformer_encoder.tf_block3.0.mlp.dwconv.dwconv_3d.bias\", \"segformer_encoder.tf_block3.0.mlp.dwconv.norm.weight\", \"segformer_encoder.tf_block3.0.mlp.dwconv.norm.bias\", \"segformer_encoder.tf_block3.1.mlp.dwconv.dwconv_1d.weight\", \"segformer_encoder.tf_block3.1.mlp.dwconv.dwconv_1d.bias\", \"segformer_encoder.tf_block3.1.mlp.dwconv.dwconv_3d.weight\", \"segformer_encoder.tf_block3.1.mlp.dwconv.dwconv_3d.bias\", \"segformer_encoder.tf_block3.1.mlp.dwconv.norm.weight\", \"segformer_encoder.tf_block3.1.mlp.dwconv.norm.bias\", \"segformer_encoder.tf_block4.0.mlp.dwconv.dwconv_1d.weight\", \"segformer_encoder.tf_block4.0.mlp.dwconv.dwconv_1d.bias\", \"segformer_encoder.tf_block4.0.mlp.dwconv.dwconv_3d.weight\", \"segformer_encoder.tf_block4.0.mlp.dwconv.dwconv_3d.bias\", \"segformer_encoder.tf_block4.0.mlp.dwconv.norm.weight\", \"segformer_encoder.tf_block4.0.mlp.dwconv.norm.bias\", \"segformer_encoder.tf_block4.1.mlp.dwconv.dwconv_1d.weight\", \"segformer_encoder.tf_block4.1.mlp.dwconv.dwconv_1d.bias\", \"segformer_encoder.tf_block4.1.mlp.dwconv.dwconv_3d.weight\", \"segformer_encoder.tf_block4.1.mlp.dwconv.dwconv_3d.bias\", \"segformer_encoder.tf_block4.1.mlp.dwconv.norm.weight\", \"segformer_encoder.tf_block4.1.mlp.dwconv.norm.bias\". \n\tUnexpected key(s) in state_dict: \"segformer_encoder.tf_block1.0.mlp.dwconv.dwconv.weight\", \"segformer_encoder.tf_block1.0.mlp.dwconv.dwconv.bias\", \"segformer_encoder.tf_block1.0.mlp.dwconv.bn.weight\", \"segformer_encoder.tf_block1.0.mlp.dwconv.bn.bias\", \"segformer_encoder.tf_block1.0.mlp.dwconv.bn.running_mean\", \"segformer_encoder.tf_block1.0.mlp.dwconv.bn.running_var\", \"segformer_encoder.tf_block1.0.mlp.dwconv.bn.num_batches_tracked\", \"segformer_encoder.tf_block1.1.mlp.dwconv.dwconv.weight\", \"segformer_encoder.tf_block1.1.mlp.dwconv.dwconv.bias\", \"segformer_encoder.tf_block1.1.mlp.dwconv.bn.weight\", \"segformer_encoder.tf_block1.1.mlp.dwconv.bn.bias\", \"segformer_encoder.tf_block1.1.mlp.dwconv.bn.running_mean\", \"segformer_encoder.tf_block1.1.mlp.dwconv.bn.running_var\", \"segformer_encoder.tf_block1.1.mlp.dwconv.bn.num_batches_tracked\", \"segformer_encoder.tf_block2.0.mlp.dwconv.dwconv.weight\", \"segformer_encoder.tf_block2.0.mlp.dwconv.dwconv.bias\", \"segformer_encoder.tf_block2.0.mlp.dwconv.bn.weight\", \"segformer_encoder.tf_block2.0.mlp.dwconv.bn.bias\", \"segformer_encoder.tf_block2.0.mlp.dwconv.bn.running_mean\", \"segformer_encoder.tf_block2.0.mlp.dwconv.bn.running_var\", \"segformer_encoder.tf_block2.0.mlp.dwconv.bn.num_batches_tracked\", \"segformer_encoder.tf_block2.1.mlp.dwconv.dwconv.weight\", \"segformer_encoder.tf_block2.1.mlp.dwconv.dwconv.bias\", \"segformer_encoder.tf_block2.1.mlp.dwconv.bn.weight\", \"segformer_encoder.tf_block2.1.mlp.dwconv.bn.bias\", \"segformer_encoder.tf_block2.1.mlp.dwconv.bn.running_mean\", \"segformer_encoder.tf_block2.1.mlp.dwconv.bn.running_var\", \"segformer_encoder.tf_block2.1.mlp.dwconv.bn.num_batches_tracked\", \"segformer_encoder.tf_block3.0.mlp.dwconv.dwconv.weight\", \"segformer_encoder.tf_block3.0.mlp.dwconv.dwconv.bias\", \"segformer_encoder.tf_block3.0.mlp.dwconv.bn.weight\", \"segformer_encoder.tf_block3.0.mlp.dwconv.bn.bias\", \"segformer_encoder.tf_block3.0.mlp.dwconv.bn.running_mean\", \"segformer_encoder.tf_block3.0.mlp.dwconv.bn.running_var\", \"segformer_encoder.tf_block3.0.mlp.dwconv.bn.num_batches_tracked\", \"segformer_encoder.tf_block3.1.mlp.dwconv.dwconv.weight\", \"segformer_encoder.tf_block3.1.mlp.dwconv.dwconv.bias\", \"segformer_encoder.tf_block3.1.mlp.dwconv.bn.weight\", \"segformer_encoder.tf_block3.1.mlp.dwconv.bn.bias\", \"segformer_encoder.tf_block3.1.mlp.dwconv.bn.running_mean\", \"segformer_encoder.tf_block3.1.mlp.dwconv.bn.running_var\", \"segformer_encoder.tf_block3.1.mlp.dwconv.bn.num_batches_tracked\", \"segformer_encoder.tf_block4.0.mlp.dwconv.dwconv.weight\", \"segformer_encoder.tf_block4.0.mlp.dwconv.dwconv.bias\", \"segformer_encoder.tf_block4.0.mlp.dwconv.bn.weight\", \"segformer_encoder.tf_block4.0.mlp.dwconv.bn.bias\", \"segformer_encoder.tf_block4.0.mlp.dwconv.bn.running_mean\", \"segformer_encoder.tf_block4.0.mlp.dwconv.bn.running_var\", \"segformer_encoder.tf_block4.0.mlp.dwconv.bn.num_batches_tracked\", \"segformer_encoder.tf_block4.1.mlp.dwconv.dwconv.weight\", \"segformer_encoder.tf_block4.1.mlp.dwconv.dwconv.bias\", \"segformer_encoder.tf_block4.1.mlp.dwconv.bn.weight\", \"segformer_encoder.tf_block4.1.mlp.dwconv.bn.bias\", \"segformer_encoder.tf_block4.1.mlp.dwconv.bn.running_mean\", \"segformer_encoder.tf_block4.1.mlp.dwconv.bn.running_var\", \"segformer_encoder.tf_block4.1.mlp.dwconv.bn.num_batches_tracked\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the trained model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m loaded_segformer3d = \u001b[43mload_trained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegformer3d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./model_checkpoints/segformer3d_best_model_5.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mload_trained_model\u001b[39m\u001b[34m(model_class, checkpoint_path)\u001b[39m\n\u001b[32m      9\u001b[39m model = model_class\n\u001b[32m     10\u001b[39m checkpoint = torch.load(checkpoint_path, map_location=torch.device(\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_state_dict\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m model.eval()\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/torch/nn/modules/module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for SegFormer3D:\n\tMissing key(s) in state_dict: \"segformer_encoder.tf_block1.0.mlp.dwconv.dwconv_1d.weight\", \"segformer_encoder.tf_block1.0.mlp.dwconv.dwconv_1d.bias\", \"segformer_encoder.tf_block1.0.mlp.dwconv.dwconv_3d.weight\", \"segformer_encoder.tf_block1.0.mlp.dwconv.dwconv_3d.bias\", \"segformer_encoder.tf_block1.0.mlp.dwconv.norm.weight\", \"segformer_encoder.tf_block1.0.mlp.dwconv.norm.bias\", \"segformer_encoder.tf_block1.1.mlp.dwconv.dwconv_1d.weight\", \"segformer_encoder.tf_block1.1.mlp.dwconv.dwconv_1d.bias\", \"segformer_encoder.tf_block1.1.mlp.dwconv.dwconv_3d.weight\", \"segformer_encoder.tf_block1.1.mlp.dwconv.dwconv_3d.bias\", \"segformer_encoder.tf_block1.1.mlp.dwconv.norm.weight\", \"segformer_encoder.tf_block1.1.mlp.dwconv.norm.bias\", \"segformer_encoder.tf_block2.0.mlp.dwconv.dwconv_1d.weight\", \"segformer_encoder.tf_block2.0.mlp.dwconv.dwconv_1d.bias\", \"segformer_encoder.tf_block2.0.mlp.dwconv.dwconv_3d.weight\", \"segformer_encoder.tf_block2.0.mlp.dwconv.dwconv_3d.bias\", \"segformer_encoder.tf_block2.0.mlp.dwconv.norm.weight\", \"segformer_encoder.tf_block2.0.mlp.dwconv.norm.bias\", \"segformer_encoder.tf_block2.1.mlp.dwconv.dwconv_1d.weight\", \"segformer_encoder.tf_block2.1.mlp.dwconv.dwconv_1d.bias\", \"segformer_encoder.tf_block2.1.mlp.dwconv.dwconv_3d.weight\", \"segformer_encoder.tf_block2.1.mlp.dwconv.dwconv_3d.bias\", \"segformer_encoder.tf_block2.1.mlp.dwconv.norm.weight\", \"segformer_encoder.tf_block2.1.mlp.dwconv.norm.bias\", \"segformer_encoder.tf_block3.0.mlp.dwconv.dwconv_1d.weight\", \"segformer_encoder.tf_block3.0.mlp.dwconv.dwconv_1d.bias\", \"segformer_encoder.tf_block3.0.mlp.dwconv.dwconv_3d.weight\", \"segformer_encoder.tf_block3.0.mlp.dwconv.dwconv_3d.bias\", \"segformer_encoder.tf_block3.0.mlp.dwconv.norm.weight\", \"segformer_encoder.tf_block3.0.mlp.dwconv.norm.bias\", \"segformer_encoder.tf_block3.1.mlp.dwconv.dwconv_1d.weight\", \"segformer_encoder.tf_block3.1.mlp.dwconv.dwconv_1d.bias\", \"segformer_encoder.tf_block3.1.mlp.dwconv.dwconv_3d.weight\", \"segformer_encoder.tf_block3.1.mlp.dwconv.dwconv_3d.bias\", \"segformer_encoder.tf_block3.1.mlp.dwconv.norm.weight\", \"segformer_encoder.tf_block3.1.mlp.dwconv.norm.bias\", \"segformer_encoder.tf_block4.0.mlp.dwconv.dwconv_1d.weight\", \"segformer_encoder.tf_block4.0.mlp.dwconv.dwconv_1d.bias\", \"segformer_encoder.tf_block4.0.mlp.dwconv.dwconv_3d.weight\", \"segformer_encoder.tf_block4.0.mlp.dwconv.dwconv_3d.bias\", \"segformer_encoder.tf_block4.0.mlp.dwconv.norm.weight\", \"segformer_encoder.tf_block4.0.mlp.dwconv.norm.bias\", \"segformer_encoder.tf_block4.1.mlp.dwconv.dwconv_1d.weight\", \"segformer_encoder.tf_block4.1.mlp.dwconv.dwconv_1d.bias\", \"segformer_encoder.tf_block4.1.mlp.dwconv.dwconv_3d.weight\", \"segformer_encoder.tf_block4.1.mlp.dwconv.dwconv_3d.bias\", \"segformer_encoder.tf_block4.1.mlp.dwconv.norm.weight\", \"segformer_encoder.tf_block4.1.mlp.dwconv.norm.bias\". \n\tUnexpected key(s) in state_dict: \"segformer_encoder.tf_block1.0.mlp.dwconv.dwconv.weight\", \"segformer_encoder.tf_block1.0.mlp.dwconv.dwconv.bias\", \"segformer_encoder.tf_block1.0.mlp.dwconv.bn.weight\", \"segformer_encoder.tf_block1.0.mlp.dwconv.bn.bias\", \"segformer_encoder.tf_block1.0.mlp.dwconv.bn.running_mean\", \"segformer_encoder.tf_block1.0.mlp.dwconv.bn.running_var\", \"segformer_encoder.tf_block1.0.mlp.dwconv.bn.num_batches_tracked\", \"segformer_encoder.tf_block1.1.mlp.dwconv.dwconv.weight\", \"segformer_encoder.tf_block1.1.mlp.dwconv.dwconv.bias\", \"segformer_encoder.tf_block1.1.mlp.dwconv.bn.weight\", \"segformer_encoder.tf_block1.1.mlp.dwconv.bn.bias\", \"segformer_encoder.tf_block1.1.mlp.dwconv.bn.running_mean\", \"segformer_encoder.tf_block1.1.mlp.dwconv.bn.running_var\", \"segformer_encoder.tf_block1.1.mlp.dwconv.bn.num_batches_tracked\", \"segformer_encoder.tf_block2.0.mlp.dwconv.dwconv.weight\", \"segformer_encoder.tf_block2.0.mlp.dwconv.dwconv.bias\", \"segformer_encoder.tf_block2.0.mlp.dwconv.bn.weight\", \"segformer_encoder.tf_block2.0.mlp.dwconv.bn.bias\", \"segformer_encoder.tf_block2.0.mlp.dwconv.bn.running_mean\", \"segformer_encoder.tf_block2.0.mlp.dwconv.bn.running_var\", \"segformer_encoder.tf_block2.0.mlp.dwconv.bn.num_batches_tracked\", \"segformer_encoder.tf_block2.1.mlp.dwconv.dwconv.weight\", \"segformer_encoder.tf_block2.1.mlp.dwconv.dwconv.bias\", \"segformer_encoder.tf_block2.1.mlp.dwconv.bn.weight\", \"segformer_encoder.tf_block2.1.mlp.dwconv.bn.bias\", \"segformer_encoder.tf_block2.1.mlp.dwconv.bn.running_mean\", \"segformer_encoder.tf_block2.1.mlp.dwconv.bn.running_var\", \"segformer_encoder.tf_block2.1.mlp.dwconv.bn.num_batches_tracked\", \"segformer_encoder.tf_block3.0.mlp.dwconv.dwconv.weight\", \"segformer_encoder.tf_block3.0.mlp.dwconv.dwconv.bias\", \"segformer_encoder.tf_block3.0.mlp.dwconv.bn.weight\", \"segformer_encoder.tf_block3.0.mlp.dwconv.bn.bias\", \"segformer_encoder.tf_block3.0.mlp.dwconv.bn.running_mean\", \"segformer_encoder.tf_block3.0.mlp.dwconv.bn.running_var\", \"segformer_encoder.tf_block3.0.mlp.dwconv.bn.num_batches_tracked\", \"segformer_encoder.tf_block3.1.mlp.dwconv.dwconv.weight\", \"segformer_encoder.tf_block3.1.mlp.dwconv.dwconv.bias\", \"segformer_encoder.tf_block3.1.mlp.dwconv.bn.weight\", \"segformer_encoder.tf_block3.1.mlp.dwconv.bn.bias\", \"segformer_encoder.tf_block3.1.mlp.dwconv.bn.running_mean\", \"segformer_encoder.tf_block3.1.mlp.dwconv.bn.running_var\", \"segformer_encoder.tf_block3.1.mlp.dwconv.bn.num_batches_tracked\", \"segformer_encoder.tf_block4.0.mlp.dwconv.dwconv.weight\", \"segformer_encoder.tf_block4.0.mlp.dwconv.dwconv.bias\", \"segformer_encoder.tf_block4.0.mlp.dwconv.bn.weight\", \"segformer_encoder.tf_block4.0.mlp.dwconv.bn.bias\", \"segformer_encoder.tf_block4.0.mlp.dwconv.bn.running_mean\", \"segformer_encoder.tf_block4.0.mlp.dwconv.bn.running_var\", \"segformer_encoder.tf_block4.0.mlp.dwconv.bn.num_batches_tracked\", \"segformer_encoder.tf_block4.1.mlp.dwconv.dwconv.weight\", \"segformer_encoder.tf_block4.1.mlp.dwconv.dwconv.bias\", \"segformer_encoder.tf_block4.1.mlp.dwconv.bn.weight\", \"segformer_encoder.tf_block4.1.mlp.dwconv.bn.bias\", \"segformer_encoder.tf_block4.1.mlp.dwconv.bn.running_mean\", \"segformer_encoder.tf_block4.1.mlp.dwconv.bn.running_var\", \"segformer_encoder.tf_block4.1.mlp.dwconv.bn.num_batches_tracked\". "
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "loaded_segformer3d = load_trained_model(segformer3d, \"./model_checkpoints/segformer3d_best_model_5.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d61d43-f9e8-4192-ae69-c1e5fee92ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAMPlusPlus\n",
    "from pytorch_grad_cam.utils.model_targets import SemanticSegmentationTarget\n",
    "\n",
    "target_layers = [model.encoder.blocks[-1].norm2]\n",
    "cam = GradCAMPlusPlus(model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available())\n",
    "\n",
    "targets = [SemanticSegmentationTarget(target_class, 112, 112)]\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0, :]\n",
    "\n",
    "plt.imshow(grayscale_cam, cmap='jet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

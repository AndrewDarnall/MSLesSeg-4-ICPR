{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15b66f1a-394d-4b5b-ab06-ea2774295e3c",
   "metadata": {},
   "source": [
    "# U-Net 4 Multiple Sclerosis Lesion Segmentation - ICPR Challenge\n",
    "### Authors: Andrew R. Darnall, Giovanni Spadaro @ UniCT\n",
    "---\n",
    "\n",
    "## üéØ Competition Objective: MS Lesion Segmentation\n",
    "\n",
    "The central goal of this competition is the **automatic segmentation of Multiple Sclerosis (MS) lesions** using **multi-modal MRI data** and **deep learning algorithms**.\n",
    "\n",
    "### üß™ Provided Data\n",
    "Participants were given:\n",
    "- **MRI scans** in three modalities:\n",
    "  - **FLAIR**\n",
    "  - **T1-weighted (T1-w)**\n",
    "  - **T2-weighted (T2-w)**\n",
    "- **Ground-truth segmentation masks**, which are:\n",
    "  - **Binary masks**:  \n",
    "    - **White pixels** ‚Üí MS lesion regions  \n",
    "    - **Black pixels** ‚Üí Background\n",
    "\n",
    "### üß† Task Description\n",
    "- Participants could use **any or all modalities**, along with the ground-truth labels, to:\n",
    "  - Develop **deep learning-based models** for **automatic lesion segmentation**\n",
    "- MS lesions appear as **irregular clusters of pixels** with **high variability in size and shape**\n",
    "- These lesions are often **difficult to detect** via visual inspection, requiring **expert-level interpretation**\n",
    "\n",
    "The ultimate goal is to create **fully automated segmentation pipelines** that can robustly identify and delineate MS lesions from raw MRI data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919db79e-a3bd-4896-89a1-1d9e0017be10",
   "metadata": {},
   "source": [
    "## üß† MSLesSeg Dataset Overview\n",
    "\n",
    "As part of this competition, participants were provided with the **MSLesSeg Dataset** ‚Äî a **comprehensively annotated, multi-modal MRI dataset** designed for advancing **lesion segmentation** research in medical imaging.\n",
    "\n",
    "### üìä Dataset Composition\n",
    "- **Total Patients:** 75 (48 women, 27 men)  \n",
    "- **Age Range:** 18‚Äì59 years (Mean: 37 ¬± 10.3 years)  \n",
    "- **Longitudinal Timepoints:**  \n",
    "  - 50 patients with 1 timepoint  \n",
    "  - 15 patients with 2 timepoints  \n",
    "  - 5 patients with 3 timepoints  \n",
    "  - 5 patients with 4 timepoints  \n",
    "- **Time Interval Between Scans:** ~1.27 ¬± 0.62 years  \n",
    "- **Total MRI Series:** 115\n",
    "\n",
    "### üß¨ Imaging Modalities\n",
    "Each timepoint includes **three core MRI modalities**:\n",
    "- **T1-weighted (T1-w)**\n",
    "- **T2-weighted (T2-w)**\n",
    "- **FLAIR (Fluid-Attenuated Inversion Recovery)**\n",
    "\n",
    "### üßë‚Äç‚öïÔ∏è Expert Annotation\n",
    "- Lesions were **manually annotated** by clinical experts.\n",
    "- **FLAIR sequences** were the primary reference for lesion labeling.\n",
    "- **T1-w and T2-w** scans supported **multi-contrast lesion characterization**.\n",
    "\n",
    "### üß™ Dataset Splits\n",
    "- **Training Set:** 53 scans  \n",
    "- **Test Set:** 22 scans  \n",
    "\n",
    "### ‚úÖ Ethical Compliance\n",
    "- **Ethical approval** was obtained from the corresponding Hospital Ethics Committee.\n",
    "- **Informed consent** was acquired from all participating patients.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b3a0a-2f43-44f9-a898-28f1fa46b75a",
   "metadata": {},
   "source": [
    "# The Experiment\n",
    "\n",
    "Below is the code used for the:\n",
    "\n",
    "1) Preprocessing of the ***Brain MRI*** scans\n",
    "2) Definition of Dataset, Dataloader and LihgtningDataModule classes\n",
    "3) ***U-Net*** architecture\n",
    "4) ***PyTorch Lightning*** Trainer\n",
    "5) Training & Evaluation\n",
    "6) Model Exaplainability with the post-hoc method ***GradCam++***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293bc032-fddb-44a7-8db3-cd8bdf0315e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from skimage.transform import resize\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13613113-a191-4399-9b42-9f6df49bc6e9",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Preprocessing & Annotation Workflow\n",
    "\n",
    "The MSLesSeg dataset underwent a **comprehensive preprocessing pipeline** and **expert-driven manual annotation** to ensure **standardization** and **label quality** for downstream MS lesion segmentation tasks.\n",
    "\n",
    "### üßº Preprocessing Pipeline\n",
    "1. **Anonymization** of all MRI scans to protect patient privacy.\n",
    "2. **DICOM to NIfTI conversion**, leveraging NIfTI's wide adoption in neuroimaging.\n",
    "3. **Co-registration to the MNI152 1mm¬≥ isotropic template** using **FLIRT** (FMRIB‚Äôs Linear Image Registration Tool), ensuring all scans are aligned to a **common anatomical space**.\n",
    "4. **Brain extraction** via **BET** (Brain Extraction Tool) to remove non-brain tissues and isolate relevant structures.\n",
    "\n",
    "This pipeline guarantees that all images are **standardized** and **aligned**, which is critical for **automated MS lesion segmentation algorithms**.\n",
    "\n",
    "---\n",
    "\n",
    "### üñãÔ∏è Ground-Truth Annotation Protocol\n",
    "- Lesions were **manually segmented** on the **FLAIR modality** for each patient and timepoint.\n",
    "- **T1-w and T2-w** modalities were used to **cross-validate ambiguous cases**.\n",
    "- Annotation was conducted by a **trained junior rater**, under supervision of:\n",
    "  - A **senior neuroradiologist**\n",
    "  - A **senior neurologist**\n",
    "- Annotation sessions included:\n",
    "  - Multiple **training meetings** to establish a **consistent segmentation strategy**\n",
    "  - Use of **JIM9** ‚Äî a high-end tool for **medical image segmentation and analysis**\n",
    "  - Regular **expert validation checkpoints** to ensure consistency and accuracy\n",
    "\n",
    "The final masks, reviewed and approved by senior experts, are considered the **gold-standard ground truth**.\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ Key Annotation Highlights\n",
    "- **Independent segmentation** for each patient/timepoint to avoid bias\n",
    "- Conducted on **FLAIR scans registered to MNI space**\n",
    "- **Validated ground-truth masks** ready for training and evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2027badd-35e2-42ac-b423-e85e9e99d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nifti(file_path):\n",
    "    return nib.load(file_path).get_fdata()\n",
    "\n",
    "def normalize_intensity(img):\n",
    "    # Normalize each modality independently (z-score)\n",
    "    img = img.astype(np.float32)\n",
    "    img = (img - np.mean(img)) / (np.std(img) + 1e-5)\n",
    "    return img\n",
    "\n",
    "def compute_distance_label(mask):\n",
    "    mask = mask.astype(bool)\n",
    "    pos_dist = distance_transform_edt(mask)\n",
    "    neg_dist = distance_transform_edt(~mask)\n",
    "    dist_map = pos_dist - neg_dist  # Signed distance transform\n",
    "    return dist_map\n",
    "\n",
    "def generate_multi_size_masks(mask, scales=[1.0, 0.5, 0.25]):\n",
    "    multi_res = []\n",
    "    for scale in scales:\n",
    "        if scale == 1.0:\n",
    "            multi_res.append(mask)\n",
    "        else:\n",
    "            resized = resize(mask, output_shape=tuple(int(s * scale) for s in mask.shape),\n",
    "                             order=1, preserve_range=True, anti_aliasing=True)\n",
    "            multi_res.append(resized.astype(mask.dtype))\n",
    "    return multi_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3515529-ff9f-4f47-935b-e09a357f22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_case(input_dir, output_dir, case_id):\n",
    "    flair = normalize_intensity(load_nifti(input_dir / f\"{case_id}_flair.nii.gz\"))\n",
    "    t1 = normalize_intensity(load_nifti(input_dir / f\"{case_id}_t1.nii.gz\"))\n",
    "    t2 = normalize_intensity(load_nifti(input_dir / f\"{case_id}_t2.nii.gz\"))\n",
    "    seg = load_nifti(input_dir / f\"{case_id}_seg.nii.gz\").astype(np.uint8)\n",
    "\n",
    "    # Stack input modalities into a tensor (C, D, H, W)\n",
    "    stacked = np.stack([flair, t1, t2], axis=0)\n",
    "    # input_tensor = torch.tensor(stacked, dtype=torch.float32).cuda()\n",
    "    # Use CPU Tensors instead\n",
    "    input_tensor = torch.tensor(stacked, dtype=torch.float32)\n",
    "    \n",
    "    # Load it back into a PyTorch Tensor for storing\n",
    "    seg_tensor = torch.tensor(seg, dtype=torch.uint8)\n",
    "    # Add the batch dimension in order to make it compatible with the other Tensor sizes\n",
    "    seg_tensor = seg_tensor.unsqueeze(0)\n",
    "\n",
    "    \n",
    "    # Save all\n",
    "    output_case_dir = output_dir / case_id\n",
    "    output_case_dir.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(input_tensor, output_case_dir / \"input_tensor.pt\")\n",
    "    torch.save(seg_tensor, output_case_dir / \"seg_mask.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac6e5ec-ad39-42e9-8a62-c1140398e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_preprocessing(root_path, output_path):\n",
    "    input_path = Path(root_path)\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    # Get all case directories\n",
    "    all_case_dirs = [d for d in input_path.iterdir() if d.is_dir()]\n",
    "    print(f\"Found {len(all_case_dirs)} cases.\")\n",
    "\n",
    "    for case_dir in tqdm(all_case_dirs):\n",
    "        case_id = case_dir.name  # e.g., MSLS_000\n",
    "        output_case_path = output_path / case_id  # Define output path for each case\n",
    "\n",
    "        # Check if the case has already been processed (e.g., output directory or file exists)\n",
    "        if output_case_path.exists():\n",
    "            print(f\"‚úÖ Skipping {case_id}, already processed.\")\n",
    "            continue  # Skip processing if the case already exists\n",
    "\n",
    "        try:\n",
    "            preprocess_case(case_dir, output_path, case_id)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed on {case_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e6a0eb7-7da6-400d-9446-97773e286c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 93 cases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:00<00:00, 7383.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Skipping MSLS_019, already processed.\n",
      "‚úÖ Skipping MSLS_039, already processed.\n",
      "‚úÖ Skipping MSLS_059, already processed.\n",
      "‚úÖ Skipping MSLS_000, already processed.\n",
      "‚úÖ Skipping MSLS_001, already processed.\n",
      "‚úÖ Skipping MSLS_002, already processed.\n",
      "‚úÖ Skipping MSLS_003, already processed.\n",
      "‚úÖ Skipping MSLS_004, already processed.\n",
      "‚úÖ Skipping MSLS_005, already processed.\n",
      "‚úÖ Skipping MSLS_006, already processed.\n",
      "‚úÖ Skipping MSLS_007, already processed.\n",
      "‚úÖ Skipping MSLS_008, already processed.\n",
      "‚úÖ Skipping MSLS_009, already processed.\n",
      "‚úÖ Skipping MSLS_010, already processed.\n",
      "‚úÖ Skipping MSLS_011, already processed.\n",
      "‚úÖ Skipping MSLS_012, already processed.\n",
      "‚úÖ Skipping MSLS_013, already processed.\n",
      "‚úÖ Skipping MSLS_014, already processed.\n",
      "‚úÖ Skipping MSLS_015, already processed.\n",
      "‚úÖ Skipping MSLS_016, already processed.\n",
      "‚úÖ Skipping MSLS_017, already processed.\n",
      "‚úÖ Skipping MSLS_018, already processed.\n",
      "‚úÖ Skipping MSLS_020, already processed.\n",
      "‚úÖ Skipping MSLS_021, already processed.\n",
      "‚úÖ Skipping MSLS_022, already processed.\n",
      "‚úÖ Skipping MSLS_023, already processed.\n",
      "‚úÖ Skipping MSLS_024, already processed.\n",
      "‚úÖ Skipping MSLS_025, already processed.\n",
      "‚úÖ Skipping MSLS_026, already processed.\n",
      "‚úÖ Skipping MSLS_027, already processed.\n",
      "‚úÖ Skipping MSLS_028, already processed.\n",
      "‚úÖ Skipping MSLS_029, already processed.\n",
      "‚úÖ Skipping MSLS_030, already processed.\n",
      "‚úÖ Skipping MSLS_031, already processed.\n",
      "‚úÖ Skipping MSLS_032, already processed.\n",
      "‚úÖ Skipping MSLS_033, already processed.\n",
      "‚úÖ Skipping MSLS_034, already processed.\n",
      "‚úÖ Skipping MSLS_035, already processed.\n",
      "‚úÖ Skipping MSLS_036, already processed.\n",
      "‚úÖ Skipping MSLS_037, already processed.\n",
      "‚úÖ Skipping MSLS_038, already processed.\n",
      "‚úÖ Skipping MSLS_040, already processed.\n",
      "‚úÖ Skipping MSLS_041, already processed.\n",
      "‚úÖ Skipping MSLS_042, already processed.\n",
      "‚úÖ Skipping MSLS_043, already processed.\n",
      "‚úÖ Skipping MSLS_044, already processed.\n",
      "‚úÖ Skipping MSLS_045, already processed.\n",
      "‚úÖ Skipping MSLS_046, already processed.\n",
      "‚úÖ Skipping MSLS_047, already processed.\n",
      "‚úÖ Skipping MSLS_048, already processed.\n",
      "‚úÖ Skipping MSLS_049, already processed.\n",
      "‚úÖ Skipping MSLS_050, already processed.\n",
      "‚úÖ Skipping MSLS_051, already processed.\n",
      "‚úÖ Skipping MSLS_052, already processed.\n",
      "‚úÖ Skipping MSLS_053, already processed.\n",
      "‚úÖ Skipping MSLS_054, already processed.\n",
      "‚úÖ Skipping MSLS_055, already processed.\n",
      "‚úÖ Skipping MSLS_056, already processed.\n",
      "‚úÖ Skipping MSLS_057, already processed.\n",
      "‚úÖ Skipping MSLS_058, already processed.\n",
      "‚úÖ Skipping MSLS_060, already processed.\n",
      "‚úÖ Skipping MSLS_061, already processed.\n",
      "‚úÖ Skipping MSLS_062, already processed.\n",
      "‚úÖ Skipping MSLS_063, already processed.\n",
      "‚úÖ Skipping MSLS_064, already processed.\n",
      "‚úÖ Skipping MSLS_065, already processed.\n",
      "‚úÖ Skipping MSLS_066, already processed.\n",
      "‚úÖ Skipping MSLS_067, already processed.\n",
      "‚úÖ Skipping MSLS_068, already processed.\n",
      "‚úÖ Skipping MSLS_069, already processed.\n",
      "‚úÖ Skipping MSLS_070, already processed.\n",
      "‚úÖ Skipping MSLS_071, already processed.\n",
      "‚úÖ Skipping MSLS_072, already processed.\n",
      "‚úÖ Skipping MSLS_073, already processed.\n",
      "‚úÖ Skipping MSLS_074, already processed.\n",
      "‚úÖ Skipping MSLS_075, already processed.\n",
      "‚úÖ Skipping MSLS_076, already processed.\n",
      "‚úÖ Skipping MSLS_077, already processed.\n",
      "‚úÖ Skipping MSLS_078, already processed.\n",
      "‚úÖ Skipping MSLS_079, already processed.\n",
      "‚úÖ Skipping MSLS_080, already processed.\n",
      "‚úÖ Skipping MSLS_081, already processed.\n",
      "‚úÖ Skipping MSLS_082, already processed.\n",
      "‚úÖ Skipping MSLS_083, already processed.\n",
      "‚úÖ Skipping MSLS_084, already processed.\n",
      "‚úÖ Skipping MSLS_085, already processed.\n",
      "‚úÖ Skipping MSLS_086, already processed.\n",
      "‚úÖ Skipping MSLS_087, already processed.\n",
      "‚úÖ Skipping MSLS_088, already processed.\n",
      "‚úÖ Skipping MSLS_089, already processed.\n",
      "‚úÖ Skipping MSLS_090, already processed.\n",
      "‚úÖ Skipping MSLS_091, already processed.\n",
      "‚úÖ Skipping MSLS_092, already processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the preprocessing on the training set\n",
    "RAW_DATA_PATH = \"../data/01-Pre-Processed-Data/train\"\n",
    "OUTPUT_PATH = \"../data/02-Tensor-Data/train\"\n",
    "\n",
    "run_preprocessing(RAW_DATA_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc06f78a-1ed2-4e4b-90f7-391b10748569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 cases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:00<00:00, 7596.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Skipping MSLS_093, already processed.\n",
      "‚úÖ Skipping MSLS_094, already processed.\n",
      "‚úÖ Skipping MSLS_095, already processed.\n",
      "‚úÖ Skipping MSLS_096, already processed.\n",
      "‚úÖ Skipping MSLS_097, already processed.\n",
      "‚úÖ Skipping MSLS_098, already processed.\n",
      "‚úÖ Skipping MSLS_099, already processed.\n",
      "‚úÖ Skipping MSLS_100, already processed.\n",
      "‚úÖ Skipping MSLS_101, already processed.\n",
      "‚úÖ Skipping MSLS_102, already processed.\n",
      "‚úÖ Skipping MSLS_103, already processed.\n",
      "‚úÖ Skipping MSLS_104, already processed.\n",
      "‚úÖ Skipping MSLS_105, already processed.\n",
      "‚úÖ Skipping MSLS_106, already processed.\n",
      "‚úÖ Skipping MSLS_107, already processed.\n",
      "‚úÖ Skipping MSLS_108, already processed.\n",
      "‚úÖ Skipping MSLS_109, already processed.\n",
      "‚úÖ Skipping MSLS_110, already processed.\n",
      "‚úÖ Skipping MSLS_111, already processed.\n",
      "‚úÖ Skipping MSLS_112, already processed.\n",
      "‚úÖ Skipping MSLS_113, already processed.\n",
      "‚úÖ Skipping MSLS_114, already processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the preprocessing on the test set (with gt mask)\n",
    "RAW_DATA_PATH = \"../data/01-Pre-Processed-Data/test/test_MASK\"\n",
    "OUTPUT_PATH = \"../data/02-Tensor-Data/test/test_MASK\"\n",
    "\n",
    "run_preprocessing(RAW_DATA_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33b1eae6-3dee-4f09-aaf6-1bcfc5da9e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variation for the test set WITHOUT the gt seg mask\n",
    "def preprocess_case_no_seg_mask(input_dir, output_dir, case_id):\n",
    "    flair = normalize_intensity(load_nifti(input_dir / f\"{case_id}_flair.nii.gz\"))\n",
    "    t1 = normalize_intensity(load_nifti(input_dir / f\"{case_id}_t1.nii.gz\"))\n",
    "    t2 = normalize_intensity(load_nifti(input_dir / f\"{case_id}_t2.nii.gz\"))\n",
    "\n",
    "    # Stack input modalities into a tensor (C, D, H, W)\n",
    "    stacked = np.stack([flair, t1, t2], axis=0)\n",
    "    # input_tensor = torch.tensor(stacked, dtype=torch.float32).cuda()\n",
    "    # Use CPU Tensors instead\n",
    "    input_tensor = torch.tensor(stacked, dtype=torch.float32)\n",
    "    \n",
    "    # Cannot compute the distance maps and multi scale masks due to lack of segmentation mask\n",
    "    \n",
    "    # Save all\n",
    "    output_case_dir = output_dir / case_id\n",
    "    output_case_dir.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(input_tensor, output_case_dir / \"input_tensor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "003c7bb6-5162-456c-8893-f286d27e1873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Variation for the test set WITHOUT the gt seg mask\n",
    "def run_preprocess_no_seg_mask(root_path, output_path):\n",
    "    input_path = Path(root_path)\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    # Get all case directories\n",
    "    all_case_dirs = [d for d in input_path.iterdir() if d.is_dir()]\n",
    "    print(f\"Found {len(all_case_dirs)} cases.\")\n",
    "\n",
    "    for case_dir in tqdm(all_case_dirs):\n",
    "        case_id = case_dir.name\n",
    "        output_case_path = output_path / case_id  # Define output path for each case\n",
    "\n",
    "        # Check if the case has already been processed (e.g., output directory or file exists)\n",
    "        if output_case_path.exists():\n",
    "            print(f\"‚úÖ Skipping {case_id}, already processed.\")\n",
    "            continue  # Skip processing if the case already exists\n",
    "\n",
    "        try:\n",
    "            preprocess_case_no_seg_mask(case_dir, output_path, case_id)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Skipping {case_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dc58dd7-0257-4954-905d-404f8975dd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 cases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:00<00:00, 7843.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Skipping MSLS_093, already processed.\n",
      "‚úÖ Skipping MSLS_094, already processed.\n",
      "‚úÖ Skipping MSLS_095, already processed.\n",
      "‚úÖ Skipping MSLS_096, already processed.\n",
      "‚úÖ Skipping MSLS_097, already processed.\n",
      "‚úÖ Skipping MSLS_098, already processed.\n",
      "‚úÖ Skipping MSLS_099, already processed.\n",
      "‚úÖ Skipping MSLS_100, already processed.\n",
      "‚úÖ Skipping MSLS_101, already processed.\n",
      "‚úÖ Skipping MSLS_102, already processed.\n",
      "‚úÖ Skipping MSLS_103, already processed.\n",
      "‚úÖ Skipping MSLS_104, already processed.\n",
      "‚úÖ Skipping MSLS_105, already processed.\n",
      "‚úÖ Skipping MSLS_106, already processed.\n",
      "‚úÖ Skipping MSLS_107, already processed.\n",
      "‚úÖ Skipping MSLS_108, already processed.\n",
      "‚úÖ Skipping MSLS_109, already processed.\n",
      "‚úÖ Skipping MSLS_110, already processed.\n",
      "‚úÖ Skipping MSLS_111, already processed.\n",
      "‚úÖ Skipping MSLS_112, already processed.\n",
      "‚úÖ Skipping MSLS_113, already processed.\n",
      "‚úÖ Skipping MSLS_114, already processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the preprocessing on the test set (no gt mask)\n",
    "RAW_DATA_PATH = \"../data/01-Pre-Processed-Data/test/test\"\n",
    "OUTPUT_PATH = \"../data/02-Tensor-Data/test/test\"\n",
    "\n",
    "run_preprocess_no_seg_mask(RAW_DATA_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2f3830-e80c-47b9-9f3f-3b1268619f14",
   "metadata": {},
   "source": [
    "## Build the Dataset and Dataloaders for the MSLesSeg preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "813cbfe2-346f-48f1-b350-c0ef69e9ae0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Check the initial memory consuption (GPU) of the project\n",
    "import torch\n",
    "\n",
    "# Check initial GPU memory usage\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59da4863-42f3-4d2c-a675-c8cdc1eeeace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MSLesionDataset(Dataset):\n",
    "    def __init__(self, root_dir, include_labels=True, sample_ids=None, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        all_samples = sorted([d for d in self.root_dir.iterdir() if d.is_dir()])\n",
    "        self.sample_dirs = [d for d in all_samples if not sample_ids or d.name in sample_ids]\n",
    "        self.include_labels = include_labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def pad_to_match(self, tensor, reference_shape):\n",
    "        \"\"\"Pads a tensor to match a 4D (C, D, H, W) reference shape.\"\"\"\n",
    "        current_shape = tensor.shape\n",
    "        pad_sizes = []\n",
    "        for i in range(3, 0, -1):  # W, H, D\n",
    "            diff = reference_shape[i] - current_shape[i]\n",
    "            if diff < 0:\n",
    "                raise ValueError(f\"Tensor dimension {i} is larger than reference: {current_shape[i]} > {reference_shape[i]}\")\n",
    "            pad_sizes.extend([0, diff])\n",
    "        return F.pad(tensor, pad_sizes, mode='constant', value=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            case_dir = self.sample_dirs[idx]\n",
    "    \n",
    "            # === Load FLAIR, T1, T2 ===\n",
    "            input_tensor = torch.load(case_dir / \"input_tensor.pt\").float()  # shape: [3, D, H, W]\n",
    "            if input_tensor.dim() == 3:\n",
    "                input_tensor = input_tensor.unsqueeze(0)\n",
    "            reference_shape = input_tensor.shape  # (C, D, H, W)\n",
    "    \n",
    "            # === Load distance map ===\n",
    "            distance_map = torch.load(case_dir / \"distance_map.pt\").float()\n",
    "            if distance_map.dim() == 3:\n",
    "                distance_map = distance_map.unsqueeze(0)\n",
    "            distance_map = self.pad_to_match(distance_map, reference_shape)\n",
    "    \n",
    "            # === Load multi-scale masks ===\n",
    "            multi_size_masks = []\n",
    "            for i in range(3):\n",
    "                path = case_dir / f\"multi_size_mask_{i}.pt\"\n",
    "                mask = torch.load(path).float() if path.exists() else torch.zeros_like(distance_map)\n",
    "                if mask.dim() == 3:\n",
    "                    mask = mask.unsqueeze(0)\n",
    "                multi_size_masks.append(self.pad_to_match(mask, reference_shape))\n",
    "            multi_masks_cat = torch.cat(multi_size_masks, dim=0)  # [3, D, H, W]\n",
    "    \n",
    "            # === Final input: [7, D, H, W] ===\n",
    "            final_input = torch.cat([input_tensor, distance_map, multi_masks_cat], dim=0)\n",
    "    \n",
    "            if self.transform:\n",
    "                final_input = self.transform(final_input)\n",
    "    \n",
    "            # === Optional target: segmentation mask ===\n",
    "            if self.include_labels:\n",
    "                seg_mask_path = case_dir / \"seg_mask.pt\"\n",
    "                seg_mask = torch.load(seg_mask_path).float()\n",
    "                if seg_mask.dim() == 3:\n",
    "                    seg_mask = seg_mask.unsqueeze(0)\n",
    "                seg_mask = self.pad_to_match(seg_mask, reference_shape)\n",
    "                return final_input, seg_mask\n",
    "    \n",
    "            return final_input, str(case_dir.name)\n",
    "        except Exception as e:\n",
    "            print(f\"Dataset raised an exception:\\t{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02dcd821-2267-490c-a3cd-afa761eac4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03106a16-273a-4298-b82f-e15990258754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_dataloader(data_root, batch_size=2, num_workers=4, shuffle=True, include_labels=True, sample_ids=None, transform=None):\n",
    "    \"\"\"\n",
    "    Returns a PyTorch DataLoader for the MSLesionDataset.\n",
    "\n",
    "    Args:\n",
    "        data_root (str or Path): Root path containing sample subdirectories.\n",
    "        batch_size (int): Batch size.\n",
    "        num_workers (int): Number of worker threads.\n",
    "        shuffle (bool): Whether to shuffle the dataset.\n",
    "        include_labels (bool): If True, return segmentation masks (for training).\n",
    "        sample_ids (list of str): Optional subset of sample names.\n",
    "        transform (callable): Optional transform to apply to the input tensor.\n",
    "    \"\"\"\n",
    "    dataset = MSLesionDataset(\n",
    "        root_dir=data_root,\n",
    "        include_labels=include_labels,\n",
    "        sample_ids=sample_ids,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5e4b760-d9c4-4bc1-b231-b8f08f48331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of some (non training) parameters\n",
    "NUM_WORKERS_TRAIN = 0\n",
    "NUM_WORKERS_VAL = 0\n",
    "NUM_WORKERS_TEST = 0\n",
    "\n",
    "BATCH_SIZE_TRAIN = 2\n",
    "BATCH_SIZE_VAL = 2\n",
    "BATCH_SIZE_TEST = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26a1d71b-abca-4933-8ec7-81302f763067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a49cd1c-7775-40f0-91ec-aae0a176be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# === Config ===\n",
    "TRAIN_TENSOR_DATA_PATH = Path(\"../data/02-Tensor-Data/train\")\n",
    "BATCH_SIZE_TRAIN = 2\n",
    "BATCH_SIZE_VAL = 2\n",
    "NUM_WORKERS_TRAIN = 0\n",
    "NUM_WORKERS_VAL = 0\n",
    "\n",
    "# === Get all valid sample folders ===\n",
    "all_cases = sorted([d for d in os.listdir(TRAIN_TENSOR_DATA_PATH) if os.path.isdir(TRAIN_TENSOR_DATA_PATH / d)])\n",
    "\n",
    "# === Split for 80-20 train/val ===\n",
    "random.seed(42)\n",
    "random.shuffle(all_cases)\n",
    "split = int(0.8 * len(all_cases))\n",
    "train_ids = all_cases[:split]\n",
    "val_ids = all_cases[split:]\n",
    "\n",
    "# === Loaders ===\n",
    "train_loader = get_dataloader(\n",
    "    data_root=TRAIN_TENSOR_DATA_PATH,\n",
    "    sample_ids=train_ids,\n",
    "    batch_size=BATCH_SIZE_TRAIN,\n",
    "    num_workers=NUM_WORKERS_TRAIN,\n",
    "    shuffle=True,\n",
    "    include_labels=True\n",
    ")\n",
    "\n",
    "val_loader = get_dataloader(\n",
    "    data_root=TRAIN_TENSOR_DATA_PATH,\n",
    "    sample_ids=val_ids,\n",
    "    batch_size=BATCH_SIZE_VAL,\n",
    "    num_workers=NUM_WORKERS_VAL,\n",
    "    shuffle=False,\n",
    "    include_labels=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9d350be-96b0-4456-bfe9-2176cdc52570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ca4c7fb-ef72-48f3-9583-19503337a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TENSOR_DATA_PATH_NOMASK = Path(\"../data/02-Tensor-Data/test/test\")\n",
    "TEST_TENSOR_DATA_PATH_MASK = Path(\"../data/02-Tensor-Data/test/test_MASK\")\n",
    "\n",
    "# Test set WITHOUT labels (for inference or prediction)\n",
    "test_loader_no_labels = get_dataloader(\n",
    "    data_root=TEST_TENSOR_DATA_PATH_NOMASK,\n",
    "    include_labels=False,\n",
    "    batch_size=BATCH_SIZE_TEST,\n",
    "    num_workers=NUM_WORKERS_TEST,\n",
    "    shuffle=False  # No shuffling for inference\n",
    ")\n",
    "\n",
    "# Test set WITH labels (for final evaluation or performance metrics)\n",
    "test_loader_with_labels = get_dataloader(\n",
    "    data_root=TEST_TENSOR_DATA_PATH_MASK,\n",
    "    include_labels=True,\n",
    "    batch_size=BATCH_SIZE_TEST,\n",
    "    num_workers=NUM_WORKERS_TEST,\n",
    "    shuffle=False  # No shuffling for evaluation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a3370c6-c8fa-4871-9093-f5e13a8d9002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "def get_k_fold_loaders(data_root, k=5, batch_size=BATCH_SIZE_TRAIN, num_workers=NUM_WORKERS_TRAIN, include_labels=True):\n",
    "    # Get all case IDs\n",
    "    all_cases = sorted(os.listdir(data_root))\n",
    "    \n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_loaders = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(all_cases):\n",
    "        train_ids = [all_cases[i] for i in train_idx]\n",
    "        val_ids = [all_cases[i] for i in val_idx]\n",
    "\n",
    "        # Updated to pass `include_labels`\n",
    "        train_loader = get_dataloader(data_root, sample_ids=train_ids, batch_size=batch_size, num_workers=num_workers, include_labels=include_labels)\n",
    "        val_loader = get_dataloader(data_root, sample_ids=val_ids, batch_size=batch_size, num_workers=num_workers, include_labels=include_labels, shuffle=False)\n",
    "\n",
    "        fold_loaders.append((train_loader, val_loader))\n",
    "\n",
    "    return fold_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a1847cf-501d-4233-8600-08376ed05e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get K-Fold DataLoaders\n",
    "fold_loaders = get_k_fold_loaders(TRAIN_TENSOR_DATA_PATH, k=5, batch_size=2, num_workers=0)\n",
    "\n",
    "# Access the first fold\n",
    "train_loader, val_loader = fold_loaders[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f0311-9b80-4bf0-b80d-996692a83815",
   "metadata": {},
   "source": [
    "### PyTorch Lightning DataModule\n",
    "\n",
    "This particular version of PyTorch Lightning, and in general from version 2.x onward require a ***LightningDataModule*** instead of passing the dataloaders directly to the ***.fit()*** method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac9f0867-1234-4a5d-b1e1-bd8802d56a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningDataModule\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class MSLesionSegmentationDataModule(LightningDataModule):\n",
    "    def __init__(self, data_root, train_split=0.8, batch_size=2, num_workers=4, include_labels=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_root: Path to the root directory containing the dataset.\n",
    "            train_split: Fraction of data to use for training (default 80%).\n",
    "            batch_size: Batch size for the dataloader.\n",
    "            num_workers: Number of workers for the dataloader.\n",
    "            include_labels: Whether to include segmentation masks during training/validation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_root = data_root\n",
    "        self.train_split = train_split\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.include_labels = include_labels\n",
    "\n",
    "        self.train_dataloader_instance = None\n",
    "        self.val_dataloader_instance = None\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_dataloader_instance\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_dataloader_instance\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.val_dataloader_instance  # Optional, if you want to test on the validation set as well.\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"Prepare any datasets if needed, such as downloading or preprocessing.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"Assign train/val datasets for use in dataloaders.\"\"\"\n",
    "        # List all sample directories\n",
    "        all_samples = sorted([d for d in Path(self.data_root).iterdir() if d.is_dir()])\n",
    "\n",
    "        # Shuffle the sample list before splitting\n",
    "        random.seed(42)  # To ensure reproducibility\n",
    "        random.shuffle(all_samples)\n",
    "\n",
    "        # Split the dataset into training and validation sets\n",
    "        split_idx = int(len(all_samples) * self.train_split)\n",
    "        train_samples = all_samples[:split_idx]\n",
    "        val_samples = all_samples[split_idx:]\n",
    "\n",
    "        # Create the train and val dataloaders, passing the `include_labels` parameter\n",
    "        self.train_dataloader_instance = get_dataloader(\n",
    "            self.data_root,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            sample_ids=[d.name for d in train_samples],\n",
    "            include_labels=self.include_labels,  # Pass this argument\n",
    "        )\n",
    "\n",
    "        self.val_dataloader_instance = get_dataloader(\n",
    "            self.data_root,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            sample_ids=[d.name for d in val_samples],\n",
    "            include_labels=self.include_labels,  # Pass this argument\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e8bcef4-4bd3-429a-9722-5ac78eeea124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b08f0-2188-400b-b30e-88997983f659",
   "metadata": {},
   "source": [
    "## The Model's Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "726efd51-8791-4abc-9204-b5571ed1a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Standard double convolution block\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "# Final output layer: Conv3D + Sigmoid for probability map\n",
    "def final_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv3d(in_channels, out_channels, kernel_size=1),\n",
    "        nn.Sigmoid()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c67520ee-53db-45f6-a184-3ff3761bb80d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 3D U-Net Architecture\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mUNet\u001b[39;00m(\u001b[43mnn\u001b[49m.Module):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_classes, in_channels):\n\u001b[32m      4\u001b[39m         \u001b[38;5;28msuper\u001b[39m(UNet, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mNameError\u001b[39m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# 3D U-Net Architecture\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.max_pool3d = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "\n",
    "        # == Contracting Path == #\n",
    "        self.down_conv_1 = double_conv(in_channels, 64)\n",
    "        self.down_conv_2 = dobule_conv(64, 128)\n",
    "        self.down_conv_3 = double_conv(128, 256)\n",
    "        self.down_conv_4 = double_conv(256, 512)\n",
    "        self.down_conv_5 = double_conv(512, 1024)\n",
    "\n",
    "        # == Expanding Path == #\n",
    "        self.up_transpose_1 = nn.ConvTranspose3d(\n",
    "            in_channels=1024,\n",
    "            out_channels=512,\n",
    "            kernel_size=2,\n",
    "        )\n",
    "        self.up_conv_1 = double_conv(1024, 512)\n",
    "        \n",
    "        self.up_transpose_2 = nn.ConvTranspose3d(\n",
    "            in_channels=512,\n",
    "            out_channels=256,\n",
    "            kernel_size=2,\n",
    "        )\n",
    "        self.up_conv_2 = double_conv(512, 256)\n",
    "\n",
    "        self.up_transpose_3 = nn.ConvTranspose3d(\n",
    "            in_channels=256,\n",
    "            out_channels=128,\n",
    "            kernel_size=2,\n",
    "        )\n",
    "        self.up_conv_3 = double_conv(256, 128)\n",
    "\n",
    "        self.up_transpose_4 = nn.ConvTranspose3d(\n",
    "            in_channels=128,\n",
    "            out_channels=64,\n",
    "            kernel_size=2,\n",
    "        )\n",
    "        self.up_conv_4 = double_conv(128, 64)\n",
    "\n",
    "        self.prob_out = final_conv(64, num_classes)\n",
    "\n",
    "        self.out = nn.Conv3d(\n",
    "            in_channels=64,\n",
    "            out_channels=num_classes,\n",
    "            kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        down_1 = self.down_conv_1(x)\n",
    "        down_2 = self.max_pool3d(down_1)\n",
    "        down_3 = self.down_conv_2(down_2)\n",
    "        down_4 = self.max_pool3d(down_3)\n",
    "        down_5 = self.down_conv_3(down_4)\n",
    "        down_6 = self.max_pool3d(down_5)\n",
    "        down_7 = self.down_conv_4(down_6)\n",
    "        down_8 = self.max_pool3d(down_7)\n",
    "        down_9 = self.down_conv_5(down_8)\n",
    "\n",
    "        up_1 = self.up_transpose_1(down_9)\n",
    "        x = self.up_conv_1(torch.cat([down_7, up_1], 1))\n",
    "        \n",
    "        up_2 = self.up_transpose_2(x)\n",
    "        x = self.up_conv_2(torch.cat([down_5, up_2], 1))\n",
    "        \n",
    "        up_3 = self.up_transpose_3(x)\n",
    "        x = self.up_conv_3(torch.cat([down_3, up_3], 1))\n",
    "        \n",
    "        up_4 = self.up_transpose_4(x)\n",
    "        x = self.up_conv_4(torch.cat([down_1, up_4], 1))\n",
    "        \n",
    "        out = self.out(x)\n",
    "        prob_out = self.prob_out(x)\n",
    "        \n",
    "        return out, prob_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e073dd21-30c4-45d3-abde-ad66916cd428",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (825341197.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mmodel = UNet(num_classes=2m in_channels=1)\u001b[39m\n                             ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "# Perform a sanity check on the model\n",
    "input_image = torch.rand((512, 512, 512))\n",
    "model = UNet(num_classes=2m in_channels=1)\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")\n",
    "outputs, prob_outputs = model(input_image)\n",
    "print(f\"U-Net outputs shape:\\t{outputs.shape}\")\n",
    "print(f\"U-Net probability map outputs shape:\\t{prob_outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60727e14-353b-4fd3-9e3e-938472763e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa3c2123-75a5-45fa-ab0b-6887c61831ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1    [-1, 32, 128, 128, 128]           6,080\n",
      "              ReLU-2    [-1, 32, 128, 128, 128]               0\n",
      "            Conv3d-3    [-1, 64, 128, 128, 128]          55,360\n",
      "              ReLU-4    [-1, 64, 128, 128, 128]               0\n",
      "         MaxPool3d-5       [-1, 64, 64, 64, 64]               0\n",
      "            Conv3d-6      [-1, 128, 64, 64, 64]         221,312\n",
      "              ReLU-7      [-1, 128, 64, 64, 64]               0\n",
      "            Conv3d-8      [-1, 128, 64, 64, 64]         442,496\n",
      "              ReLU-9      [-1, 128, 64, 64, 64]               0\n",
      "  ConvTranspose3d-10    [-1, 64, 128, 128, 128]          65,600\n",
      "           Conv3d-11    [-1, 64, 128, 128, 128]         110,656\n",
      "             ReLU-12    [-1, 64, 128, 128, 128]               0\n",
      "           Conv3d-13    [-1, 32, 128, 128, 128]          55,328\n",
      "             ReLU-14    [-1, 32, 128, 128, 128]               0\n",
      "           Conv3d-15     [-1, 1, 128, 128, 128]              33\n",
      "           Conv3d-16        [-1, 1, 64, 64, 64]             129\n",
      "  ConvTranspose3d-17     [-1, 1, 128, 128, 128]               9\n",
      "================================================================\n",
      "Total params: 957,003\n",
      "Trainable params: 957,003\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 56.00\n",
      "Forward/backward pass size (MB): 8354.00\n",
      "Params size (MB): 3.65\n",
      "Estimated Total Size (MB): 8413.65\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# The Summary of the Architecture\n",
    "from torchsummary import summary\n",
    "\n",
    "# Set the device and the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(in_channels=1, out_channels=1).to(device)\n",
    "\n",
    "# Get the summary of the model (for 3D input: channels, depth, height, width)\n",
    "summary(model, (128, 128, 128))  # 7 input channels now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "686f871e-cfea-440a-afd9-c950b441427c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 12.29 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ff77208-1cc9-4c68-8aa7-eaf9a3def39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the graphical visualization\n",
    "from torchviz import make_dot\n",
    "import gc\n",
    "\n",
    "# Dummy input with correct number of channels (7), batch size = 1\n",
    "dummy_input = torch.randn(128, 128, 128).cuda()\n",
    "\n",
    "# Explicitly instruct the model to not save the gradients to avoid computation graph creation\n",
    "with torch.no_grad():\n",
    "    # Forward pass through the model\n",
    "    output, _ = model(dummy_input)\n",
    "\n",
    "    # Visualize the graph and save as PNG\n",
    "    make_dot(output, params=dict(model.named_parameters())).render(\"model_architecture\", format=\"png\")\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Clear non-essential GPU memory\n",
    "del dummy_input\n",
    "del output\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87ecec9a-397d-492f-a6a6-0762cd22dfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 12.29 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bbba010-2e2d-4aa1-9c7f-86d110084e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the GPU memory used for the dummy input and re-initialize the model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(num_classes=2, in_channels=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52a73d93-c9ae-41ba-9bb8-b297067a69cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 11.78 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dcd022-a711-4931-b46c-336f8565d581",
   "metadata": {},
   "source": [
    "## Visualized Model Architecture\n",
    "\n",
    "![Model Architecture Plot](./model_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be907f2a-2600-407f-8649-42965a1fb3d1",
   "metadata": {},
   "source": [
    "## The Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b2d3471-ee79-41ff-b176-929864412593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.segmentation import DiceScore\n",
    "from torch.nn import BCELoss\n",
    "\n",
    "class MSLesionSegmentationModel(pl.LightningModule):\n",
    "    def __init__(self, model, lr=1e-4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "        # DiceScore for binary segmentation\n",
    "        self.dice_metric = DiceScore(num_classes=2, average='micro')\n",
    "\n",
    "        # Binary Cross-Entropy loss\n",
    "        self.bce_loss = BCELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch  # x: [B, D, H, W], y: [B, D, H, W]\n",
    "\n",
    "        # Forward pass: get both voxel prediction and probability map\n",
    "        output, prob_map = self(x)\n",
    "\n",
    "        # Compute Dice Loss for segmentation (voxel)\n",
    "        dice_loss = 1 - self.dice_metric(torch.sigmoid(output), y.int())\n",
    "\n",
    "        # Compute BCELoss for the probability map\n",
    "        bce_loss = self.bce_loss(torch.sigmoid(prob_map), y.float())  # sigmoid for probability map\n",
    "\n",
    "        # Combine losses (you can adjust the weighting based on your need)\n",
    "        loss = dice_loss + bce_loss\n",
    "\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch  # x: [B, D, H, W], y: [B, D, H, W]\n",
    "        \n",
    "        # Forward pass: get both voxel prediction and probability map\n",
    "        output, prob_map = self(x)\n",
    "\n",
    "        # Compute Dice Loss for segmentation (voxel)\n",
    "        dice_loss = 1 - self.dice_metric(torch.sigmoid(output), y.int())\n",
    "\n",
    "        # Compute BCELoss for the probability map\n",
    "        bce_loss = self.bce_loss(torch.sigmoid(prob_map), y.float())\n",
    "\n",
    "        # Total validation loss\n",
    "        val_loss = dice_loss + bce_loss\n",
    "\n",
    "        # Calculate Dice score for the voxel output (segmentation mask)\n",
    "        preds = torch.sigmoid(output)\n",
    "        dice = self.dice_metric(preds, y.int())\n",
    "\n",
    "        self.log('val_loss', val_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_dice', dice, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return {\"val_loss\": val_loss, \"val_dice\": dice}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch  # x: [B, D, H, W], y: [B, D, H, W]\n",
    "        \n",
    "        # Forward pass: get both voxel prediction and probability map\n",
    "        output, prob_map = self(x)\n",
    "\n",
    "        # Calculate Dice score for the voxel output (segmentation mask)\n",
    "        preds = torch.sigmoid(output)\n",
    "        dice = self.dice_metric(preds, y.int())\n",
    "\n",
    "        self.log('test_dice', dice, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return dice\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_dice'}\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        # Log the epoch's progress\n",
    "        self.log('epoch', self.current_epoch, prog_bar=False)\n",
    "        \n",
    "        model_path = f\"./model_checkpoints/model_epoch_{self.current_epoch}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': self.current_epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.trainer.optimizers[0].state_dict(),\n",
    "            'lr': self.lr,\n",
    "            'dice_metric': self.dice_metric.compute().item(),  # Save metric if desired\n",
    "        }, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b884f-771e-4852-be17-23973479e5a5",
   "metadata": {},
   "source": [
    "## The Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "330d4d97-5326-4dcf-b27e-2f3a57de10f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    }
   ],
   "source": [
    "# Register a free account with Weights and Biases, and create a new project in order to obtain an API Key for the training\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the env variable\n",
    "api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "# Login to wandb\n",
    "if api_key:\n",
    "    os.environ[\"WANDB_API_KEY\"] = api_key\n",
    "    wandb.login()\n",
    "else:\n",
    "    print(\"‚ùå WANDB_API_KEY not found in .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd78050f-09dd-4251-a8b1-468756f2f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Weights and Biases logger\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project='MSLesSeg-4-ICPR',     # Change to your actual project name\n",
    "    name='nnUNet_MoE_run_3', # A specific run name\n",
    "    log_model=True          # Optional: log model checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2cc45e04-32b8-4824-a118-01950a2d1976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the multiprocessing start to 'spawn' instead of 'fork' due to CUDA issues with the Dataloader\n",
    "import multiprocessing\n",
    "import torch\n",
    "\n",
    "# Set the start method for multiprocessing\n",
    "multiprocessing.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "43d7e0a3-b3d2-4688-bf74-66b0b8054b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 262.15 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f7969d2-7bdf-4b36-bd8e-db0e5413527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the profiler to keep track of GPU overhead\n",
    "from pytorch_lightning.profilers import PyTorchProfiler\n",
    "\n",
    "# Slightly more sophisticated profiler\n",
    "profiler = PyTorchProfiler(\n",
    "    on_trace_ready=lambda prof: print(prof.key_averages().table(\n",
    "        sort_by=\"self_cuda_memory_usage\", row_limit=15  # Change as needed\n",
    "    )),\n",
    "    profile_memory=True,\n",
    "    record_shapes=True,\n",
    "    with_stack=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "60716b23-a05b-4ca7-8c5c-611575ea6a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 262.15 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "318e0b5b-a55c-4dd2-8aa0-a2c43dab9569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1090383\n",
      "Estimated model size: 4.16 MB\n"
     ]
    }
   ],
   "source": [
    "# A computation to estimate the GPU memory consumption of the model\n",
    "\n",
    "# Print the total number of parameters in your model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "\n",
    "# Calculate the approximate memory size of the model (in MB)\n",
    "model_size_MB = total_params * 4 / (1024 ** 2)  # 4 bytes per parameter for float32\n",
    "print(f\"Estimated model size: {model_size_MB:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e89832cf-b12f-4996-92f5-4078eeb524a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17ec947e-62a9-4280-9b98-76de9eb29f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Run garbage collection to clean up unused objects\n",
    "gc.collect()\n",
    "\n",
    "# Empty the PyTorch cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8c82afa2-56b6-4202-8aba-365d4206b4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drew/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/drew/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type      | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | model       | UNetMoE   | 957 K  | train\n",
      "1 | dice_metric | DiceScore | 0      | train\n",
      "--------------------------------------------------\n",
      "957 K     Trainable params\n",
      "0         Non-trainable params\n",
      "957 K     Total params\n",
      "3.828     Total estimated model params size (MB)\n",
      "22        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf4f956abb04017a0cba34ef356ac34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drew/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bffd503f8024e4e9580db2fff160370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0496244c1bfe41fbaed04a508138ee4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d4ce6ab49a41b0aa1a677c447630f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18a1bd5279e444b9cb759ffe5e890fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979b92ad7df748efa1dcf2097aa51c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651c2c8d88054a55aed0b9b7a4707769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e963e752d7472ba950b606ba4c2137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d042c3bb5df54d7e8e3bd3484006ab49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7def353a522438b9ccac31a0f83d454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17adfb4cf442431b86ff5313dc058632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc85a31d374e413c8ea5e997e7267a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4fb8816952a43b2926fe76207dd1add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "032cb703b86741a8be981cd72579a290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d4d92ecdea4508b7ef42422e9421b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33dfec731cf8419d8a51c8d001a7480a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b13c92b35946aaa7ab272e316fa7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2923dd022a4dccb82199e6913f8a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac46aee273e2419dbf6811056e42ca99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7973bd00189246d1ab5e267cc55c3c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7103e0bb20544e1fbb4c60e79d3833e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b391ed8b7d409ca3942373ef1e4277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a32d428715c45f5aa3f1986f16ca304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6212b9ad164541893528a3d874cdf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647e427bd9ff48e39354ce5ae1c5ceb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1aa237137cb412aa9233b29244007e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0acfb93cac04566937c8e6b6099baa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00197f7ce83497f8224e4bebb7e4fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10192edf725489bbef8e7d3422b07a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e663c1952f444fef8936bb5b00be856a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3407ff96d8634bccb8066ceea4dcc0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6530590dade5437f8b4dfe4d5a877824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181c68e4e69b48d99c74975866b3a51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78897a811a354184bcbf2d07253c660c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc3d561fbdd648c091d9180b736eb700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cef0cf8355f4899bf0600fd7db0830f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6c3d0f0eb94e3393167bc4eff74f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a341473cda4e6e80395b10a314b332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea71c7f67030471c9afd4fa5e681c632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7c97cb40f4472e8b8b7c540b28297a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db5a3cf444d4a27a5ed439914a10579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5730bc41a0ff4dbfb36f50d59f1b300d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb4f41497a241d2b787df5f71fda61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245c435dfb5c4f51a0cee993ea891702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8836ff08fa41afb363c2d8b01fceff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d037fe8dad48b2adccf5a04e22f9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ca8d64fe0c499f805896e491e37a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab21cfc207944469c65d31bb3f8b875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec511a6a6264dc5be56b63f62543f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a2535f6f1b4fbbb6bef91b16d61244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b69c3d8bf545729f5cce3d05af47a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56de21a5afe44e7aab09771b92ef1688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527afef4df2e425194f6cb17332da9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137e9c0c51624952bd87608557acb6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88829e4a4e0748b0897cf68e14b4e860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2fbb0ca069b4732a3ec922891caf1a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db203d3fb4c4486997bf2d69e39ea65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719c4b6ab5994f978d02c627784a2604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9d756e74094dfc951564b04588fb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42350db675b4e468f2d591225ba201e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee5d9225ac54fa789531c299ef67b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee19d082187b422096be7ab8508e4b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd3f25f9f974ea897d6b06e6cbab489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2542e4549a84cc0bced8c795e211968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9fba75b13f45efb8f63afbb1d10e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27f982b795b4ab6a97e9c45b8d252c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433f41e2b7484986b6343eef7ee80e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf53591791247aa90494feb7e804d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f694436d3e644f4ca0c6bd70ac0c3048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34514a1b8366402494c2d5a89dc3fde6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81bb5c8d55324a4f97a2e68d8df3baac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4994ebd5dc284106baf6a1c58d45b587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc56d24d11f34c22b1126705b37dc0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f311974471420eb870dfd84a2ce26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2fc68eb2824befb55a567fb8d33b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7b3d75c6274711a4898d4e8c1b3a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd57f7f54c554a98a99a1e216b411d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c49ccee2f9418f9bc3ce424e0d4be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e35a7dc4910412c983120343aeae310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5201a0bd55a84e66b67b66e30c4d4a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948a583843a544e48dab8d7c53f2acbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ef3a4ebb704e9d828c5990d1a3d0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1a24b54b9b4f49a4ce7c24c7f3dce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d789d3d8fb5435992b5714749000c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5095cfc9e6b54f98a8a95ee948f41ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b84074715998495780e07b0540dc30cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ae6bb4b8874aad8183b2b60811198e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e4f78b2d164797b29a725798c82bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c16bb1a84e24509bda868ede86cd14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a448a69ea64e308391298d8b710c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d11d735a05b4acfb5dff7c0f67bc743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5892d401a74e7a9ed7c1056d3afb78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8b40c8b2384a90a5b258cb0f5055f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12178a60f04d4dc19fa7ce6b4d003458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f070a9561943b8b90417663d6c7ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36b108553ed4c509d6d3d5e9b6d5035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51597177f58b48f4bd2f3df45b5cff01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88847e98384640d1ab31b2db19342658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35cb5288e4a14815aaf26657b6d8b525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807af1f8a460418ea4cc7317dd534de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1152e66b80d045a98e88ce87d3822a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d6c61e22e0416da109a5886ccf8821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "\n",
    "# Initialize the TQDM Progress bar for global performance metrics\n",
    "progress_bar = TQDMProgressBar(refresh_rate=1)\n",
    "\n",
    "# Initialize the model with 7 input channels\n",
    "model = MSLesionSegmentationModel(\n",
    "    model=UNetMoE(in_channels=7, out_channels=1, num_experts=4, expert_dim=512)\n",
    ")\n",
    "\n",
    "# Set up checkpointing to save best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_dice',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='best_checkpoint',\n",
    "    save_top_k=1,\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_dice',\n",
    "    patience=10,\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "# Path to training data\n",
    "data_root = TRAIN_TENSOR_DATA_PATH  # Replace with actual path if needed\n",
    "\n",
    "# Initialize the data module\n",
    "data_module = MSLesionSegmentationDataModule(\n",
    "    data_root=data_root,\n",
    "    train_split=0.8,\n",
    "    batch_size=1,\n",
    "    num_workers=0,  # Can be tuned depending on your CPU\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",       # Or \"auto\" if unsure\n",
    "    devices=1,\n",
    "    precision=16,\n",
    "    max_epochs=100,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    logger=wandb_logger,\n",
    "    # profiler=\"simple\"  # Optional, if performance profiling is needed\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3886c494-2f08-4fb4-868e-ce9e4866b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model's checkpoint and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee0a5d-61d4-4561-a731-b24483798f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "trainer.test(model, test_dataloader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6833331b-df4e-485b-8b26-e617ad583746",
   "metadata": {},
   "source": [
    "## Visualizing the Learned Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2febf704-6408-4a85-8d9c-19b88747f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to extract features from the model\n",
    "def extract_features(model, dataloader, device='cuda'):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, target = data['image'].to(device), data['mask'].to(device)  # Replace 'image' and 'mask' keys as per your dataset\n",
    "            \n",
    "            # Forward pass through the model\n",
    "            output = model(inputs)  # This is where we extract the feature; adjust according to your model's architecture\n",
    "            \n",
    "            # Extract features from MoE (or any other layer you want to visualize)\n",
    "            # Here I assume output is the final feature map after the MoE layer\n",
    "            feature_map = output.view(output.size(0), -1)  # Flatten the features to (batch_size, features)\n",
    "            features.append(feature_map.cpu().numpy())\n",
    "            labels.append(target.cpu().numpy())  # Add the target (or ground truth) labels for color coding in t-SNE\n",
    "\n",
    "    features = np.concatenate(features, axis=0)  # Combine all the feature maps\n",
    "    labels = np.concatenate(labels, axis=0)  # Combine all the labels\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "# Function to apply t-SNE on features\n",
    "def plot_tsne(features, labels):\n",
    "    # Standardize the features (optional but recommended for t-SNE)\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(features)\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=labels, cmap='jet', alpha=0.5)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('t-SNE Visualization of Learned Representations')\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage\n",
    "train_dataloader = # Your train DataLoader here\n",
    "\n",
    "# Extract features and labels from the model\n",
    "features, labels = extract_features(model, train_dataloader, device='cuda')\n",
    "\n",
    "# Plot t-SNE visualization\n",
    "plot_tsne(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4089cdf9-90ee-4700-8ec5-2c246a69d848",
   "metadata": {},
   "source": [
    "## Post Hoc Model Explainability - GradCAM++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee0469-f55a-4c46-9c72-a7ba80ad41b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from captum.attr import LayerGradCam, LayerGradCamPlusPlus\n",
    "from captum.attr import GuidedBackprop\n",
    "\n",
    "# Function to visualize GradCAM++ output\n",
    "def visualize_gradcam_plus_plus(model, input_tensor, target_class=None, layer_name='decoder', device='cuda'):\n",
    "    # Move input tensor to the appropriate device\n",
    "    input_tensor = input_tensor.to(device)\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Create GradCAM++ object for the specified layer\n",
    "    gradcam_pp = LayerGradCamPlusPlus(model, model.decoder[2])  # Assuming 'decoder' is your final layer, adjust accordingly.\n",
    "    \n",
    "    # Apply GradCAM++ to input tensor\n",
    "    attributions = gradcam_pp.attribute(input_tensor, target=target_class)\n",
    "    \n",
    "    # Convert attributions to numpy\n",
    "    attributions = attributions.cpu().detach().numpy()\n",
    "    \n",
    "    # Normalize the heatmap\n",
    "    heatmap = np.sum(attributions[0], axis=0)  # Summing over channels for a single-channel heatmap\n",
    "    heatmap = np.maximum(heatmap, 0)  # ReLU to ignore negative values\n",
    "    heatmap = cv2.resize(heatmap, (input_tensor.shape[2], input_tensor.shape[3]))  # Resize to input size\n",
    "    heatmap = cv2.normalize(heatmap, None, 0, 1, cv2.NORM_MINMAX)  # Normalize the heatmap to [0, 1]\n",
    "    \n",
    "    # Convert the original image to numpy and rescale\n",
    "    input_image = input_tensor[0].cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    input_image = cv2.resize(input_image, (input_tensor.shape[2], input_tensor.shape[3]))  # Resize to input size\n",
    "    \n",
    "    # Create a colormap for the heatmap\n",
    "    colormap = plt.get_cmap('jet')\n",
    "    colored_heatmap = colormap(heatmap)  # Apply colormap\n",
    "    \n",
    "    # Overlay the heatmap on top of the original image\n",
    "    superimposed_img = np.uint8(input_image * 255)  # Convert original image back to [0, 255] range\n",
    "    superimposed_img = cv2.addWeighted(superimposed_img, 0.7, np.uint8(colored_heatmap[:, :, :3] * 255), 0.3, 0)\n",
    "    \n",
    "    # Plot the result\n",
    "    plt.imshow(superimposed_img)\n",
    "    plt.title('GradCAM++ Heatmap')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage (assuming `model` is your trained model and `input_tensor` is an input image)\n",
    "input_tensor = torch.randn(1, 3, 128, 128).to(device)  # Example input, use your actual input tensor\n",
    "target_class = None  # You can provide a target class or leave as None for model's top predicted class\n",
    "\n",
    "visualize_gradcam_plus_plus(model, input_tensor, target_class=target_class, layer_name='decoder', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adfb9090-3260-49ce-be05-a03c2d122e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size of the stored (pre-processed) Tensors\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def inspect_pt_file(file_path):\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(f\"File does not exist: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    data = torch.load(file_path, map_location='cpu')\n",
    "\n",
    "    print(f\"\\nLoaded file: {file_path}\")\n",
    "    \n",
    "    if isinstance(data, torch.Tensor):\n",
    "        print(f\"Single Tensor - Shape: {data.shape}\")\n",
    "    \n",
    "    elif isinstance(data, dict):\n",
    "        print(\"Dictionary of tensors:\")\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                print(f\"  {key}: shape = {value.shape}\")\n",
    "            else:\n",
    "                print(f\"  {key}: type = {type(value)}\")\n",
    "    \n",
    "    elif isinstance(data, (list, tuple)):\n",
    "        print(f\"{type(data).__name__} of tensors:\")\n",
    "        for idx, item in enumerate(data):\n",
    "            if isinstance(item, torch.Tensor):\n",
    "                print(f\"  [{idx}]: shape = {item.shape}\")\n",
    "            else:\n",
    "                print(f\"  [{idx}]: type = {type(item)}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Unknown type loaded: {type(data)}\")\n",
    "        print(f\"Unkown type shape: {data.shape}\")\n",
    "\n",
    "INPUT_PATH_PREFIX = \"../data/02-Tensor-Data/train/MSLS_000/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6af84775-fb63-4bfe-9fda-a1b238a7a20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded file: ../data/02-Tensor-Data/train/MSLS_000/input_tensor.pt\n",
      "Single Tensor - Shape: torch.Size([3, 182, 218, 182])\n"
     ]
    }
   ],
   "source": [
    "# Input Tensor Shape (Stacked Modalities)\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"input_tensor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753172b8-22b4-4290-8104-f0edfad737ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance Map Shape\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"distance_map.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0192a99-65c0-491a-97d2-d0b3fc847c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Size Mask - 0\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"multi_size_mask_0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f505b915-04d4-437b-bcf7-e0760cac75fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Size Mask - 1\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"multi_size_mask_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fe78ce-84b5-4936-a774-88c2f457b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Size Mask - 2\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"multi_size_mask_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feeedd4-a2d9-4e3d-900d-d3ca5a68d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation Mask\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"seg_mask.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858f58bf-be14-45b3-9f56-44f6d8719a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

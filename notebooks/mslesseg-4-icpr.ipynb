{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "919db79e-a3bd-4896-89a1-1d9e0017be10",
   "metadata": {},
   "source": [
    "# Multiple Sclerosis Lesion Segmentation Dataset Overview\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f36701b-9820-49dd-8553-ca671481b49c",
   "metadata": {},
   "source": [
    "\n",
    "- Introduction to the challenge {evaluation metrics, comparrison to other datasets} (with link to website [https://iplab.dmi.unict.it/mfs/ms-les-seg/] and paper)\n",
    "- Introduction to the Dataset (creation, pre-processing, contents and distribution data with visualizations)\n",
    "\n",
    "Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4026f2-e84e-4427-8ab1-eff9d1b54d98",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What have winning strategies been so far?\n",
    "\n",
    "1) Ensamble methods\n",
    "2) Different Registration Spaces\n",
    "3) Selecting only the specific data from the image that can better find the segmentation mask\n",
    "4) Using only ONE modality (FLAIR) and performing Data Augmentations on it + Deep CNN Architecture\n",
    "5) nnU-Net for automatic configuration to the dataset (Interesting)\n",
    "6) Usage of Mamba-based LightM-UNet\n",
    "7) Focal Loss + DICE Loss\n",
    "\n",
    "---\n",
    "\n",
    "## Personal Goals\n",
    "\n",
    "1) Parameter Efficiency\n",
    "2) (XAI) Explainable Architecture\n",
    "3) Segmentation Accuracy (Mean DICE >= 60)\n",
    "4) [Optional] --> Single Model (No Ensambles)\n",
    "\n",
    "---\n",
    "\n",
    "## Personal Model Proposition\n",
    "\n",
    "1) Use ALL modalities\n",
    "2) Preprocess with (Distance Based Labelling, Multi-Sized Labelling ~ Could aid the MoE variant)\n",
    "3) nnU-Net + MoE (+ Deep Supervision ?)\n",
    "\n",
    "### Optional\n",
    "\n",
    "- Bring the model visualization to Extended Reality Domain ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b3a0a-2f43-44f9-a898-28f1fa46b75a",
   "metadata": {},
   "source": [
    "# The Experiments\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293bc032-fddb-44a7-8db3-cd8bdf0315e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from skimage.transform import resize\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13613113-a191-4399-9b42-9f6df49bc6e9",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing\n",
    "\n",
    "- Expand more on the performed preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2027badd-35e2-42ac-b423-e85e9e99d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nifti(file_path):\n",
    "    return nib.load(file_path).get_fdata()\n",
    "\n",
    "def normalize_intensity(img):\n",
    "    # Normalize each modality independently (z-score)\n",
    "    img = img.astype(np.float32)\n",
    "    img = (img - np.mean(img)) / (np.std(img) + 1e-5)\n",
    "    return img\n",
    "\n",
    "def compute_distance_label(mask):\n",
    "    mask = mask.astype(bool)\n",
    "    pos_dist = distance_transform_edt(mask)\n",
    "    neg_dist = distance_transform_edt(~mask)\n",
    "    dist_map = pos_dist - neg_dist  # Signed distance transform\n",
    "    return dist_map\n",
    "\n",
    "def generate_multi_size_masks(mask, scales=[1.0, 0.5, 0.25]):\n",
    "    multi_res = []\n",
    "    for scale in scales:\n",
    "        if scale == 1.0:\n",
    "            multi_res.append(mask)\n",
    "        else:\n",
    "            resized = resize(mask, output_shape=tuple(int(s * scale) for s in mask.shape),\n",
    "                             order=1, preserve_range=True, anti_aliasing=True)\n",
    "            multi_res.append(resized.astype(mask.dtype))\n",
    "    return multi_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3515529-ff9f-4f47-935b-e09a357f22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_case(input_dir, output_dir, case_id):\n",
    "    flair = normalize_intensity(load_nifti(input_dir / f\"{case_id}_flair.nii.gz\"))\n",
    "    t1 = normalize_intensity(load_nifti(input_dir / f\"{case_id}_t1.nii.gz\"))\n",
    "    t2 = normalize_intensity(load_nifti(input_dir / f\"{case_id}_t2.nii.gz\"))\n",
    "    seg = load_nifti(input_dir / f\"{case_id}_seg.nii.gz\").astype(np.uint8)\n",
    "\n",
    "    # Stack input modalities into a tensor (C, D, H, W)\n",
    "    stacked = np.stack([flair, t1, t2], axis=0)\n",
    "    # input_tensor = torch.tensor(stacked, dtype=torch.float32).cuda()\n",
    "    # Use CPU Tensors instead\n",
    "    input_tensor = torch.tensor(stacked, dtype=torch.float32)\n",
    "    \n",
    "    # Compute distance map from segmentation mask\n",
    "    distance_map = compute_distance_label(seg)\n",
    "    # distance_tensor = torch.tensor(distance_map, dtype=torch.float32).unsqueeze(0).cuda()\n",
    "    # Use CPU Tensors instead\n",
    "    distance_tensor = torch.tensor(distance_map, dtype=torch.float32).unsqueeze(0)\n",
    "    \n",
    "    # Generate multi-size label masks\n",
    "    multi_size_masks = generate_multi_size_masks(seg)\n",
    "    # multi_size_tensors = [torch.tensor(m, dtype=torch.uint8).unsqueeze(0).cuda() for m in multi_size_masks]\n",
    "    # Use CPU Tensors instead\n",
    "    multi_size_tensors = [torch.tensor(m, dtype=torch.uint8).unsqueeze(0) for m in multi_size_masks]\n",
    "\n",
    "    \n",
    "    # Save all\n",
    "    output_case_dir = output_dir / case_id\n",
    "    output_case_dir.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(input_tensor, output_case_dir / \"input_tensor.pt\")\n",
    "    torch.save(distance_tensor, output_case_dir / \"distance_map.pt\")\n",
    "    for i, mask in enumerate(multi_size_tensors):\n",
    "        torch.save(mask, output_case_dir / f\"multi_size_mask_{i}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac6e5ec-ad39-42e9-8a62-c1140398e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preprocessing(root_path, output_path):\n",
    "    input_path = Path(root_path)\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    all_case_dirs = [d for d in input_path.iterdir() if d.is_dir()]\n",
    "    print(f\"Found {len(all_case_dirs)} cases.\")\n",
    "\n",
    "    for case_dir in tqdm(all_case_dirs):\n",
    "        case_id = case_dir.name  # e.g., MSLS_000\n",
    "        try:\n",
    "            preprocess_case(case_dir, output_path, case_id)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed on {case_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e6a0eb7-7da6-400d-9446-97773e286c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 93 cases.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6cda8ca64345d497cd609ccf088967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the preprocessing on the training set\n",
    "RAW_DATA_PATH = \"../data/01-Pre-Processed-Data/train\"\n",
    "OUTPUT_PATH = \"../data/02-Tensor-Data/train\"\n",
    "\n",
    "run_preprocessing(RAW_DATA_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc06f78a-1ed2-4e4b-90f7-391b10748569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 cases.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe863736a314bc5bbf893d22543810e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the preprocessing on the test set (with gt mask)\n",
    "RAW_DATA_PATH = \"../data/01-Pre-Processed-Data/test/test_MASK\"\n",
    "OUTPUT_PATH = \"../data/02-Tensor-Data/test/test_MASK\"\n",
    "\n",
    "run_preprocessing(RAW_DATA_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33b1eae6-3dee-4f09-aaf6-1bcfc5da9e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variation for the test set WITHOUT the gt seg mask\n",
    "def preprocess_case_inputs_only(input_dir, output_dir, case_id):\n",
    "    # Required input modalities only\n",
    "    required_files = {\n",
    "        \"flair\": input_dir / f\"{case_id}_flair.nii.gz\",\n",
    "        \"t1\": input_dir / f\"{case_id}_t1.nii.gz\",\n",
    "        \"t2\": input_dir / f\"{case_id}_t2.nii.gz\",\n",
    "    }\n",
    "\n",
    "    for key, path in required_files.items():\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"Missing file for '{key}': {path}\")\n",
    "\n",
    "    # Load and normalize each modality\n",
    "    flair = normalize_intensity(load_nifti(required_files[\"flair\"]))\n",
    "    t1 = normalize_intensity(load_nifti(required_files[\"t1\"]))\n",
    "    t2 = normalize_intensity(load_nifti(required_files[\"t2\"]))\n",
    "\n",
    "    # Stack modalities (C, D, H, W)\n",
    "    stacked = np.stack([flair, t1, t2], axis=0)\n",
    "    input_tensor = torch.tensor(stacked, dtype=torch.float32).cuda()\n",
    "\n",
    "    # Save preprocessed tensor\n",
    "    output_case_dir = output_dir / case_id\n",
    "    output_case_dir.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(input_tensor, output_case_dir / \"input_tensor.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "003c7bb6-5162-456c-8893-f286d27e1873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variation for the test set WITHOUT the gt seg mask\n",
    "def run_preprocessing_inputs_only(root_path, output_path):\n",
    "    input_path = Path(root_path)\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    all_case_dirs = [d for d in input_path.iterdir() if d.is_dir()]\n",
    "    print(f\"Found {len(all_case_dirs)} cases.\")\n",
    "\n",
    "    for case_dir in tqdm(all_case_dirs):\n",
    "        case_id = case_dir.name\n",
    "        try:\n",
    "            preprocess_case_inputs_only(case_dir, output_path, case_id)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Skipping {case_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dc58dd7-0257-4954-905d-404f8975dd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 cases.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f8f0a6ff7d4e6f9412371392392276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the preprocessing on the test set (no gt mask)\n",
    "RAW_DATA_PATH = \"../data/01-Pre-Processed-Data/test/test\"\n",
    "OUTPUT_PATH = \"../data/02-Tensor-Data/test/test\"\n",
    "\n",
    "run_preprocessing_inputs_only(RAW_DATA_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2f3830-e80c-47b9-9f3f-3b1268619f14",
   "metadata": {},
   "source": [
    "## Build the Dataset and Dataloaders for the MSLesSeg preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "813cbfe2-346f-48f1-b350-c0ef69e9ae0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Check the initial memory consuption (GPU) of the project\n",
    "import torch\n",
    "\n",
    "# Check initial GPU memory usage\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59da4863-42f3-4d2c-a675-c8cdc1eeeace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MSLesionDataset(Dataset):\n",
    "    def __init__(self, root_dir, include_labels=True, sample_ids=None, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        all_samples = sorted([d for d in self.root_dir.iterdir() if d.is_dir()])\n",
    "        \n",
    "        if sample_ids:\n",
    "            self.sample_dirs = [d for d in all_samples if d.name in sample_ids]\n",
    "        else:\n",
    "            self.sample_dirs = all_samples\n",
    "        \n",
    "        self.include_labels = include_labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def pad_to_match(self, tensor, reference_shape):\n",
    "        \"\"\"\n",
    "        Pads a tensor of shape (C, D, H, W) to match reference_shape.\n",
    "        \"\"\"\n",
    "        current_shape = tensor.shape\n",
    "        assert len(current_shape) == 4, f\"Expected 4D tensor (C, D, H, W), got {current_shape}\"\n",
    "        assert len(reference_shape) == 4, f\"Reference shape must be 4D, got {reference_shape}\"\n",
    "\n",
    "        pad_sizes = []\n",
    "        for i in range(3, 0, -1):  # W, H, D — reverse order\n",
    "            diff = reference_shape[i] - current_shape[i]\n",
    "            if diff < 0:\n",
    "                raise ValueError(f\"Tensor dimension {i} is larger than reference: {current_shape[i]} > {reference_shape[i]}\")\n",
    "            pad_sizes.extend([0, diff])  # pad only after\n",
    "\n",
    "        padded_tensor = F.pad(tensor, pad_sizes, mode='constant', value=0)\n",
    "        return padded_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        case_dir = self.sample_dirs[idx]\n",
    "\n",
    "        # === Load input tensor ===\n",
    "        input_tensor = torch.load(case_dir / \"input_tensor.pt\").float()\n",
    "        if input_tensor.dim() == 3:\n",
    "            input_tensor = input_tensor.unsqueeze(0)  # (D, H, W) → (1, D, H, W)\n",
    "\n",
    "        reference_shape = input_tensor.shape\n",
    "        print(f\"Input tensor shape: {reference_shape}\")\n",
    "\n",
    "        if self.transform:\n",
    "            input_tensor = self.transform(input_tensor)\n",
    "\n",
    "        if not self.include_labels:\n",
    "            return input_tensor, str(case_dir.name)\n",
    "\n",
    "        # === Load and pad distance map ===\n",
    "        distance_map = torch.load(case_dir / \"distance_map.pt\").float()\n",
    "        if distance_map.dim() == 3:\n",
    "            distance_map = distance_map.unsqueeze(0)\n",
    "\n",
    "        distance_map = self.pad_to_match(distance_map, reference_shape)\n",
    "\n",
    "        # === Load and pad multi-size masks ===\n",
    "        multi_size_masks = []\n",
    "        for i in range(3):\n",
    "            path = case_dir / f\"multi_size_mask_{i}.pt\"\n",
    "            if path.exists():\n",
    "                mask = torch.load(path).float()\n",
    "            else:\n",
    "                mask = torch.zeros_like(distance_map)\n",
    "\n",
    "            if mask.dim() == 3:\n",
    "                mask = mask.unsqueeze(0)\n",
    "\n",
    "            mask = self.pad_to_match(mask, reference_shape)\n",
    "            multi_size_masks.append(mask)\n",
    "\n",
    "        multi_masks_cat = torch.cat(multi_size_masks, dim=0)  # shape: (3, D, H, W)\n",
    "\n",
    "        return {\n",
    "            \"input\": input_tensor,\n",
    "            \"distance\": distance_map,\n",
    "            \"multi_masks\": multi_masks_cat,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02dcd821-2267-490c-a3cd-afa761eac4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03106a16-273a-4298-b82f-e15990258754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_dataloader(data_root, batch_size=2, num_workers=4, shuffle=True, include_labels=True, sample_ids=None):\n",
    "    \"\"\"\n",
    "    Get the DataLoader for the given dataset directory.\n",
    "\n",
    "    Args:\n",
    "        data_root: Path to the dataset root directory.\n",
    "        batch_size: Number of samples per batch.\n",
    "        num_workers: Number of workers for loading data in parallel.\n",
    "        shuffle: Whether to shuffle the data.\n",
    "        include_labels: Whether to include segmentation masks and distance maps.\n",
    "        sample_ids: A list of sample ids for a specific split (for k-fold).\n",
    "    \"\"\"\n",
    "    dataset = MSLesionDataset(\n",
    "        data_root,\n",
    "        include_labels=include_labels,\n",
    "        sample_ids=sample_ids\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5e4b760-d9c4-4bc1-b231-b8f08f48331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of some (non training) parameters\n",
    "NUM_WORKERS_TRAIN = 0\n",
    "NUM_WORKERS_VAL = 0\n",
    "NUM_WORKERS_TEST = 0\n",
    "\n",
    "BATCH_SIZE_TRAIN = 2\n",
    "BATCH_SIZE_VAL = 2\n",
    "BATCH_SIZE_TEST = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26a1d71b-abca-4933-8ec7-81302f763067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a49cd1c-7775-40f0-91ec-aae0a176be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "TRAIN_TENSOR_DATA_PATH = \"../data/02-Tensor-Data/train\"\n",
    "\n",
    "# List of all available case IDs in the train directory\n",
    "all_cases = sorted(os.listdir(TRAIN_TENSOR_DATA_PATH))\n",
    "random.seed(42)\n",
    "\n",
    "# For k-fold cross-validation, split train into train and validation (80-20)\n",
    "split = int(0.8 * len(all_cases))\n",
    "train_ids = all_cases[:split]\n",
    "val_ids = all_cases[split:]\n",
    "\n",
    "train_loader = get_dataloader(TRAIN_TENSOR_DATA_PATH, sample_ids=train_ids, batch_size=BATCH_SIZE_TRAIN, num_workers=NUM_WORKERS_TRAIN)\n",
    "val_loader = get_dataloader(TRAIN_TENSOR_DATA_PATH, sample_ids=val_ids, batch_size=BATCH_SIZE_VAL, num_workers=NUM_WORKERS_VAL, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9d350be-96b0-4456-bfe9-2176cdc52570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ca4c7fb-ef72-48f3-9583-19503337a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TENSOR_DATA_PATH_NOMASK = \"../data/02-Tensor-Data/test/test\"\n",
    "TEST_TENSOR_DATA_PATH_MASK = \"../data/02-Tensor-Data/test/test_MASK\"\n",
    "\n",
    "# Test set WITHOUT labels (for inference or prediction)\n",
    "test_loader_no_labels = get_dataloader(TEST_TENSOR_DATA_PATH_NOMASK, include_labels=False, batch_size=BATCH_SIZE_TEST, num_workers=NUM_WORKERS_TEST)\n",
    "\n",
    "# Test set WITH labels (for final evaluation or performance metrics)\n",
    "test_loader_with_labels = get_dataloader(TEST_TENSOR_DATA_PATH_MASK, include_labels=True, batch_size=BATCH_SIZE_TEST, num_workers=NUM_WORKERS_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a3370c6-c8fa-4871-9093-f5e13a8d9002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to perform K-Fold Cross Validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def get_k_fold_loaders(data_root, k=5, batch_size=BATCH_SIZE_TRAIN, num_workers=NUM_WORKERS_TRAIN):\n",
    "    # Get all case IDs\n",
    "    all_cases = sorted(os.listdir(data_root))\n",
    "    \n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_loaders = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(all_cases):\n",
    "        train_ids = [all_cases[i] for i in train_idx]\n",
    "        val_ids = [all_cases[i] for i in val_idx]\n",
    "\n",
    "        train_loader = get_dataloader(data_root, sample_ids=train_ids, batch_size=batch_size, num_workers=num_workers)\n",
    "        val_loader = get_dataloader(data_root, sample_ids=val_ids, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "        fold_loaders.append((train_loader, val_loader))\n",
    "\n",
    "    return fold_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a1847cf-501d-4233-8600-08376ed05e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_loaders = get_k_fold_loaders(TRAIN_TENSOR_DATA_PATH, k=5, batch_size=2, num_workers=4)\n",
    "\n",
    "# Example: Access the first fold\n",
    "# train_loader, val_loader = fold_loaders[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f0311-9b80-4bf0-b80d-996692a83815",
   "metadata": {},
   "source": [
    "### PyTorch Lightning DataModule\n",
    "\n",
    "This particular version of PyTorch Lightning, and in general from version 2.x onward require a ***LightningDataModule*** instead of passing the dataloaders directly to the ***.fit()*** method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac9f0867-1234-4a5d-b1e1-bd8802d56a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningDataModule\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class MSLesionSegmentationDataModule(LightningDataModule):\n",
    "    def __init__(self, data_root, train_split=0.8, batch_size=2, num_workers=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_root: Path to the root directory containing the dataset.\n",
    "            train_split: Fraction of data to use for training (default 80%).\n",
    "            batch_size: Batch size for the dataloader.\n",
    "            num_workers: Number of workers for the dataloader.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_root = data_root\n",
    "        self.train_split = train_split\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.train_dataloader_instance = None\n",
    "        self.val_dataloader_instance = None\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_dataloader_instance\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_dataloader_instance\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.val_dataloader_instance  # Optional, if you want to test on the validation set as well.\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"Prepare any datasets if needed, such as downloading or preprocessing.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"Assign train/val datasets for use in dataloaders.\"\"\"\n",
    "        # List all sample directories\n",
    "        all_samples = sorted([d for d in Path(self.data_root).iterdir() if d.is_dir()])\n",
    "\n",
    "        # Shuffle the sample list before splitting\n",
    "        random.seed(42)  # To ensure reproducibility\n",
    "        random.shuffle(all_samples)\n",
    "\n",
    "        # Split the dataset into training and validation sets\n",
    "        split_idx = int(len(all_samples) * self.train_split)\n",
    "        train_samples = all_samples[:split_idx]\n",
    "        val_samples = all_samples[split_idx:]\n",
    "\n",
    "        # Create the train and val dataloaders\n",
    "        self.train_dataloader_instance = get_dataloader(\n",
    "            self.data_root,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            sample_ids=[d.name for d in train_samples],\n",
    "        )\n",
    "\n",
    "        self.val_dataloader_instance = get_dataloader(\n",
    "            self.data_root,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            sample_ids=[d.name for d in val_samples],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e8bcef4-4bd3-429a-9722-5ac78eeea124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b08f0-2188-400b-b30e-88997983f659",
   "metadata": {},
   "source": [
    "## The Model's Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "726efd51-8791-4abc-9204-b5571ed1a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture of Experts with dynamic input size support via GAP.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_experts=4, expert_dim=512):\n",
    "        super(MoE, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "        self.gate = nn.Linear(input_dim, num_experts)\n",
    "        self.experts = nn.ModuleList([nn.Linear(input_dim, expert_dim) for _ in range(num_experts)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is expected to be (B, C), already pooled\n",
    "        gate_weights = F.softmax(self.gate(x), dim=-1)\n",
    "        expert_outputs = [expert(x) for expert in self.experts]\n",
    "        output = sum(gate_weights[:, i].unsqueeze(-1) * expert_outputs[i] for i in range(self.num_experts))\n",
    "        return output\n",
    "\n",
    "class UNetMoE(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, num_experts=4, expert_dim=512):\n",
    "        super(UNetMoE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(2)\n",
    "        )\n",
    "\n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv3d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(32, out_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        # Will be set dynamically after seeing one input\n",
    "        self.moe = None\n",
    "        self.num_experts = num_experts\n",
    "        self.expert_dim = expert_dim\n",
    "\n",
    "        self.deep_supervision = nn.Conv3d(128, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder[0:2](x)\n",
    "        enc2 = self.encoder[2:](enc1)\n",
    "        mid = self.middle(enc2)\n",
    "        dec = self.decoder[0:2](mid)\n",
    "        dec_out = self.decoder[2:](dec)\n",
    "\n",
    "        # Global pooling before MoE\n",
    "        pooled = F.adaptive_avg_pool3d(dec, 1).view(x.size(0), -1)\n",
    "\n",
    "        # Init MoE if not yet initialized\n",
    "        if self.moe is None:\n",
    "            self.moe = MoE(input_dim=pooled.size(1), num_experts=self.num_experts, expert_dim=self.expert_dim).to(x.device)\n",
    "\n",
    "        moe_out = self.moe(pooled)\n",
    "        deep_supervision_out = self.deep_supervision(mid)\n",
    "\n",
    "        return dec_out, deep_supervision_out, moe_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60727e14-353b-4fd3-9e3e-938472763e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "406d7a31-ebcf-4781-aefe-9a2ff343c4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 64, 64, 64]) torch.Size([2, 1, 64, 64, 64]) torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "# Example usage for testing (TO REMOVE)\n",
    "model = UNetMoE(in_channels=3, out_channels=1)\n",
    "dummy_input = torch.randn(2, 3, 128, 128, 128)  # e.g., batch size 2, 3 modalities\n",
    "dec_out, deep_out, moe_out = model(dummy_input)\n",
    "print(dec_out.shape, deep_out.shape, moe_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80d7e20b-62c4-493a-b457-2fcea6133015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa3c2123-75a5-45fa-ab0b-6887c61831ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1    [-1, 32, 128, 128, 128]           2,624\n",
      "              ReLU-2    [-1, 32, 128, 128, 128]               0\n",
      "            Conv3d-3    [-1, 64, 128, 128, 128]          55,360\n",
      "              ReLU-4    [-1, 64, 128, 128, 128]               0\n",
      "         MaxPool3d-5       [-1, 64, 64, 64, 64]               0\n",
      "            Conv3d-6      [-1, 128, 64, 64, 64]         221,312\n",
      "              ReLU-7      [-1, 128, 64, 64, 64]               0\n",
      "            Conv3d-8      [-1, 128, 64, 64, 64]         442,496\n",
      "              ReLU-9      [-1, 128, 64, 64, 64]               0\n",
      "           Conv3d-10       [-1, 64, 64, 64, 64]         221,248\n",
      "             ReLU-11       [-1, 64, 64, 64, 64]               0\n",
      "           Conv3d-12       [-1, 32, 64, 64, 64]          55,328\n",
      "             ReLU-13       [-1, 32, 64, 64, 64]               0\n",
      "           Conv3d-14        [-1, 1, 64, 64, 64]              33\n",
      "           Conv3d-15        [-1, 1, 64, 64, 64]             129\n",
      "================================================================\n",
      "Total params: 998,530\n",
      "Trainable params: 998,530\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 24.00\n",
      "Forward/backward pass size (MB): 4612.00\n",
      "Params size (MB): 3.81\n",
      "Estimated Total Size (MB): 4639.81\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# The Summary of the Architecture\n",
    "from torchsummary import summary\n",
    "\n",
    "# Set the device and the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNetMoE(in_channels=3, out_channels=1, num_experts=4, expert_dim=512).to(device)\n",
    "\n",
    "# Get the summary of the model (use an example input size, e.g., 128x128x128 for 3D)\n",
    "summary(model, (3, 128, 128, 128))  # (channels, depth, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "686f871e-cfea-440a-afd9-c950b441427c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 12.45 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ff77208-1cc9-4c68-8aa7-eaf9a3def39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the graphical visualization\n",
    "from torchviz import make_dot\n",
    "import gc\n",
    "\n",
    "# Dummy input\n",
    "dummy_input = torch.randn(1, 3, 128, 128, 128).cuda()  # Adjust for batch size and input shape\n",
    "\n",
    "# Explicitly instruct the model to not save the gratients to avoid computation graph creation\n",
    "with torch.no_grad():\n",
    "    # Forward pass through the model\n",
    "    output, _, _ = model(dummy_input)\n",
    "\n",
    "    # Visualize the graph\n",
    "    make_dot(output, params=dict(model.named_parameters())).render(\"model_architecture\", format=\"png\")\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Clear the non-essentialy GPU memory usage\n",
    "del dummy_input  # Delete the dummy input tensor\n",
    "del output       # Optionally, delete the output if you don't need it anymore\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87ecec9a-397d-492f-a6a6-0762cd22dfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 12.45 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bbba010-2e2d-4aa1-9c7f-86d110084e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the GPU memory used for the dummy input and re-initialize the model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNetMoE(in_channels=3, out_channels=1, num_experts=4, expert_dim=512).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52a73d93-c9ae-41ba-9bb8-b297067a69cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 11.94 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dcd022-a711-4931-b46c-336f8565d581",
   "metadata": {},
   "source": [
    "## Visualized Model Architecture\n",
    "\n",
    "![Model Architecture Plot](./model_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be907f2a-2600-407f-8649-42965a1fb3d1",
   "metadata": {},
   "source": [
    "## The Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b2d3471-ee79-41ff-b176-929864412593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.segmentation import DiceScore\n",
    "\n",
    "class MSLeasionSegmentationModel(pl.LightningModule):\n",
    "    def __init__(self, model, lr=1e-4):\n",
    "        super(MSLeasionSegmentationModel, self).__init__()\n",
    "        \n",
    "        # Initialize the model (nnU-Net + MoE)\n",
    "        self.model = model\n",
    "        \n",
    "        # Learning rate for optimizer\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Set up Dice Score metric for evaluation\n",
    "        self.dice_metric = DiceScore(num_classes=2)  # Binary segmentation: lesion or background\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass (output, deep supervision, MoE output)\n",
    "        output, deep_supervision, moe_output = self.model(x)\n",
    "        return output, deep_supervision, moe_output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Extract input data and target segmentation masks from batch\n",
    "        x = batch['input_tensor']  # Shape: (B, 3, D, H, W)\n",
    "        y = batch['segmentation_mask']  # Shape: (B, 1, D, H, W)\n",
    "        \n",
    "        # Get model output\n",
    "        output, _, _ = self(x)\n",
    "        \n",
    "        # Compute loss (using binary cross entropy for binary segmentation)\n",
    "        loss = F.binary_cross_entropy_with_logits(output, y)\n",
    "        \n",
    "        # Log the loss\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Extract input data and target segmentation masks from batch\n",
    "        x = batch['input']  # Change 'input_tensor' to 'input' to match dataset\n",
    "        y = batch['multi_masks']  # Change 'segmentation_mask' to 'multi_masks' to match dataset\n",
    "        \n",
    "        # Get model output\n",
    "        output, _, _ = self(x)\n",
    "        \n",
    "        # Calculate the Dice score\n",
    "        dice_score = self.dice_metric(output.sigmoid(), y)\n",
    "        \n",
    "        # Log validation metrics\n",
    "        self.log('val_dice', dice_score, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return dice_score\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Extract input data and target segmentation masks from batch\n",
    "        x = batch['input_tensor']  # Shape: (B, 3, D, H, W)\n",
    "        y = batch['segmentation_mask']  # Shape: (B, 1, D, H, W)\n",
    "        \n",
    "        # Get model output\n",
    "        output, _, _ = self(x)\n",
    "        \n",
    "        # Calculate the Dice score\n",
    "        dice_score = self.dice_metric(output.sigmoid(), y)\n",
    "        \n",
    "        # Log test metrics\n",
    "        self.log('test_dice', dice_score, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return dice_score\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Set up Adam optimizer with the specified learning rate\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        # Optionally, add a scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "        \n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_dice'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b884f-771e-4852-be17-23973479e5a5",
   "metadata": {},
   "source": [
    "## The Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "330d4d97-5326-4dcf-b27e-2f3a57de10f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdrnnrw00m10c351s\u001b[0m (\u001b[33mfpv-perceivelab-unict\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "# Register a free account with Weights and Biases, and create a new project in order to obtain an API Key for the training\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the env variable\n",
    "api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "# Login to wandb\n",
    "if api_key:\n",
    "    os.environ[\"WANDB_API_KEY\"] = api_key\n",
    "    wandb.login()\n",
    "else:\n",
    "    print(\"❌ WANDB_API_KEY not found in .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd78050f-09dd-4251-a8b1-468756f2f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Weights and Biases logger\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project='MSLesSeg-4-ICPR',     # Change to your actual project name\n",
    "    name='nnUNet_MoE_run1', # A specific run name\n",
    "    log_model=True          # Optional: log model checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cc45e04-32b8-4824-a118-01950a2d1976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the multiprocessing start to 'spawn' instead of 'fork' due to CUDA issues with the Dataloader\n",
    "import multiprocessing\n",
    "import torch\n",
    "\n",
    "# Set the start method for multiprocessing\n",
    "multiprocessing.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43d7e0a3-b3d2-4688-bf74-66b0b8054b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 11.94 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f7969d2-7bdf-4b36-bd8e-db0e5413527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the profiler to keep track of GPU overhead\n",
    "from pytorch_lightning.profilers import PyTorchProfiler\n",
    "\n",
    "# Slightly more sophisticated profiler\n",
    "profiler = PyTorchProfiler(\n",
    "    on_trace_ready=lambda prof: print(prof.key_averages().table(\n",
    "        sort_by=\"self_cuda_memory_usage\", row_limit=15  # Change as needed\n",
    "    )),\n",
    "    profile_memory=True,\n",
    "    record_shapes=True,\n",
    "    with_stack=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60716b23-a05b-4ca7-8c5c-611575ea6a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 11.94 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "318e0b5b-a55c-4dd2-8aa0-a2c43dab9569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 998530\n",
      "Estimated model size: 3.81 MB\n"
     ]
    }
   ],
   "source": [
    "# A computation to estimate the GPU memory consumption of the model\n",
    "\n",
    "# Print the total number of parameters in your model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "\n",
    "# Calculate the approximate memory size of the model (in MB)\n",
    "model_size_MB = total_params * 4 / (1024 ** 2)  # 4 bytes per parameter for float32\n",
    "print(f\"Estimated model size: {model_size_MB:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e89832cf-b12f-4996-92f5-4078eeb524a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17ec947e-62a9-4280-9b98-76de9eb29f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Run garbage collection to clean up unused objects\n",
    "gc.collect()\n",
    "\n",
    "# Empty the PyTorch cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c82afa2-56b6-4202-8aba-365d4206b4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drew/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250505_003409-od6kl314</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR/runs/od6kl314' target=\"_blank\">nnUNet_MoE_run1</a></strong> to <a href='https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR' target=\"_blank\">https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR/runs/od6kl314' target=\"_blank\">https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR/runs/od6kl314</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type      | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | model       | UNetMoE   | 998 K  | train\n",
      "1 | dice_metric | DiceScore | 0      | train\n",
      "--------------------------------------------------\n",
      "998 K     Trainable params\n",
      "0         Non-trainable params\n",
      "998 K     Total params\n",
      "3.994     Total estimated model params size (MB)\n",
      "20        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21126f38257549f0ab2de8c569f5e469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drew/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/drew/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([3, 182, 218, 182])\n",
      "Input tensor shape: torch.Size([3, 182, 218, 182])\n",
      "Input tensor shape: torch.Size([3, 182, 218, 182])\n",
      "Input tensor shape: torch.Size([3, 182, 218, 182])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Predictions and targets are expected to have the same shape, but got torch.Size([4, 1, 91, 109, 91]) and torch.Size([4, 3, 182, 218, 182]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     35\u001b[39m trainer = Trainer(\n\u001b[32m     36\u001b[39m     accelerator=\u001b[33m\"\u001b[39m\u001b[33mgpu\u001b[39m\u001b[33m\"\u001b[39m,        \u001b[38;5;66;03m# or \"cpu\" or \"auto\"\u001b[39;00m\n\u001b[32m     37\u001b[39m     devices=\u001b[32m1\u001b[39m,                \u001b[38;5;66;03m# Number of GPUs to use\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# profiler=profiler          # Ensures that the model is being profiled for efficient usage\u001b[39;00m\n\u001b[32m     43\u001b[39m )\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Train the model using the data module\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     51\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    592\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1017\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1054\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1053\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m   1056\u001b[39m         \u001b[38;5;28mself\u001b[39m.fit_loop.run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1083\u001b[39m, in \u001b[36mTrainer._run_sanity_check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1080\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_start\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1082\u001b[39m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m \u001b[43mval_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1085\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_end\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1087\u001b[39m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    177\u001b[39m     context_manager = torch.no_grad\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:145\u001b[39m, in \u001b[36m_EvaluationLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28mself\u001b[39m.batch_progress.is_last_batch = data_fetcher.done\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    147\u001b[39m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:437\u001b[39m, in \u001b[36m_EvaluationLoop._evaluation_step\u001b[39m\u001b[34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[39m\n\u001b[32m    431\u001b[39m hook_name = \u001b[33m\"\u001b[39m\u001b[33mtest_step\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.testing \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvalidation_step\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    432\u001b[39m step_args = (\n\u001b[32m    433\u001b[39m     \u001b[38;5;28mself\u001b[39m._build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[32m    436\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.batch_progress.increment_processed()\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[32m    442\u001b[39m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:328\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    331\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:412\u001b[39m, in \u001b[36mStrategy.validation_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mvalidation_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mMSLeasionSegmentationModel.validation_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m     47\u001b[39m output, _, _ = \u001b[38;5;28mself\u001b[39m(x)\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Calculate the Dice score\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m dice_score = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdice_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Log validation metrics\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mself\u001b[39m.log(\u001b[33m'\u001b[39m\u001b[33mval_dice\u001b[39m\u001b[33m'\u001b[39m, dice_score, on_step=\u001b[38;5;28;01mTrue\u001b[39;00m, on_epoch=\u001b[38;5;28;01mTrue\u001b[39;00m, prog_bar=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/torchmetrics/metric.py:315\u001b[39m, in \u001b[36mMetric.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    313\u001b[39m     \u001b[38;5;28mself\u001b[39m._forward_cache = \u001b[38;5;28mself\u001b[39m._forward_full_state_update(*args, **kwargs)\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     \u001b[38;5;28mself\u001b[39m._forward_cache = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_reduce_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_cache\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/torchmetrics/metric.py:384\u001b[39m, in \u001b[36mMetric._forward_reduce_state_update\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28mself\u001b[39m._enable_grad = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# allow grads for batch computation\u001b[39;00m\n\u001b[32m    383\u001b[39m \u001b[38;5;66;03m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    385\u001b[39m batch_val = \u001b[38;5;28mself\u001b[39m.compute()\n\u001b[32m    387\u001b[39m \u001b[38;5;66;03m# reduce batch and global state\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/torchmetrics/metric.py:559\u001b[39m, in \u001b[36mMetric._wrap_update.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    551\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mExpected all tensors to be on\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n\u001b[32m    552\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    553\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mEncountered different devices in metric calculation (see stacktrace for details).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    554\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m This could be due to the metric class not being on the same device as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    557\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m device corresponds to the device of the input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    558\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_on_cpu:\n\u001b[32m    562\u001b[39m     \u001b[38;5;28mself\u001b[39m._move_list_states_to_cpu()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/torchmetrics/metric.py:549\u001b[39m, in \u001b[36mMetric._wrap_update.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m._enable_grad):\n\u001b[32m    548\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    550\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    551\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mExpected all tensors to be on\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/torchmetrics/segmentation/dice.py:133\u001b[39m, in \u001b[36mDiceScore.update\u001b[39m\u001b[34m(self, preds, target)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds: Tensor, target: Tensor) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    132\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Update the state with new data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     numerator, denominator, support = \u001b[43m_dice_score_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minclude_background\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_format\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28mself\u001b[39m.numerator.append(numerator)\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m.denominator.append(denominator)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/torchmetrics/functional/segmentation/dice.py:56\u001b[39m, in \u001b[36m_dice_score_update\u001b[39m\u001b[34m(preds, target, num_classes, include_background, input_format)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_dice_score_update\u001b[39m(\n\u001b[32m     49\u001b[39m     preds: Tensor,\n\u001b[32m     50\u001b[39m     target: Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m     53\u001b[39m     input_format: Literal[\u001b[33m\"\u001b[39m\u001b[33mone-hot\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mone-hot\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     54\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[Tensor, Tensor, Tensor]:\n\u001b[32m     55\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Update the state with the current prediction and target.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[43m_check_same_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m input_format == \u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     59\u001b[39m         preds = torch.nn.functional.one_hot(preds, num_classes=num_classes).movedim(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/torchmetrics/utilities/checks.py:39\u001b[39m, in \u001b[36m_check_same_shape\u001b[39m\u001b[34m(preds, target)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check that predictions and target have the same shape, else raise error.\"\"\"\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m preds.shape != target.shape:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     40\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPredictions and targets are expected to have the same shape, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     41\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: Predictions and targets are expected to have the same shape, but got torch.Size([4, 1, 91, 109, 91]) and torch.Size([4, 3, 182, 218, 182])."
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "\n",
    "# Initialize the model\n",
    "model = MSLeasionSegmentationModel(model=UNetMoE(in_channels=3, out_channels=1, num_experts=4, expert_dim=512))\n",
    "\n",
    "# Set up callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_dice',  # Monitor the validation Dice score\n",
    "    dirpath='checkpoints/',\n",
    "    filename='best_checkpoint',\n",
    "    save_top_k=1,\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_dice',  # Early stopping based on the validation Dice score\n",
    "    patience=10,\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "# Specify the data root where the preprocessed data is located\n",
    "data_root = TRAIN_TENSOR_DATA_PATH  # Replace this with your actual path\n",
    "\n",
    "# Instantiate the PyTorch Lightning DataModule\n",
    "data_module = MSLesionSegmentationDataModule(\n",
    "    data_root=data_root,    # Dataset root directory\n",
    "    train_split=0.8,         # Use an 80/20 split for train/validation\n",
    "    batch_size=4,            # Adjust batch size as needed\n",
    "    num_workers=0,           # Number of workers for data loading\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",        # or \"cpu\" or \"auto\"\n",
    "    devices=1,                # Number of GPUs to use\n",
    "    precision=16,             # Sets a given precision to save GPU memory\n",
    "    max_epochs=100,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    logger=wandb_logger,       # Ensure `wandb_logger` is initialized beforehand\n",
    "    # profiler=profiler          # Ensures that the model is being profiled for efficient usage\n",
    ")\n",
    "\n",
    "# Train the model using the data module\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee0a5d-61d4-4561-a731-b24483798f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "trainer.test(model, test_dataloader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6833331b-df4e-485b-8b26-e617ad583746",
   "metadata": {},
   "source": [
    "## Visualizing the Learned Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2febf704-6408-4a85-8d9c-19b88747f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to extract features from the model\n",
    "def extract_features(model, dataloader, device='cuda'):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, target = data['image'].to(device), data['mask'].to(device)  # Replace 'image' and 'mask' keys as per your dataset\n",
    "            \n",
    "            # Forward pass through the model\n",
    "            output = model(inputs)  # This is where we extract the feature; adjust according to your model's architecture\n",
    "            \n",
    "            # Extract features from MoE (or any other layer you want to visualize)\n",
    "            # Here I assume output is the final feature map after the MoE layer\n",
    "            feature_map = output.view(output.size(0), -1)  # Flatten the features to (batch_size, features)\n",
    "            features.append(feature_map.cpu().numpy())\n",
    "            labels.append(target.cpu().numpy())  # Add the target (or ground truth) labels for color coding in t-SNE\n",
    "\n",
    "    features = np.concatenate(features, axis=0)  # Combine all the feature maps\n",
    "    labels = np.concatenate(labels, axis=0)  # Combine all the labels\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "# Function to apply t-SNE on features\n",
    "def plot_tsne(features, labels):\n",
    "    # Standardize the features (optional but recommended for t-SNE)\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(features)\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=labels, cmap='jet', alpha=0.5)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('t-SNE Visualization of Learned Representations')\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage\n",
    "train_dataloader = # Your train DataLoader here\n",
    "\n",
    "# Extract features and labels from the model\n",
    "features, labels = extract_features(model, train_dataloader, device='cuda')\n",
    "\n",
    "# Plot t-SNE visualization\n",
    "plot_tsne(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4089cdf9-90ee-4700-8ec5-2c246a69d848",
   "metadata": {},
   "source": [
    "## Post Hoc Model Explainability - GradCAM++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee0469-f55a-4c46-9c72-a7ba80ad41b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from captum.attr import LayerGradCam, LayerGradCamPlusPlus\n",
    "from captum.attr import GuidedBackprop\n",
    "\n",
    "# Function to visualize GradCAM++ output\n",
    "def visualize_gradcam_plus_plus(model, input_tensor, target_class=None, layer_name='decoder', device='cuda'):\n",
    "    # Move input tensor to the appropriate device\n",
    "    input_tensor = input_tensor.to(device)\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Create GradCAM++ object for the specified layer\n",
    "    gradcam_pp = LayerGradCamPlusPlus(model, model.decoder[2])  # Assuming 'decoder' is your final layer, adjust accordingly.\n",
    "    \n",
    "    # Apply GradCAM++ to input tensor\n",
    "    attributions = gradcam_pp.attribute(input_tensor, target=target_class)\n",
    "    \n",
    "    # Convert attributions to numpy\n",
    "    attributions = attributions.cpu().detach().numpy()\n",
    "    \n",
    "    # Normalize the heatmap\n",
    "    heatmap = np.sum(attributions[0], axis=0)  # Summing over channels for a single-channel heatmap\n",
    "    heatmap = np.maximum(heatmap, 0)  # ReLU to ignore negative values\n",
    "    heatmap = cv2.resize(heatmap, (input_tensor.shape[2], input_tensor.shape[3]))  # Resize to input size\n",
    "    heatmap = cv2.normalize(heatmap, None, 0, 1, cv2.NORM_MINMAX)  # Normalize the heatmap to [0, 1]\n",
    "    \n",
    "    # Convert the original image to numpy and rescale\n",
    "    input_image = input_tensor[0].cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    input_image = cv2.resize(input_image, (input_tensor.shape[2], input_tensor.shape[3]))  # Resize to input size\n",
    "    \n",
    "    # Create a colormap for the heatmap\n",
    "    colormap = plt.get_cmap('jet')\n",
    "    colored_heatmap = colormap(heatmap)  # Apply colormap\n",
    "    \n",
    "    # Overlay the heatmap on top of the original image\n",
    "    superimposed_img = np.uint8(input_image * 255)  # Convert original image back to [0, 255] range\n",
    "    superimposed_img = cv2.addWeighted(superimposed_img, 0.7, np.uint8(colored_heatmap[:, :, :3] * 255), 0.3, 0)\n",
    "    \n",
    "    # Plot the result\n",
    "    plt.imshow(superimposed_img)\n",
    "    plt.title('GradCAM++ Heatmap')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage (assuming `model` is your trained model and `input_tensor` is an input image)\n",
    "input_tensor = torch.randn(1, 3, 128, 128).to(device)  # Example input, use your actual input tensor\n",
    "target_class = None  # You can provide a target class or leave as None for model's top predicted class\n",
    "\n",
    "visualize_gradcam_plus_plus(model, input_tensor, target_class=target_class, layer_name='decoder', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb9090-3260-49ce-be05-a03c2d122e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size of the stored (pre-processed) Tensors\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def inspect_pt_file(file_path):\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(f\"File does not exist: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    data = torch.load(file_path, map_location='cpu')\n",
    "\n",
    "    print(f\"\\nLoaded file: {file_path}\")\n",
    "    \n",
    "    if isinstance(data, torch.Tensor):\n",
    "        print(f\"Single Tensor - Shape: {data.shape}\")\n",
    "    \n",
    "    elif isinstance(data, dict):\n",
    "        print(\"Dictionary of tensors:\")\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                print(f\"  {key}: shape = {value.shape}\")\n",
    "            else:\n",
    "                print(f\"  {key}: type = {type(value)}\")\n",
    "    \n",
    "    elif isinstance(data, (list, tuple)):\n",
    "        print(f\"{type(data).__name__} of tensors:\")\n",
    "        for idx, item in enumerate(data):\n",
    "            if isinstance(item, torch.Tensor):\n",
    "                print(f\"  [{idx}]: shape = {item.shape}\")\n",
    "            else:\n",
    "                print(f\"  [{idx}]: type = {type(item)}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Unknown type loaded: {type(data)}\")\n",
    "\n",
    "INPUT_PATH_PREFIX = \"../data/02-Tensor-Data/train/MSLS_000/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af84775-fb63-4bfe-9fda-a1b238a7a20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Tensor Shape (Stacked Modalities)\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"input_tensor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753172b8-22b4-4290-8104-f0edfad737ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance Map Shape\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"distance_map.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0192a99-65c0-491a-97d2-d0b3fc847c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Size Mask - 0\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"multi_size_mask_0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f505b915-04d4-437b-bcf7-e0760cac75fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Size Mask - 1\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"multi_size_mask_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fe78ce-84b5-4936-a774-88c2f457b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Size Mask - 2\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"multi_size_mask_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feeedd4-a2d9-4e3d-900d-d3ca5a68d18b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

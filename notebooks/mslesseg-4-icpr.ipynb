{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "919db79e-a3bd-4896-89a1-1d9e0017be10",
   "metadata": {},
   "source": [
    "# Multiple Sclerosis Lesion Segmentation Dataset Overview\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f36701b-9820-49dd-8553-ca671481b49c",
   "metadata": {},
   "source": [
    "\n",
    "- Introduction to the challenge {evaluation metrics, comparrison to other datasets} (with link to website [https://iplab.dmi.unict.it/mfs/ms-les-seg/] and paper)\n",
    "- Introduction to the Dataset (creation, pre-processing, contents and distribution data with visualizations)\n",
    "\n",
    "Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4026f2-e84e-4427-8ab1-eff9d1b54d98",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What have winning strategies been so far?\n",
    "\n",
    "1) Ensamble methods\n",
    "2) Different Registration Spaces\n",
    "3) Selecting only the specific data from the image that can better find the segmentation mask\n",
    "4) Using only ONE modality (FLAIR) and performing Data Augmentations on it + Deep CNN Architecture\n",
    "5) nnU-Net for automatic configuration to the dataset (Interesting)\n",
    "6) Usage of Mamba-based LightM-UNet\n",
    "7) Focal Loss + DICE Loss\n",
    "\n",
    "---\n",
    "\n",
    "## Personal Goals\n",
    "\n",
    "1) Parameter Efficiency\n",
    "2) (XAI) Explainable Architecture\n",
    "3) Segmentation Accuracy (Mean DICE >= 60)\n",
    "4) [Optional] --> Single Model (No Ensambles)\n",
    "\n",
    "---\n",
    "\n",
    "## Personal Model Proposition\n",
    "\n",
    "1) Use ALL modalities\n",
    "2) Preprocess with (Distance Based Labelling, Multi-Sized Labelling ~ Could aid the MoE variant)\n",
    "3) nnU-Net + MoE (+ Deep Supervision ?)\n",
    "\n",
    "### Optional\n",
    "\n",
    "- Bring the model visualization to Extended Reality Domain ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b3a0a-2f43-44f9-a898-28f1fa46b75a",
   "metadata": {},
   "source": [
    "# The Code\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293bc032-fddb-44a7-8db3-cd8bdf0315e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from skimage.transform import resize\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2027badd-35e2-42ac-b423-e85e9e99d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nifti(file_path):\n",
    "    return nib.load(file_path).get_fdata()\n",
    "\n",
    "def normalize_intensity(img):\n",
    "    # Normalize each modality independently (z-score)\n",
    "    img = img.astype(np.float32)\n",
    "    img = (img - np.mean(img)) / (np.std(img) + 1e-5)\n",
    "    return img\n",
    "\n",
    "def compute_distance_label(mask):\n",
    "    mask = mask.astype(bool)\n",
    "    pos_dist = distance_transform_edt(mask)\n",
    "    neg_dist = distance_transform_edt(~mask)\n",
    "    dist_map = pos_dist - neg_dist  # Signed distance transform\n",
    "    return dist_map\n",
    "\n",
    "def generate_multi_size_masks(mask, scales=[1.0, 0.5, 0.25]):\n",
    "    multi_res = []\n",
    "    for scale in scales:\n",
    "        if scale == 1.0:\n",
    "            multi_res.append(mask)\n",
    "        else:\n",
    "            resized = resize(mask, output_shape=tuple(int(s * scale) for s in mask.shape),\n",
    "                             order=1, preserve_range=True, anti_aliasing=True)\n",
    "            multi_res.append(resized.astype(mask.dtype))\n",
    "    return multi_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3515529-ff9f-4f47-935b-e09a357f22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_case(input_dir, output_dir, case_id):\n",
    "    flair = normalize_intensity(load_nifti(input_dir / f\"{case_id}_flair.nii.gz\"))\n",
    "    t1 = normalize_intensity(load_nifti(input_dir / f\"{case_id}_t1.nii.gz\"))\n",
    "    t2 = normalize_intensity(load_nifti(input_dir / f\"{case_id}_t2.nii.gz\"))\n",
    "    seg = load_nifti(input_dir / f\"{case_id}_seg.nii.gz\").astype(np.uint8)\n",
    "\n",
    "    # Stack input modalities into a tensor (C, D, H, W)\n",
    "    stacked = np.stack([flair, t1, t2], axis=0)\n",
    "    input_tensor = torch.tensor(stacked, dtype=torch.float32).cuda()\n",
    "\n",
    "    # Compute distance map from segmentation mask\n",
    "    distance_map = compute_distance_label(seg)\n",
    "    distance_tensor = torch.tensor(distance_map, dtype=torch.float32).unsqueeze(0).cuda()\n",
    "\n",
    "    # Generate multi-size label masks\n",
    "    multi_size_masks = generate_multi_size_masks(seg)\n",
    "    multi_size_tensors = [torch.tensor(m, dtype=torch.uint8).unsqueeze(0).cuda() for m in multi_size_masks]\n",
    "\n",
    "    # Save all\n",
    "    output_case_dir = output_dir / case_id\n",
    "    output_case_dir.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(input_tensor, output_case_dir / \"input_tensor.pt\")\n",
    "    torch.save(distance_tensor, output_case_dir / \"distance_map.pt\")\n",
    "    for i, mask in enumerate(multi_size_tensors):\n",
    "        torch.save(mask, output_case_dir / f\"multi_size_mask_{i}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac6e5ec-ad39-42e9-8a62-c1140398e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preprocessing(root_path, output_path):\n",
    "    input_path = Path(root_path)\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    all_case_dirs = [d for d in input_path.iterdir() if d.is_dir()]\n",
    "    print(f\"Found {len(all_case_dirs)} cases.\")\n",
    "\n",
    "    for case_dir in tqdm(all_case_dirs):\n",
    "        case_id = case_dir.name  # e.g., MSLS_000\n",
    "        try:\n",
    "            preprocess_case(case_dir, output_path, case_id)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed on {case_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e6a0eb7-7da6-400d-9446-97773e286c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 93 cases.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3898edd29ec248b9ade9c59576aa9e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the preprocessing on the training set\n",
    "RAW_DATA_PATH = \"../data/01-Pre-Processed-Data/train\"\n",
    "OUTPUT_PATH = \"../data/02-Tensor-Data/train\"\n",
    "\n",
    "run_preprocessing(RAW_DATA_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc06f78a-1ed2-4e4b-90f7-391b10748569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 cases.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd91821ad48148c18c4c4fc21301417f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the preprocessing on the test set (with gt mask)\n",
    "RAW_DATA_PATH = \"../data/01-Pre-Processed-Data/test/test_MASK\"\n",
    "OUTPUT_PATH = \"../data/02-Tensor-Data/test/test_MASK\"\n",
    "\n",
    "run_preprocessing(RAW_DATA_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33b1eae6-3dee-4f09-aaf6-1bcfc5da9e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variation for the test set WITHOUT the gt seg mask\n",
    "def preprocess_case_inputs_only(input_dir, output_dir, case_id):\n",
    "    # Required input modalities only\n",
    "    required_files = {\n",
    "        \"flair\": input_dir / f\"{case_id}_flair.nii.gz\",\n",
    "        \"t1\": input_dir / f\"{case_id}_t1.nii.gz\",\n",
    "        \"t2\": input_dir / f\"{case_id}_t2.nii.gz\",\n",
    "    }\n",
    "\n",
    "    for key, path in required_files.items():\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"Missing file for '{key}': {path}\")\n",
    "\n",
    "    # Load and normalize each modality\n",
    "    flair = normalize_intensity(load_nifti(required_files[\"flair\"]))\n",
    "    t1 = normalize_intensity(load_nifti(required_files[\"t1\"]))\n",
    "    t2 = normalize_intensity(load_nifti(required_files[\"t2\"]))\n",
    "\n",
    "    # Stack modalities (C, D, H, W)\n",
    "    stacked = np.stack([flair, t1, t2], axis=0)\n",
    "    input_tensor = torch.tensor(stacked, dtype=torch.float32).cuda()\n",
    "\n",
    "    # Save preprocessed tensor\n",
    "    output_case_dir = output_dir / case_id\n",
    "    output_case_dir.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(input_tensor, output_case_dir / \"input_tensor.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "003c7bb6-5162-456c-8893-f286d27e1873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variation for the test set WITHOUT the gt seg mask\n",
    "def run_preprocessing_inputs_only(root_path, output_path):\n",
    "    input_path = Path(root_path)\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    all_case_dirs = [d for d in input_path.iterdir() if d.is_dir()]\n",
    "    print(f\"Found {len(all_case_dirs)} cases.\")\n",
    "\n",
    "    for case_dir in tqdm(all_case_dirs):\n",
    "        case_id = case_dir.name\n",
    "        try:\n",
    "            preprocess_case_inputs_only(case_dir, output_path, case_id)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Skipping {case_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dc58dd7-0257-4954-905d-404f8975dd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 cases.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922b91484fc541fdaa8b2304e8f13c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the preprocessing on the test set (no gt mask)\n",
    "RAW_DATA_PATH = \"../data/01-Pre-Processed-Data/test/test\"\n",
    "OUTPUT_PATH = \"../data/02-Tensor-Data/test/test\"\n",
    "\n",
    "run_preprocessing_inputs_only(RAW_DATA_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0f1e1e-edb1-4b81-915d-ed2e698330a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

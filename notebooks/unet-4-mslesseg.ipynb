{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15b66f1a-394d-4b5b-ab06-ea2774295e3c",
   "metadata": {},
   "source": [
    "# U-Net 4 Multiple Sclerosis Lesion Segmentation - ICPR Challenge\n",
    "### Authors: Andrew R. Darnall, Giovanni Spadaro @ UniCT\n",
    "---\n",
    "\n",
    "## üéØ Competition Objective: MS Lesion Segmentation\n",
    "\n",
    "The central goal of this competition is the **automatic segmentation of Multiple Sclerosis (MS) lesions** using **multi-modal MRI data** and **deep learning algorithms**.\n",
    "\n",
    "### üß™ Provided Data\n",
    "Participants were given:\n",
    "- **MRI scans** in three modalities:\n",
    "  - **FLAIR**\n",
    "  - **T1-weighted (T1-w)**\n",
    "  - **T2-weighted (T2-w)**\n",
    "- **Ground-truth segmentation masks**, which are:\n",
    "  - **Binary masks**:  \n",
    "    - **White pixels** ‚Üí MS lesion regions  \n",
    "    - **Black pixels** ‚Üí Background\n",
    "\n",
    "### üß† Task Description\n",
    "- Participants could use **any or all modalities**, along with the ground-truth labels, to:\n",
    "  - Develop **deep learning-based models** for **automatic lesion segmentation**\n",
    "- MS lesions appear as **irregular clusters of pixels** with **high variability in size and shape**\n",
    "- These lesions are often **difficult to detect** via visual inspection, requiring **expert-level interpretation**\n",
    "\n",
    "The ultimate goal is to create **fully automated segmentation pipelines** that can robustly identify and delineate MS lesions from raw MRI data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919db79e-a3bd-4896-89a1-1d9e0017be10",
   "metadata": {},
   "source": [
    "## üß† MSLesSeg Dataset Overview\n",
    "\n",
    "As part of this competition, participants were provided with the **MSLesSeg Dataset** ‚Äî a **comprehensively annotated, multi-modal MRI dataset** designed for advancing **lesion segmentation** research in medical imaging.\n",
    "\n",
    "### üìä Dataset Composition\n",
    "- **Total Patients:** 75 (48 women, 27 men)  \n",
    "- **Age Range:** 18‚Äì59 years (Mean: 37 ¬± 10.3 years)  \n",
    "- **Longitudinal Timepoints:**  \n",
    "  - 50 patients with 1 timepoint  \n",
    "  - 15 patients with 2 timepoints  \n",
    "  - 5 patients with 3 timepoints  \n",
    "  - 5 patients with 4 timepoints  \n",
    "- **Time Interval Between Scans:** ~1.27 ¬± 0.62 years  \n",
    "- **Total MRI Series:** 115\n",
    "\n",
    "### üß¨ Imaging Modalities\n",
    "Each timepoint includes **three core MRI modalities**:\n",
    "- **T1-weighted (T1-w)**\n",
    "- **T2-weighted (T2-w)**\n",
    "- **FLAIR (Fluid-Attenuated Inversion Recovery)**\n",
    "\n",
    "### üßë‚Äç‚öïÔ∏è Expert Annotation\n",
    "- Lesions were **manually annotated** by clinical experts.\n",
    "- **FLAIR sequences** were the primary reference for lesion labeling.\n",
    "- **T1-w and T2-w** scans supported **multi-contrast lesion characterization**.\n",
    "\n",
    "### üß™ Dataset Splits\n",
    "- **Training Set:** 53 scans  \n",
    "- **Test Set:** 22 scans  \n",
    "\n",
    "### ‚úÖ Ethical Compliance\n",
    "- **Ethical approval** was obtained from the corresponding Hospital Ethics Committee.\n",
    "- **Informed consent** was acquired from all participating patients.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b3a0a-2f43-44f9-a898-28f1fa46b75a",
   "metadata": {},
   "source": [
    "# The Experiment\n",
    "\n",
    "Below is the code used for the:\n",
    "\n",
    "1) Preprocessing of the ***Brain MRI*** scans\n",
    "2) Definition of Dataset, Dataloader and LihgtningDataModule classes\n",
    "3) ***U-Net*** architecture\n",
    "4) ***PyTorch Lightning*** Trainer\n",
    "5) Training & Evaluation\n",
    "6) Model Exaplainability with the post-hoc method ***GradCam++***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13613113-a191-4399-9b42-9f6df49bc6e9",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Preprocessing & Annotation Workflow\n",
    "\n",
    "The MSLesSeg dataset underwent a **comprehensive preprocessing pipeline** and **expert-driven manual annotation** to ensure **standardization** and **label quality** for downstream MS lesion segmentation tasks.\n",
    "\n",
    "### üßº Preprocessing Pipeline\n",
    "1. **Anonymization** of all MRI scans to protect patient privacy.\n",
    "2. **DICOM to NIfTI conversion**, leveraging NIfTI's wide adoption in neuroimaging.\n",
    "3. **Co-registration to the MNI152 1mm¬≥ isotropic template** using **FLIRT** (FMRIB‚Äôs Linear Image Registration Tool), ensuring all scans are aligned to a **common anatomical space**.\n",
    "4. **Brain extraction** via **BET** (Brain Extraction Tool) to remove non-brain tissues and isolate relevant structures.\n",
    "\n",
    "This pipeline guarantees that all images are **standardized** and **aligned**, which is critical for **automated MS lesion segmentation algorithms**.\n",
    "\n",
    "---\n",
    "\n",
    "### üñãÔ∏è Ground-Truth Annotation Protocol\n",
    "- Lesions were **manually segmented** on the **FLAIR modality** for each patient and timepoint.\n",
    "- **T1-w and T2-w** modalities were used to **cross-validate ambiguous cases**.\n",
    "- Annotation was conducted by a **trained junior rater**, under supervision of:\n",
    "  - A **senior neuroradiologist**\n",
    "  - A **senior neurologist**\n",
    "- Annotation sessions included:\n",
    "  - Multiple **training meetings** to establish a **consistent segmentation strategy**\n",
    "  - Use of **JIM9** ‚Äî a high-end tool for **medical image segmentation and analysis**\n",
    "  - Regular **expert validation checkpoints** to ensure consistency and accuracy\n",
    "\n",
    "The final masks, reviewed and approved by senior experts, are considered the **gold-standard ground truth**.\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ Key Annotation Highlights\n",
    "- **Independent segmentation** for each patient/timepoint to avoid bias\n",
    "- Conducted on **FLAIR scans registered to MNI space**\n",
    "- **Validated ground-truth masks** ready for training and evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293bc032-fddb-44a7-8db3-cd8bdf0315e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from skimage.transform import resize\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2027badd-35e2-42ac-b423-e85e9e99d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load the nifti files\n",
    "def load_nifti(file_path):\n",
    "    return nib.load(file_path).get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3515529-ff9f-4f47-935b-e09a357f22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_case(input_dir, output_dir, case_id):\n",
    "    flair = load_nifti(input_dir / f\"{case_id}_flair.nii.gz\")\n",
    "    t1 = load_nifti(input_dir / f\"{case_id}_t1.nii.gz\")\n",
    "    t2 = load_nifti(input_dir / f\"{case_id}_t2.nii.gz\")\n",
    "    seg = load_nifti(input_dir / f\"{case_id}_seg.nii.gz\").astype(np.uint8)\n",
    "\n",
    "    # Stack input modalities into a tensor (C, D, H, W)\n",
    "    stacked = np.stack([flair, t1, t2], axis=0)\n",
    "    # Use CPU Tensors instead\n",
    "    input_tensor = torch.tensor(stacked, dtype=torch.float32)\n",
    "    \n",
    "    # Load it back into a PyTorch Tensor for storing\n",
    "    seg_tensor = torch.tensor(seg, dtype=torch.uint8)\n",
    "    # Add the batch dimension in order to make it compatible with the other Tensor sizes\n",
    "    seg_tensor = seg_tensor.unsqueeze(0)\n",
    "\n",
    "    \n",
    "    # Save all\n",
    "    output_case_dir = output_dir / case_id\n",
    "    output_case_dir.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(input_tensor, output_case_dir / \"input_tensor.pt\")\n",
    "    torch.save(seg_tensor, output_case_dir / \"seg_mask.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac6e5ec-ad39-42e9-8a62-c1140398e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_preprocessing(root_path, output_path):\n",
    "    input_path = Path(root_path)\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    # Get all case directories\n",
    "    all_case_dirs = [d for d in input_path.iterdir() if d.is_dir()]\n",
    "    print(f\"Found {len(all_case_dirs)} cases.\")\n",
    "\n",
    "    for case_dir in tqdm(all_case_dirs):\n",
    "        case_id = case_dir.name  # e.g., MSLS_000\n",
    "        output_case_path = output_path / case_id  # Define output path for each case\n",
    "\n",
    "        # Check if the case has already been processed (e.g., output directory or file exists)\n",
    "        if output_case_path.exists():\n",
    "            print(f\"‚úÖ Skipping {case_id}, already processed.\")\n",
    "            continue  # Skip processing if the case already exists\n",
    "\n",
    "        try:\n",
    "            preprocess_case(case_dir, output_path, case_id)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed on {case_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e6a0eb7-7da6-400d-9446-97773e286c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 93 cases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:00<00:00, 10126.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Skipping MSLS_019, already processed.\n",
      "‚úÖ Skipping MSLS_039, already processed.\n",
      "‚úÖ Skipping MSLS_059, already processed.\n",
      "‚úÖ Skipping MSLS_000, already processed.\n",
      "‚úÖ Skipping MSLS_001, already processed.\n",
      "‚úÖ Skipping MSLS_002, already processed.\n",
      "‚úÖ Skipping MSLS_003, already processed.\n",
      "‚úÖ Skipping MSLS_004, already processed.\n",
      "‚úÖ Skipping MSLS_005, already processed.\n",
      "‚úÖ Skipping MSLS_006, already processed.\n",
      "‚úÖ Skipping MSLS_007, already processed.\n",
      "‚úÖ Skipping MSLS_008, already processed.\n",
      "‚úÖ Skipping MSLS_009, already processed.\n",
      "‚úÖ Skipping MSLS_010, already processed.\n",
      "‚úÖ Skipping MSLS_011, already processed.\n",
      "‚úÖ Skipping MSLS_012, already processed.\n",
      "‚úÖ Skipping MSLS_013, already processed.\n",
      "‚úÖ Skipping MSLS_014, already processed.\n",
      "‚úÖ Skipping MSLS_015, already processed.\n",
      "‚úÖ Skipping MSLS_016, already processed.\n",
      "‚úÖ Skipping MSLS_017, already processed.\n",
      "‚úÖ Skipping MSLS_018, already processed.\n",
      "‚úÖ Skipping MSLS_020, already processed.\n",
      "‚úÖ Skipping MSLS_021, already processed.\n",
      "‚úÖ Skipping MSLS_022, already processed.\n",
      "‚úÖ Skipping MSLS_023, already processed.\n",
      "‚úÖ Skipping MSLS_024, already processed.\n",
      "‚úÖ Skipping MSLS_025, already processed.\n",
      "‚úÖ Skipping MSLS_026, already processed.\n",
      "‚úÖ Skipping MSLS_027, already processed.\n",
      "‚úÖ Skipping MSLS_028, already processed.\n",
      "‚úÖ Skipping MSLS_029, already processed.\n",
      "‚úÖ Skipping MSLS_030, already processed.\n",
      "‚úÖ Skipping MSLS_031, already processed.\n",
      "‚úÖ Skipping MSLS_032, already processed.\n",
      "‚úÖ Skipping MSLS_033, already processed.\n",
      "‚úÖ Skipping MSLS_034, already processed.\n",
      "‚úÖ Skipping MSLS_035, already processed.\n",
      "‚úÖ Skipping MSLS_036, already processed.\n",
      "‚úÖ Skipping MSLS_037, already processed.\n",
      "‚úÖ Skipping MSLS_038, already processed.\n",
      "‚úÖ Skipping MSLS_040, already processed.\n",
      "‚úÖ Skipping MSLS_041, already processed.\n",
      "‚úÖ Skipping MSLS_042, already processed.\n",
      "‚úÖ Skipping MSLS_043, already processed.\n",
      "‚úÖ Skipping MSLS_044, already processed.\n",
      "‚úÖ Skipping MSLS_045, already processed.\n",
      "‚úÖ Skipping MSLS_046, already processed.\n",
      "‚úÖ Skipping MSLS_047, already processed.\n",
      "‚úÖ Skipping MSLS_048, already processed.\n",
      "‚úÖ Skipping MSLS_049, already processed.\n",
      "‚úÖ Skipping MSLS_050, already processed.\n",
      "‚úÖ Skipping MSLS_051, already processed.\n",
      "‚úÖ Skipping MSLS_052, already processed.\n",
      "‚úÖ Skipping MSLS_053, already processed.\n",
      "‚úÖ Skipping MSLS_054, already processed.\n",
      "‚úÖ Skipping MSLS_055, already processed.\n",
      "‚úÖ Skipping MSLS_056, already processed.\n",
      "‚úÖ Skipping MSLS_057, already processed.\n",
      "‚úÖ Skipping MSLS_058, already processed.\n",
      "‚úÖ Skipping MSLS_060, already processed.\n",
      "‚úÖ Skipping MSLS_061, already processed.\n",
      "‚úÖ Skipping MSLS_062, already processed.\n",
      "‚úÖ Skipping MSLS_063, already processed.\n",
      "‚úÖ Skipping MSLS_064, already processed.\n",
      "‚úÖ Skipping MSLS_065, already processed.\n",
      "‚úÖ Skipping MSLS_066, already processed.\n",
      "‚úÖ Skipping MSLS_067, already processed.\n",
      "‚úÖ Skipping MSLS_068, already processed.\n",
      "‚úÖ Skipping MSLS_069, already processed.\n",
      "‚úÖ Skipping MSLS_070, already processed.\n",
      "‚úÖ Skipping MSLS_071, already processed.\n",
      "‚úÖ Skipping MSLS_072, already processed.\n",
      "‚úÖ Skipping MSLS_073, already processed.\n",
      "‚úÖ Skipping MSLS_074, already processed.\n",
      "‚úÖ Skipping MSLS_075, already processed.\n",
      "‚úÖ Skipping MSLS_076, already processed.\n",
      "‚úÖ Skipping MSLS_077, already processed.\n",
      "‚úÖ Skipping MSLS_078, already processed.\n",
      "‚úÖ Skipping MSLS_079, already processed.\n",
      "‚úÖ Skipping MSLS_080, already processed.\n",
      "‚úÖ Skipping MSLS_081, already processed.\n",
      "‚úÖ Skipping MSLS_082, already processed.\n",
      "‚úÖ Skipping MSLS_083, already processed.\n",
      "‚úÖ Skipping MSLS_084, already processed.\n",
      "‚úÖ Skipping MSLS_085, already processed.\n",
      "‚úÖ Skipping MSLS_086, already processed.\n",
      "‚úÖ Skipping MSLS_087, already processed.\n",
      "‚úÖ Skipping MSLS_088, already processed.\n",
      "‚úÖ Skipping MSLS_089, already processed.\n",
      "‚úÖ Skipping MSLS_090, already processed.\n",
      "‚úÖ Skipping MSLS_091, already processed.\n",
      "‚úÖ Skipping MSLS_092, already processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the preprocessing on the training set\n",
    "RAW_DATA_PATH = \"../data/01-Pre-Processed-Data/train\"\n",
    "OUTPUT_PATH = \"../data/02-Tensor-Data/train\"\n",
    "\n",
    "run_preprocessing(RAW_DATA_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc06f78a-1ed2-4e4b-90f7-391b10748569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 cases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:00<00:00, 8430.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Skipping MSLS_093, already processed.\n",
      "‚úÖ Skipping MSLS_094, already processed.\n",
      "‚úÖ Skipping MSLS_095, already processed.\n",
      "‚úÖ Skipping MSLS_096, already processed.\n",
      "‚úÖ Skipping MSLS_097, already processed.\n",
      "‚úÖ Skipping MSLS_098, already processed.\n",
      "‚úÖ Skipping MSLS_099, already processed.\n",
      "‚úÖ Skipping MSLS_100, already processed.\n",
      "‚úÖ Skipping MSLS_101, already processed.\n",
      "‚úÖ Skipping MSLS_102, already processed.\n",
      "‚úÖ Skipping MSLS_103, already processed.\n",
      "‚úÖ Skipping MSLS_104, already processed.\n",
      "‚úÖ Skipping MSLS_105, already processed.\n",
      "‚úÖ Skipping MSLS_106, already processed.\n",
      "‚úÖ Skipping MSLS_107, already processed.\n",
      "‚úÖ Skipping MSLS_108, already processed.\n",
      "‚úÖ Skipping MSLS_109, already processed.\n",
      "‚úÖ Skipping MSLS_110, already processed.\n",
      "‚úÖ Skipping MSLS_111, already processed.\n",
      "‚úÖ Skipping MSLS_112, already processed.\n",
      "‚úÖ Skipping MSLS_113, already processed.\n",
      "‚úÖ Skipping MSLS_114, already processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the preprocessing on the test set\n",
    "RAW_DATA_PATH = \"../data/01-Pre-Processed-Data/test/test_MASK\"\n",
    "OUTPUT_PATH = \"../data/02-Tensor-Data/test\"\n",
    "\n",
    "run_preprocessing(RAW_DATA_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2f3830-e80c-47b9-9f3f-3b1268619f14",
   "metadata": {},
   "source": [
    "## Build the Dataset and Dataloaders for the MSLesSeg preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "813cbfe2-346f-48f1-b350-c0ef69e9ae0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Check the initial memory consuption (GPU) of the project\n",
    "import torch\n",
    "\n",
    "# Check initial GPU memory usage\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59da4863-42f3-4d2c-a675-c8cdc1eeeace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSLesSeg Tensor Dataset class\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MSLesSegDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.patient_dirs = self._get_patient_dirs()\n",
    "\n",
    "    def _get_patient_dirs(self):\n",
    "        \"\"\"\n",
    "        Helper function to search for all patient directories within train/test directories.\n",
    "        \"\"\"\n",
    "        patient_dirs = []\n",
    "        \n",
    "        # Get all patient directories within the root_dir (either train or test)\n",
    "        for patient_dir in os.listdir(self.root_dir):\n",
    "            patient_path = os.path.join(self.root_dir, patient_dir)\n",
    "            \n",
    "            # Make sure it's a directory\n",
    "            if os.path.isdir(patient_path):\n",
    "                # Check if both 'input_tensor.pt' and 'seg_mask.pt' exist\n",
    "                input_tensor_path = os.path.join(patient_path, 'input_tensor.pt')\n",
    "                seg_mask_path = os.path.join(patient_path, 'seg_mask.pt')\n",
    "                \n",
    "                if os.path.exists(input_tensor_path) and os.path.exists(seg_mask_path):\n",
    "                    patient_dirs.append(patient_path)\n",
    "        \n",
    "        return patient_dirs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "        \n",
    "        Returns:\n",
    "            dict: {'input': tensor, 'mask': tensor} for the requested sample.\n",
    "        \"\"\"\n",
    "        patient_dir = self.patient_dirs[idx]\n",
    "        \n",
    "        # Load the input tensor and segmentation mask\n",
    "        input_tensor = torch.load(os.path.join(patient_dir, 'input_tensor.pt'))  # Shape: [3, 182, 218, 182]\n",
    "        seg_mask = torch.load(os.path.join(patient_dir, 'seg_mask.pt'))  # Shape: [1, 182, 218, 182]\n",
    "        \n",
    "        return input_tensor, seg_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f0311-9b80-4bf0-b80d-996692a83815",
   "metadata": {},
   "source": [
    "### PyTorch Lightning DataModule\n",
    "\n",
    "This particular version of PyTorch Lightning, and in general from version 2.x onward require a ***LightningDataModule*** instead of passing the dataloaders directly to the ***.fit()*** method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03106a16-273a-4298-b82f-e15990258754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSLesSeg (PyTorch) LightningDataModule definition\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class MSLesSegDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, root_data_dir, batch_size, val_split, num_workers):\n",
    "        \"\"\"\n",
    "        root_data_dir: Is the path to the train and test data\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_dir = root_data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.val_split = val_split\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Load full training dataset\n",
    "        full_dataset = MSLesSegDataset(root_dir=os.path.join(self.data_dir, 'train'))\n",
    "\n",
    "        # Split into train and val\n",
    "        val_size = int(len(full_dataset) * self.val_split)\n",
    "        train_size = len(full_dataset) - val_size\n",
    "        self.train_dataset, self.val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "        # Load test dataset (if it exists)\n",
    "        test_dir = os.path.join(self.data_dir, 'test')\n",
    "        if os.path.exists(test_dir):\n",
    "            self.test_dataset = MSLesSegDataset(root_dir=test_dir)\n",
    "        else:\n",
    "            self.test_dataset = None\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if self.test_dataset:\n",
    "            return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b08f0-2188-400b-b30e-88997983f659",
   "metadata": {},
   "source": [
    "## The Model's Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "303c976d-7038-4798-91fb-99a94c810caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The U-Net architecture is based on the tutorial offered by the author of the architecture\n",
    "# Link --> https://github.com/bnsreenu/python_for_image_processing_APEER/blob/master/tutorial122_3D_Unet.ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu1 = nn.GELU()\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu2 = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x\n",
    "\n",
    "# Downsampling path Conv block followed by maxpooling.\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down, self).__init__()\n",
    "        self.conv_block = ConvBlock(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_output = self.conv_block(x)\n",
    "        pool_output = self.pool(conv_output)\n",
    "        return conv_output, pool_output\n",
    "\n",
    "# Upsampling path: Skip features gets input from encoder for concatenation\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Up, self).__init__()\n",
    "        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv_block = ConvBlock(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, skip_features):\n",
    "        x = self.conv_transpose(x)\n",
    "        x = torch.cat((x, skip_features), dim=1)\n",
    "        x = self.conv_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48ea20e2-8ad7-4693-ab48-3d442038dedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to pad to a multiple of 16 (because of the 4 down and up convolutions)\n",
    "def pad_to_multiple(x, multiple=16):\n",
    "    _, _, h, w, d = x.shape\n",
    "    pad_h = (multiple - h % multiple) % multiple\n",
    "    pad_w = (multiple - w % multiple) % multiple\n",
    "    pad_d = (multiple - d % multiple) % multiple\n",
    "\n",
    "    pad = [0, pad_d, 0, pad_w, 0, pad_h]  # D, W, H\n",
    "    return F.pad(x, pad, mode='constant', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbdc3277-c204-4867-baea-7abf416655df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center cropping helper function to obtain the desired shape for the prediction with the segmentation mask\n",
    "def center_crop(x, target_shape):\n",
    "    _, h, w, d = x.shape\n",
    "    th, tw, td = target_shape\n",
    "    h1 = (h - th) // 2\n",
    "    w1 = (w - tw) // 2\n",
    "    d1 = (d - td) // 2\n",
    "    return x[:, h1:h1+th, w1:w1+tw, d1:d1+td]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c67520ee-53db-45f6-a184-3ff3761bb80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D U-Net Architecture\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    The U-Net model will accept input Tensors of shape: [B, C, H, W, D]\n",
    "    Which based on the used MSLesSeg dataset will be [B, 3, 182, 218, 182]\n",
    "    The input Tensor is nothing more than the stacked [FLAIR, T1w, T2w] modalities\n",
    "\n",
    "    The output will be the segmentation mask of shape [B, 1, 182, 218, 182]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        self.down_1 = Down(in_channels, 64)\n",
    "        self.down_2 = Down(64, 128)\n",
    "        self.down_3 = Down(128, 256)\n",
    "        self.down_4 = Down(256, 512)\n",
    "\n",
    "        self.bottleneck = ConvBlock(512, 1024)\n",
    "\n",
    "        self.up_1 = Up(1024, 512)\n",
    "        self.up_2 = Up(512, 256)\n",
    "        self.up_3 = Up(256, 128)\n",
    "        self.up_4 = Up(128, 64)\n",
    "\n",
    "        self.classifier = nn.Conv3d(64, 1, kernel_size=1, padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Pad to multiple\n",
    "        x = pad_to_multiple(x)\n",
    "        \n",
    "        # Downsampling path\n",
    "        conv1, pool1 = self.down_1(x)\n",
    "        conv2, pool2 = self.down_2(pool1)\n",
    "        conv3, pool3 = self.down_3(pool2)\n",
    "        conv4, pool4 = self.down_4(pool3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(pool4)\n",
    "\n",
    "        # Upsampling path\n",
    "        upconv1 = self.up_1(bottleneck, conv4)\n",
    "        upconv2 = self.up_2(upconv1, conv3)\n",
    "        upconv3 = self.up_3(upconv2, conv2)\n",
    "        upconv4 = self.up_4(upconv3, conv1)\n",
    "\n",
    "        output = self.sigmoid(self.classifier(upconv4))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fd1805d-2f4e-4057-9d61-2ac0755cbe69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "UNet                                     [1, 1, 192, 224, 192]     --\n",
       "‚îú‚îÄDown: 1-1                              [1, 64, 192, 224, 192]    --\n",
       "‚îÇ    ‚îî‚îÄConvBlock: 2-1                    [1, 64, 192, 224, 192]    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-1                  [1, 64, 192, 224, 192]    5,248\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-2             [1, 64, 192, 224, 192]    128\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-3                    [1, 64, 192, 224, 192]    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-4                  [1, 64, 192, 224, 192]    110,656\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-5             [1, 64, 192, 224, 192]    128\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-6                    [1, 64, 192, 224, 192]    --\n",
       "‚îÇ    ‚îî‚îÄMaxPool3d: 2-2                    [1, 64, 96, 112, 96]      --\n",
       "‚îú‚îÄDown: 1-2                              [1, 128, 96, 112, 96]     --\n",
       "‚îÇ    ‚îî‚îÄConvBlock: 2-3                    [1, 128, 96, 112, 96]     --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-7                  [1, 128, 96, 112, 96]     221,312\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-8             [1, 128, 96, 112, 96]     256\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-9                    [1, 128, 96, 112, 96]     --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-10                 [1, 128, 96, 112, 96]     442,496\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-11            [1, 128, 96, 112, 96]     256\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-12                   [1, 128, 96, 112, 96]     --\n",
       "‚îÇ    ‚îî‚îÄMaxPool3d: 2-4                    [1, 128, 48, 56, 48]      --\n",
       "‚îú‚îÄDown: 1-3                              [1, 256, 48, 56, 48]      --\n",
       "‚îÇ    ‚îî‚îÄConvBlock: 2-5                    [1, 256, 48, 56, 48]      --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-13                 [1, 256, 48, 56, 48]      884,992\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-14            [1, 256, 48, 56, 48]      512\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-15                   [1, 256, 48, 56, 48]      --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-16                 [1, 256, 48, 56, 48]      1,769,728\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-17            [1, 256, 48, 56, 48]      512\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-18                   [1, 256, 48, 56, 48]      --\n",
       "‚îÇ    ‚îî‚îÄMaxPool3d: 2-6                    [1, 256, 24, 28, 24]      --\n",
       "‚îú‚îÄDown: 1-4                              [1, 512, 24, 28, 24]      --\n",
       "‚îÇ    ‚îî‚îÄConvBlock: 2-7                    [1, 512, 24, 28, 24]      --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-19                 [1, 512, 24, 28, 24]      3,539,456\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-20            [1, 512, 24, 28, 24]      1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-21                   [1, 512, 24, 28, 24]      --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-22                 [1, 512, 24, 28, 24]      7,078,400\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-23            [1, 512, 24, 28, 24]      1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-24                   [1, 512, 24, 28, 24]      --\n",
       "‚îÇ    ‚îî‚îÄMaxPool3d: 2-8                    [1, 512, 12, 14, 12]      --\n",
       "‚îú‚îÄConvBlock: 1-5                         [1, 1024, 12, 14, 12]     --\n",
       "‚îÇ    ‚îî‚îÄConv3d: 2-9                       [1, 1024, 12, 14, 12]     14,156,800\n",
       "‚îÇ    ‚îî‚îÄBatchNorm3d: 2-10                 [1, 1024, 12, 14, 12]     2,048\n",
       "‚îÇ    ‚îî‚îÄGELU: 2-11                        [1, 1024, 12, 14, 12]     --\n",
       "‚îÇ    ‚îî‚îÄConv3d: 2-12                      [1, 1024, 12, 14, 12]     28,312,576\n",
       "‚îÇ    ‚îî‚îÄBatchNorm3d: 2-13                 [1, 1024, 12, 14, 12]     2,048\n",
       "‚îÇ    ‚îî‚îÄGELU: 2-14                        [1, 1024, 12, 14, 12]     --\n",
       "‚îú‚îÄUp: 1-6                                [1, 512, 24, 28, 24]      --\n",
       "‚îÇ    ‚îî‚îÄConvTranspose3d: 2-15             [1, 512, 24, 28, 24]      4,194,816\n",
       "‚îÇ    ‚îî‚îÄConvBlock: 2-16                   [1, 512, 24, 28, 24]      --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-25                 [1, 512, 24, 28, 24]      14,156,288\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-26            [1, 512, 24, 28, 24]      1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-27                   [1, 512, 24, 28, 24]      --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-28                 [1, 512, 24, 28, 24]      7,078,400\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-29            [1, 512, 24, 28, 24]      1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-30                   [1, 512, 24, 28, 24]      --\n",
       "‚îú‚îÄUp: 1-7                                [1, 256, 48, 56, 48]      --\n",
       "‚îÇ    ‚îî‚îÄConvTranspose3d: 2-17             [1, 256, 48, 56, 48]      1,048,832\n",
       "‚îÇ    ‚îî‚îÄConvBlock: 2-18                   [1, 256, 48, 56, 48]      --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-31                 [1, 256, 48, 56, 48]      3,539,200\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-32            [1, 256, 48, 56, 48]      512\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-33                   [1, 256, 48, 56, 48]      --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-34                 [1, 256, 48, 56, 48]      1,769,728\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-35            [1, 256, 48, 56, 48]      512\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-36                   [1, 256, 48, 56, 48]      --\n",
       "‚îú‚îÄUp: 1-8                                [1, 128, 96, 112, 96]     --\n",
       "‚îÇ    ‚îî‚îÄConvTranspose3d: 2-19             [1, 128, 96, 112, 96]     262,272\n",
       "‚îÇ    ‚îî‚îÄConvBlock: 2-20                   [1, 128, 96, 112, 96]     --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-37                 [1, 128, 96, 112, 96]     884,864\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-38            [1, 128, 96, 112, 96]     256\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-39                   [1, 128, 96, 112, 96]     --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-40                 [1, 128, 96, 112, 96]     442,496\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-41            [1, 128, 96, 112, 96]     256\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-42                   [1, 128, 96, 112, 96]     --\n",
       "‚îú‚îÄUp: 1-9                                [1, 64, 192, 224, 192]    --\n",
       "‚îÇ    ‚îî‚îÄConvTranspose3d: 2-21             [1, 64, 192, 224, 192]    65,600\n",
       "‚îÇ    ‚îî‚îÄConvBlock: 2-22                   [1, 64, 192, 224, 192]    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-43                 [1, 64, 192, 224, 192]    221,248\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-44            [1, 64, 192, 224, 192]    128\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-45                   [1, 64, 192, 224, 192]    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-46                 [1, 64, 192, 224, 192]    110,656\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-47            [1, 64, 192, 224, 192]    128\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-48                   [1, 64, 192, 224, 192]    --\n",
       "‚îú‚îÄConv3d: 1-10                           [1, 1, 192, 224, 192]     65\n",
       "‚îú‚îÄSigmoid: 1-11                          [1, 1, 192, 224, 192]     --\n",
       "==========================================================================================\n",
       "Total params: 90,307,905\n",
       "Trainable params: 90,307,905\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.TERABYTES): 8.40\n",
       "==========================================================================================\n",
       "Input size (MB): 86.65\n",
       "Forward/backward pass size (MB): 50668.24\n",
       "Params size (MB): 361.23\n",
       "Estimated Total Size (MB): 51116.12\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the model summary\n",
    "from torchinfo import summary\n",
    "\n",
    "# Assuming your model is already defined (UNet or similar)\n",
    "model = UNet(in_channels=3)\n",
    "\n",
    "# Example input size: (Batch Size, Channels, Height, Width, Depth)\n",
    "input_tensor = torch.randn(1, 3, 182, 218, 182)  # Modify based on your use case\n",
    "\n",
    "# Use summary from torchinfo to display the model summary\n",
    "summary(model, input_data=input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60727e14-353b-4fd3-9e3e-938472763e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be907f2a-2600-407f-8649-42965a1fb3d1",
   "metadata": {},
   "source": [
    "## The Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec1451e1-9756-49f3-a595-7fda87796588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we implement the DiceLoss\n",
    "def dice_loss_3d(pred, target, smooth=1):\n",
    "    \"\"\"\n",
    "    Computes Dice Loss for 3D segmentation tasks.\n",
    "    Args:\n",
    "    pred: Tensor of predictions (batch_size, C, D, H, W).\n",
    "    target: Tensor of ground truth (batch_size, C, D, H, W).\n",
    "    smooth: Smoothing factor.\n",
    "    Returns:\n",
    "    Scalar Dice Loss.\n",
    "    \"\"\"\n",
    "    pred = F.softmax(pred, dim=1)\n",
    "    num_classes = pred.shape[1]\n",
    "    dice = 0\n",
    "    for c in range(num_classes):\n",
    "        pred_c = pred[:, c]\n",
    "        target_c = target[:, c]\n",
    "        intersection = (pred_c * target_c).sum(dim=(2, 3, 4))\n",
    "        union = pred_c.sum(dim=(2, 3, 4)) + target_c.sum(dim=(2, 3, 4))\n",
    "        dice += (2. * intersection + smooth) / (union + smooth)\n",
    "        \n",
    "    return 1 - dice.mean() / num_classes\n",
    "\n",
    "# Then we implement the Mean Dice\n",
    "def mean_dice(pred, target, smooth=1):\n",
    "    \"\"\"\n",
    "    Computes Dice Loss for 3D segmentation tasks.\n",
    "    Args:\n",
    "    pred: Tensor of predictions (batch_size, C, D, H, W).\n",
    "    target: Tensor of ground truth (batch_size, C, D, H, W).\n",
    "    smooth: Smoothing factor.\n",
    "    Returns:\n",
    "    Scalar Dice Loss.\n",
    "    \"\"\"\n",
    "    pred = F.softmax(pred, dim=1)\n",
    "    num_classes = pred.shape[1]\n",
    "    dice = 0\n",
    "    for c in range(num_classes):\n",
    "        pred_c = pred[:, c]\n",
    "        target_c = target[:, c]\n",
    "        intersection = (pred_c * target_c).sum(dim=(2, 3, 4))\n",
    "        union = pred_c.sum(dim=(2, 3, 4)) + target_c.sum(dim=(2, 3, 4))\n",
    "        dice += (2. * intersection + smooth) / (union + smooth)\n",
    "        \n",
    "    return dice.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6b5560d-a488-4a2f-89bf-b8e4f5a2eb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Dice Score and Dice Loss\n",
    "def dice_coefficient(y_pred, y_true, smooth=1e-5):\n",
    "    y_true_flat = y_true.contiguous().view(y_true.shape[0], -1)\n",
    "    y_pred_flat = y_pred.contiguous().view(y_pred.shape[0], -1)\n",
    "    \n",
    "    intersection = (y_true_flat * y_pred_flat).sum(dim=1)\n",
    "    union = y_true_flat.sum(dim=1) + y_pred_flat.sum(dim=1)\n",
    "    \n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return dice.mean()\n",
    "\n",
    "def dice_coefficient_loss(y_pred, y_true):\n",
    "    return 1 - dice_coefficient(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b2d3471-ee79-41ff-b176-929864412593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning Trainer for the U-Net model\n",
    "import torch.optim as optim\n",
    "\n",
    "class MSLesionSegmentationModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, model, checkpoint_dir, lr=1e-4):\n",
    "        super(MSLesionSegmentationModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Separate the input Tensor from the Segmentation Mask\n",
    "        input_tensor, gt = batch\n",
    "        # Forward Pass on the model\n",
    "        y_pred = self(input_tensor)\n",
    "        # Crop y_pred shape to match the seg mask gt\n",
    "        y_pred = center_crop(y_pred, (182, 218, 182))\n",
    "        # Compute DiceLoss\n",
    "        loss = dice_coefficient_loss(y_pred, gt)\n",
    "\n",
    "        # log the train loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Separate the input Tensor from the Segmentation Mask\n",
    "        input_tensor, gt = batch\n",
    "        # Forward pass on the model\n",
    "        y_pred = self(input_tensor)[0]\n",
    "        # Crop y_pred shape to match the seg mask gt\n",
    "        y_pred = center_crop(y_pred, (182, 218, 182))\n",
    "        # Compute DiceLoss\n",
    "        loss = dice_coefficient_loss(y_pred, gt)\n",
    "        # Compute the Mean Dice Score\n",
    "        dice_score = dice_coefficient(y_pred, gt)\n",
    "        # log the val loss\n",
    "        self.log(\"val_loss\", loss)\n",
    "        # log the Mean Dice Score\n",
    "        self.log(\"dice_score\", dice_score)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Separate the input Tensor from the Segmentation Mask\n",
    "        input_tensor, gt = batch\n",
    "        # Forward pass on the model\n",
    "        y_pred = self(input_tensor)\n",
    "        # Crop y_pred shape to match the seg mask gt\n",
    "        y_pred = center_crop(y_pred, (182, 218, 182))\n",
    "        # Compute DiceLoss\n",
    "        loss = dice_coefficient_loss(y_pred, gt)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "        \n",
    "    def on_train_epoch_end(self):\n",
    "        # Save the model's checkpoint at the end of the epoch\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "        save_path = os.path.join(self.save_dir, \"unet_best_model.pth\")\n",
    "        \n",
    "        # Prepare the checkpoint dict\n",
    "        checkpoint = {\n",
    "            'epoch': self.current_epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,\n",
    "            'learning_rate': self.lr,\n",
    "        }\n",
    "\n",
    "        # Save and flush to disk\n",
    "        with open(save_path, 'wb') as f:\n",
    "            torch.save(checkpoint, f)\n",
    "            f.flush()\n",
    "            os.fsync(f.fileno())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b884f-771e-4852-be17-23973479e5a5",
   "metadata": {},
   "source": [
    "## The Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "330d4d97-5326-4dcf-b27e-2f3a57de10f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdrnnrw00m10c351s\u001b[0m (\u001b[33mfpv-perceivelab-unict\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "# Register a free account with Weights and Biases, and create a new project in order to obtain an API Key for the training\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the env variable\n",
    "api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "# Login to wandb\n",
    "if api_key:\n",
    "    os.environ[\"WANDB_API_KEY\"] = api_key\n",
    "    wandb.login()\n",
    "else:\n",
    "    print(\"‚ùå WANDB_API_KEY not found in .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd78050f-09dd-4251-a8b1-468756f2f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Weights and Biases logger\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project='MSLesSeg-4-ICPR',     # Change to your actual project name\n",
    "    name='UNet_run_1', # A specific run name\n",
    "    log_model=True          # Optional: log model checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1804c28e-27c2-490d-bf6f-83be924e2674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the constant for the maximum number of epochs for the training\n",
    "MAX_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe02bc36-efab-409d-9ff4-780ebf605e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of Dataloader hyperparameters (Batch size, seed and num workers)\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "TRAIN_NUM_WORKERS = 0\n",
    "TEST_NUM_WORKERS = 0\n",
    "\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "VAL_BATCH_SIZE = 1\n",
    "TEST_BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "094c3a1c-11bf-456a-a77b-8a0eb8698b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drew/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Create the Trainer object\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    accelerator=\"gpu\",\n",
    "    precision=16,\n",
    "    logger=wandb_logger,\n",
    "    benchmark=True,  # optimize CUDA kernels for performance\n",
    "    detect_anomaly=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05a9d033-88de-417f-ad1a-1be7b3b3c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data module\n",
    "data_module = MSLesSegDataModule(\n",
    "    root_data_dir=\"../data/02-Tensor-Data/\",\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    val_split=VAL_SPLIT,\n",
    "    num_workers=TRAIN_NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80715b4e-731b-496e-85f4-1f3f31f7c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the PyTorch Lightning Module (wrapper for the PyTorch nn.Module ~ Architecture)\n",
    "lightning_model = MSLesionSegmentationModel(\n",
    "    model=UNet(in_channels=3),\n",
    "    checkpoint_dir=\"./model_checkpoints\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "038c95be-0282-431c-b17f-75932d56e760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Check the initial memory consuption (GPU) of the project\n",
    "import torch\n",
    "\n",
    "# Check initial GPU memory usage\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4cf24dbb-10f4-4a5a-955d-84f46cf566f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250508_180028-hjx9tyzx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR/runs/hjx9tyzx' target=\"_blank\">UNet_run_1</a></strong> to <a href='https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR' target=\"_blank\">https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR/runs/hjx9tyzx' target=\"_blank\">https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR/runs/hjx9tyzx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params | Mode \n",
      "---------------------------------------\n",
      "0 | model | UNet | 90.3 M | train\n",
      "---------------------------------------\n",
      "90.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "90.3 M    Total params\n",
      "361.232   Total estimated model params size (MB)\n",
      "82        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e2ac6aaeaa4bb08503ea0452a58129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drew/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/drew/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c320b622d5a24c04bf7e6ec5a4862890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 504.00 MiB. GPU 0 has a total capacity of 11.72 GiB of which 118.62 MiB is free. Including non-PyTorch memory, this process has 10.61 GiB memory in use. Of the allocated memory 10.28 GiB is allocated by PyTorch, and 148.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train (fit) the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlightning_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     51\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    592\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1017\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1054\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_sanity_check()\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    215\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end()\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_epoch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:150\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done:\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m         \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:320\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_batch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m         batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautomatic_optimization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    322\u001b[39m         batch_output = \u001b[38;5;28mself\u001b[39m.manual_optimization.run(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:192\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         closure()\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m result = closure.consume_result()\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:270\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_step\u001b[39m\u001b[34m(self, batch_idx, train_step_and_backward_closure)\u001b[39m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_ready()\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[32m    280\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_completed()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:176\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m pl_module._current_fx_name = hook_name\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    179\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/core/module.py:1302\u001b[39m, in \u001b[36mLightningModule.optimizer_step\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimizer_step\u001b[39m(\n\u001b[32m   1272\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1273\u001b[39m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1276\u001b[39m     optimizer_closure: Optional[Callable[[], Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1277\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1278\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[32m   1279\u001b[39m \u001b[33;03m    the optimizer.\u001b[39;00m\n\u001b[32m   1280\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1300\u001b[39m \n\u001b[32m   1301\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1302\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:154\u001b[39m, in \u001b[36mLightningOptimizer.step\u001b[39m\u001b[34m(self, closure, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[33m\"\u001b[39m\u001b[33mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._on_after_step()\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:239\u001b[39m, in \u001b[36mStrategy.optimizer_step\u001b[39m\u001b[34m(self, optimizer, closure, model, **kwargs)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl.LightningModule)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/amp.py:79\u001b[39m, in \u001b[36mMixedPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(optimizer, LBFGS):\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[33m\"\u001b[39m\u001b[33mAMP and the LBFGS optimizer are not compatible.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m closure_result = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# If backward was skipped in automatic optimization (return None), unscaling is not needed\u001b[39;00m\n\u001b[32m     82\u001b[39m skip_unscaling = closure_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m model.automatic_optimization\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:146\u001b[39m, in \u001b[36mClosure.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Optional[Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result.loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:131\u001b[39m, in \u001b[36mClosure.closure\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;129m@torch\u001b[39m.enable_grad()\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> ClosureResult:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m step_output.closure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    134\u001b[39m         \u001b[38;5;28mself\u001b[39m.warning_cache.warn(\u001b[33m\"\u001b[39m\u001b[33m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:319\u001b[39m, in \u001b[36m_AutomaticOptimization._training_step\u001b[39m\u001b[34m(self, kwargs)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[32m    309\u001b[39m \n\u001b[32m    310\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    315\u001b[39m \n\u001b[32m    316\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    317\u001b[39m trainer = \u001b[38;5;28mself\u001b[39m.trainer\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m training_step_output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.strategy.post_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer.world_size > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:328\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    331\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:391\u001b[39m, in \u001b[36mStrategy.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mtraining_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mMSLesionSegmentationModel.training_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m     18\u001b[39m input_tensor, gt = batch\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Forward Pass on the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m y_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Crop y_pred shape to match the seg mask gt\u001b[39;00m\n\u001b[32m     22\u001b[39m y_pred = center_crop(y_pred, (\u001b[32m182\u001b[39m, \u001b[32m218\u001b[39m, \u001b[32m182\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mMSLesionSegmentationModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mUNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     42\u001b[39m upconv1 = \u001b[38;5;28mself\u001b[39m.up_1(bottleneck, conv4)\n\u001b[32m     43\u001b[39m upconv2 = \u001b[38;5;28mself\u001b[39m.up_2(upconv1, conv3)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m upconv3 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup_3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupconv2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m upconv4 = \u001b[38;5;28mself\u001b[39m.up_4(upconv3, conv1)\n\u001b[32m     47\u001b[39m output = \u001b[38;5;28mself\u001b[39m.sigmoid(\u001b[38;5;28mself\u001b[39m.classifier(upconv4))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mUp.forward\u001b[39m\u001b[34m(self, x, skip_features)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, skip_features):\n\u001b[32m     46\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.conv_transpose(x)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     x = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.conv_block(x)\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 504.00 MiB. GPU 0 has a total capacity of 11.72 GiB of which 118.62 MiB is free. Including non-PyTorch memory, this process has 10.61 GiB memory in use. Of the allocated memory 10.28 GiB is allocated by PyTorch, and 148.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Train (fit) the model\n",
    "trainer.fit(lightning_model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3886c494-2f08-4fb4-868e-ce9e4866b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model's checkpoint and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee0a5d-61d4-4561-a731-b24483798f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "trainer.test(model, test_dataloader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6833331b-df4e-485b-8b26-e617ad583746",
   "metadata": {},
   "source": [
    "## Visualizing the Learned Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4089cdf9-90ee-4700-8ec5-2c246a69d848",
   "metadata": {},
   "source": [
    "## Post Hoc Model Explainability - GradCAM++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb9090-3260-49ce-be05-a03c2d122e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size of the stored (pre-processed) Tensors\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def inspect_pt_file(file_path):\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(f\"File does not exist: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    data = torch.load(file_path, map_location='cpu')\n",
    "\n",
    "    print(f\"\\nLoaded file: {file_path}\")\n",
    "    \n",
    "    if isinstance(data, torch.Tensor):\n",
    "        print(f\"Single Tensor - Shape: {data.shape}\")\n",
    "    \n",
    "    elif isinstance(data, dict):\n",
    "        print(\"Dictionary of tensors:\")\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                print(f\"  {key}: shape = {value.shape}\")\n",
    "            else:\n",
    "                print(f\"  {key}: type = {type(value)}\")\n",
    "    \n",
    "    elif isinstance(data, (list, tuple)):\n",
    "        print(f\"{type(data).__name__} of tensors:\")\n",
    "        for idx, item in enumerate(data):\n",
    "            if isinstance(item, torch.Tensor):\n",
    "                print(f\"  [{idx}]: shape = {item.shape}\")\n",
    "            else:\n",
    "                print(f\"  [{idx}]: type = {type(item)}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Unknown type loaded: {type(data)}\")\n",
    "        print(f\"Unkown type shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45d428c-86fa-4c27-b37b-da49338e2bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH_PREFIX = \"../data/02-Tensor-Data/train/MSLS_000/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af84775-fb63-4bfe-9fda-a1b238a7a20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Tensor Shape (Stacked Modalities)\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"input_tensor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feeedd4-a2d9-4e3d-900d-d3ca5a68d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation Mask\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"seg_mask.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858f58bf-be14-45b3-9f56-44f6d8719a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH_PREFIX = \"../data/02-Tensor-Data/train/MSLS_010/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d59b454-f46d-4220-9263-9da064ed08b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Tensor Shape (Stacked Modalities)\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"input_tensor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc47eb8c-e1ab-45b5-b8fc-e8b29ce136f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation Mask\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"seg_mask.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7696a6-4413-405e-b1aa-734afea539de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

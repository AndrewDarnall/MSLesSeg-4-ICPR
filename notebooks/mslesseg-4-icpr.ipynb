{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "919db79e-a3bd-4896-89a1-1d9e0017be10",
   "metadata": {},
   "source": [
    "# Multiple Sclerosis Lesion Segmentation Dataset Overview\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f36701b-9820-49dd-8553-ca671481b49c",
   "metadata": {},
   "source": [
    "\n",
    "- Introduction to the challenge {evaluation metrics, comparrison to other datasets} (with link to website [https://iplab.dmi.unict.it/mfs/ms-les-seg/] and paper)\n",
    "- Introduction to the Dataset (creation, pre-processing, contents and distribution data with visualizations)\n",
    "\n",
    "Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4026f2-e84e-4427-8ab1-eff9d1b54d98",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What have winning strategies been so far?\n",
    "\n",
    "1) Ensamble methods\n",
    "2) Different Registration Spaces\n",
    "3) Selecting only the specific data from the image that can better find the segmentation mask\n",
    "4) Using only ONE modality (FLAIR) and performing Data Augmentations on it + Deep CNN Architecture\n",
    "5) nnU-Net for automatic configuration to the dataset (Interesting)\n",
    "6) Usage of Mamba-based LightM-UNet\n",
    "7) Focal Loss + DICE Loss\n",
    "\n",
    "---\n",
    "\n",
    "## Personal Goals\n",
    "\n",
    "1) Parameter Efficiency\n",
    "2) (XAI) Explainable Architecture\n",
    "3) Segmentation Accuracy (Mean DICE >= 60)\n",
    "4) [Optional] --> Single Model (No Ensambles)\n",
    "\n",
    "---\n",
    "\n",
    "## Personal Model Proposition\n",
    "\n",
    "1) Use ALL modalities\n",
    "2) Preprocess with (Distance Based Labelling, Multi-Sized Labelling ~ Could aid the MoE variant)\n",
    "3) nnU-Net + MoE (+ Deep Supervision ?)\n",
    "\n",
    "### Optional\n",
    "\n",
    "- Bring the model visualization to Extended Reality Domain ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b3a0a-2f43-44f9-a898-28f1fa46b75a",
   "metadata": {},
   "source": [
    "# The Experiments\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293bc032-fddb-44a7-8db3-cd8bdf0315e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from skimage.transform import resize\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13613113-a191-4399-9b42-9f6df49bc6e9",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing\n",
    "\n",
    "- Expand more on the performed preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2027badd-35e2-42ac-b423-e85e9e99d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nifti(file_path):\n",
    "    return nib.load(file_path).get_fdata()\n",
    "\n",
    "def normalize_intensity(img):\n",
    "    # Normalize each modality independently (z-score)\n",
    "    img = img.astype(np.float32)\n",
    "    img = (img - np.mean(img)) / (np.std(img) + 1e-5)\n",
    "    return img\n",
    "\n",
    "def compute_distance_label(mask):\n",
    "    mask = mask.astype(bool)\n",
    "    pos_dist = distance_transform_edt(mask)\n",
    "    neg_dist = distance_transform_edt(~mask)\n",
    "    dist_map = pos_dist - neg_dist  # Signed distance transform\n",
    "    return dist_map\n",
    "\n",
    "def generate_multi_size_masks(mask, scales=[1.0, 0.5, 0.25]):\n",
    "    multi_res = []\n",
    "    for scale in scales:\n",
    "        if scale == 1.0:\n",
    "            multi_res.append(mask)\n",
    "        else:\n",
    "            resized = resize(mask, output_shape=tuple(int(s * scale) for s in mask.shape),\n",
    "                             order=1, preserve_range=True, anti_aliasing=True)\n",
    "            multi_res.append(resized.astype(mask.dtype))\n",
    "    return multi_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3515529-ff9f-4f47-935b-e09a357f22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_case(input_dir, output_dir, case_id):\n",
    "    flair = normalize_intensity(load_nifti(input_dir / f\"{case_id}_flair.nii.gz\"))\n",
    "    t1 = normalize_intensity(load_nifti(input_dir / f\"{case_id}_t1.nii.gz\"))\n",
    "    t2 = normalize_intensity(load_nifti(input_dir / f\"{case_id}_t2.nii.gz\"))\n",
    "    seg = load_nifti(input_dir / f\"{case_id}_seg.nii.gz\").astype(np.uint8)\n",
    "\n",
    "    # Stack input modalities into a tensor (C, D, H, W)\n",
    "    stacked = np.stack([flair, t1, t2], axis=0)\n",
    "    input_tensor = torch.tensor(stacked, dtype=torch.float32).cuda()\n",
    "\n",
    "    # Compute distance map from segmentation mask\n",
    "    distance_map = compute_distance_label(seg)\n",
    "    distance_tensor = torch.tensor(distance_map, dtype=torch.float32).unsqueeze(0).cuda()\n",
    "\n",
    "    # Generate multi-size label masks\n",
    "    multi_size_masks = generate_multi_size_masks(seg)\n",
    "    multi_size_tensors = [torch.tensor(m, dtype=torch.uint8).unsqueeze(0).cuda() for m in multi_size_masks]\n",
    "\n",
    "    # Save all\n",
    "    output_case_dir = output_dir / case_id\n",
    "    output_case_dir.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(input_tensor, output_case_dir / \"input_tensor.pt\")\n",
    "    torch.save(distance_tensor, output_case_dir / \"distance_map.pt\")\n",
    "    for i, mask in enumerate(multi_size_tensors):\n",
    "        torch.save(mask, output_case_dir / f\"multi_size_mask_{i}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac6e5ec-ad39-42e9-8a62-c1140398e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preprocessing(root_path, output_path):\n",
    "    input_path = Path(root_path)\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    all_case_dirs = [d for d in input_path.iterdir() if d.is_dir()]\n",
    "    print(f\"Found {len(all_case_dirs)} cases.\")\n",
    "\n",
    "    for case_dir in tqdm(all_case_dirs):\n",
    "        case_id = case_dir.name  # e.g., MSLS_000\n",
    "        try:\n",
    "            preprocess_case(case_dir, output_path, case_id)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed on {case_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e6a0eb7-7da6-400d-9446-97773e286c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 93 cases.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3898edd29ec248b9ade9c59576aa9e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the preprocessing on the training set\n",
    "RAW_DATA_PATH = \"../data/01-Pre-Processed-Data/train\"\n",
    "OUTPUT_PATH = \"../data/02-Tensor-Data/train\"\n",
    "\n",
    "run_preprocessing(RAW_DATA_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc06f78a-1ed2-4e4b-90f7-391b10748569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 cases.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd91821ad48148c18c4c4fc21301417f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the preprocessing on the test set (with gt mask)\n",
    "RAW_DATA_PATH = \"../data/01-Pre-Processed-Data/test/test_MASK\"\n",
    "OUTPUT_PATH = \"../data/02-Tensor-Data/test/test_MASK\"\n",
    "\n",
    "run_preprocessing(RAW_DATA_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33b1eae6-3dee-4f09-aaf6-1bcfc5da9e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variation for the test set WITHOUT the gt seg mask\n",
    "def preprocess_case_inputs_only(input_dir, output_dir, case_id):\n",
    "    # Required input modalities only\n",
    "    required_files = {\n",
    "        \"flair\": input_dir / f\"{case_id}_flair.nii.gz\",\n",
    "        \"t1\": input_dir / f\"{case_id}_t1.nii.gz\",\n",
    "        \"t2\": input_dir / f\"{case_id}_t2.nii.gz\",\n",
    "    }\n",
    "\n",
    "    for key, path in required_files.items():\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"Missing file for '{key}': {path}\")\n",
    "\n",
    "    # Load and normalize each modality\n",
    "    flair = normalize_intensity(load_nifti(required_files[\"flair\"]))\n",
    "    t1 = normalize_intensity(load_nifti(required_files[\"t1\"]))\n",
    "    t2 = normalize_intensity(load_nifti(required_files[\"t2\"]))\n",
    "\n",
    "    # Stack modalities (C, D, H, W)\n",
    "    stacked = np.stack([flair, t1, t2], axis=0)\n",
    "    input_tensor = torch.tensor(stacked, dtype=torch.float32).cuda()\n",
    "\n",
    "    # Save preprocessed tensor\n",
    "    output_case_dir = output_dir / case_id\n",
    "    output_case_dir.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(input_tensor, output_case_dir / \"input_tensor.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "003c7bb6-5162-456c-8893-f286d27e1873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variation for the test set WITHOUT the gt seg mask\n",
    "def run_preprocessing_inputs_only(root_path, output_path):\n",
    "    input_path = Path(root_path)\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    all_case_dirs = [d for d in input_path.iterdir() if d.is_dir()]\n",
    "    print(f\"Found {len(all_case_dirs)} cases.\")\n",
    "\n",
    "    for case_dir in tqdm(all_case_dirs):\n",
    "        case_id = case_dir.name\n",
    "        try:\n",
    "            preprocess_case_inputs_only(case_dir, output_path, case_id)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Skipping {case_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dc58dd7-0257-4954-905d-404f8975dd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 cases.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922b91484fc541fdaa8b2304e8f13c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the preprocessing on the test set (no gt mask)\n",
    "RAW_DATA_PATH = \"../data/01-Pre-Processed-Data/test/test\"\n",
    "OUTPUT_PATH = \"../data/02-Tensor-Data/test/test\"\n",
    "\n",
    "run_preprocessing_inputs_only(RAW_DATA_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2f3830-e80c-47b9-9f3f-3b1268619f14",
   "metadata": {},
   "source": [
    "## Build the Dataset and Dataloaders for the MSLesSeg preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "59da4863-42f3-4d2c-a675-c8cdc1eeeace",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 29 (3461500416.py, line 30)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[119]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mcase_dir = self.sample_dirs[idx]\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after function definition on line 29\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MSLesionDataset(Dataset):\n",
    "    def __init__(self, root_dir, include_labels=True, sample_ids=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir: The root directory containing the sample directories.\n",
    "            include_labels: Whether to include the segmentation masks and distance maps.\n",
    "            sample_ids: List of sample directories to use (if performing k-fold).\n",
    "            transform: Optional transformation to apply to the input tensor.\n",
    "        \"\"\"\n",
    "        self.root_dir = Path(root_dir)\n",
    "        all_samples = sorted([d for d in self.root_dir.iterdir() if d.is_dir()])\n",
    "\n",
    "        if sample_ids:\n",
    "            self.sample_dirs = [d for d in all_samples if d.name in sample_ids]\n",
    "        else:\n",
    "            self.sample_dirs = all_samples\n",
    "\n",
    "        self.include_labels = include_labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_dirs)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "        case_dir = self.sample_dirs[idx]\n",
    "\n",
    "        # Load the input tensor\n",
    "        input_tensor = torch.load(case_dir / \"input_tensor.pt\").float()\n",
    "\n",
    "        # Get the size of the input tensor (height, width)\n",
    "        target_size = input_tensor.shape[1:]  # H, W\n",
    "\n",
    "        if self.transform:\n",
    "            input_tensor = self.transform(input_tensor)\n",
    "\n",
    "        # If no labels are required (inference or test without labels)\n",
    "        if not self.include_labels:\n",
    "            return input_tensor, str(case_dir.name)\n",
    "\n",
    "        # Load labels (distance map and multi-size masks)\n",
    "        distance_map = torch.load(case_dir / \"distance_map.pt\").float()\n",
    "        multi_size_masks = []\n",
    "\n",
    "        for i in range(3):  # Hardcoded for 3 multi-size masks\n",
    "            path = case_dir / f\"multi_size_mask_{i}.pt\"\n",
    "            if path.exists():\n",
    "                mask = torch.load(path).float()\n",
    "            else:\n",
    "                mask = torch.zeros_like(distance_map)  # Fallback\n",
    "\n",
    "            # Get the current size of the mask\n",
    "            current_size = mask.shape\n",
    "            print(f\"Mask {i} shape: {current_size}\")\n",
    "\n",
    "            # If the mask is not the same size as the input tensor, pad it\n",
    "            if current_size != target_size:\n",
    "                print(f\"Padding mask from {current_size} to {target_size}\")\n",
    "                pad_height = target_size[0] - current_size[0]\n",
    "                pad_width = target_size[1] - current_size[1]\n",
    "\n",
    "                # Apply padding (this pads with zeros)\n",
    "                padding = (0, pad_width, 0, pad_height)  # For width and height\n",
    "                mask_padded = F.pad(mask, padding, mode='constant', value=0)\n",
    "            else:\n",
    "                mask_padded = mask\n",
    "\n",
    "            print(f\"Padded mask {i} shape: {mask_padded.shape}\")\n",
    "            multi_size_masks.append(mask_padded)\n",
    "\n",
    "        # Concatenate the padded masks along the 0th dimension\n",
    "        try:\n",
    "            return {\n",
    "                \"input\": input_tensor,\n",
    "                \"distance\": distance_map,\n",
    "                \"multi_masks\": torch.cat(multi_size_masks, dim=0),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error during concatenation: {e}\")\n",
    "            for i, mask in enumerate(multi_size_masks):\n",
    "                print(f\"Shape of mask {i}: {mask.shape}\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "03106a16-273a-4298-b82f-e15990258754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_dataloader(data_root, batch_size=2, num_workers=4, shuffle=True, include_labels=True, sample_ids=None):\n",
    "    \"\"\"\n",
    "    Get the DataLoader for the given dataset directory.\n",
    "\n",
    "    Args:\n",
    "        data_root: Path to the dataset root directory.\n",
    "        batch_size: Number of samples per batch.\n",
    "        num_workers: Number of workers for loading data in parallel.\n",
    "        shuffle: Whether to shuffle the data.\n",
    "        include_labels: Whether to include segmentation masks and distance maps.\n",
    "        sample_ids: A list of sample ids for a specific split (for k-fold).\n",
    "    \"\"\"\n",
    "    dataset = MSLesionDataset(data_root, include_labels=include_labels, sample_ids=sample_ids)\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c5e4b760-d9c4-4bc1-b231-b8f08f48331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of some (non training) parameters\n",
    "NUM_WORKERS_TRAIN = 0\n",
    "NUM_WORKERS_VAL = 0\n",
    "NUM_WORKERS_TEST = 0\n",
    "\n",
    "BATCH_SIZE_TRAIN = 6\n",
    "BATCH_SIZE_VAL = 3\n",
    "BATCH_SIZE_TEST = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0a49cd1c-7775-40f0-91ec-aae0a176be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "TRAIN_TENSOR_DATA_PATH = \"../data/02-Tensor-Data/train\"\n",
    "\n",
    "# List of all available case IDs in the train directory\n",
    "all_cases = sorted(os.listdir(TRAIN_TENSOR_DATA_PATH))\n",
    "random.seed(42)\n",
    "\n",
    "# For k-fold cross-validation, split train into train and validation (80-20)\n",
    "split = int(0.8 * len(all_cases))\n",
    "train_ids = all_cases[:split]\n",
    "val_ids = all_cases[split:]\n",
    "\n",
    "train_loader = get_dataloader(TRAIN_TENSOR_DATA_PATH, sample_ids=train_ids, batch_size=BATCH_SIZE_TRAIN, num_workers=NUM_WORKERS_TRAIN)\n",
    "val_loader = get_dataloader(TRAIN_TENSOR_DATA_PATH, sample_ids=val_ids, batch_size=BATCH_SIZE_VAL, num_workers=NUM_WORKERS_VAL, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6ca4c7fb-ef72-48f3-9583-19503337a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TENSOR_DATA_PATH_NOMASK = \"../data/02-Tensor-Data/test/test\"\n",
    "TEST_TENSOR_DATA_PATH_MASK = \"../data/02-Tensor-Data/test/test_MASK\"\n",
    "\n",
    "# Test set WITHOUT labels (for inference or prediction)\n",
    "test_loader_no_labels = get_dataloader(TEST_TENSOR_DATA_PATH_NOMASK, include_labels=False, batch_size=BATCH_SIZE_TEST, num_workers=NUM_WORKERS_TEST)\n",
    "\n",
    "# Test set WITH labels (for final evaluation or performance metrics)\n",
    "test_loader_with_labels = get_dataloader(TEST_TENSOR_DATA_PATH_MASK, include_labels=True, batch_size=BATCH_SIZE_TEST, num_workers=NUM_WORKERS_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3a3370c6-c8fa-4871-9093-f5e13a8d9002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to perform K-Fold Cross Validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def get_k_fold_loaders(data_root, k=5, batch_size=BATCH_SIZE_TRAIN, num_workers=NUM_WORKERS_TRAIN):\n",
    "    # Get all case IDs\n",
    "    all_cases = sorted(os.listdir(data_root))\n",
    "    \n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_loaders = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(all_cases):\n",
    "        train_ids = [all_cases[i] for i in train_idx]\n",
    "        val_ids = [all_cases[i] for i in val_idx]\n",
    "\n",
    "        train_loader = get_dataloader(data_root, sample_ids=train_ids, batch_size=batch_size, num_workers=num_workers)\n",
    "        val_loader = get_dataloader(data_root, sample_ids=val_ids, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "        fold_loaders.append((train_loader, val_loader))\n",
    "\n",
    "    return fold_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3a1847cf-501d-4233-8600-08376ed05e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_loaders = get_k_fold_loaders(TRAIN_TENSOR_DATA_PATH, k=5, batch_size=2, num_workers=4)\n",
    "\n",
    "# Example: Access the first fold\n",
    "# train_loader, val_loader = fold_loaders[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f0311-9b80-4bf0-b80d-996692a83815",
   "metadata": {},
   "source": [
    "### PyTorch Lightning DataModule\n",
    "\n",
    "This particular version of PyTorch Lightning, and in general from version 2.x onward require a ***LightningDataModule*** instead of passing the dataloaders directly to the ***.fit()*** method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ac9f0867-1234-4a5d-b1e1-bd8802d56a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningDataModule\n",
    "\n",
    "class MSLesionSegmentationDataModule(LightningDataModule):\n",
    "    def __init__(self, train_dataloader, val_dataloader):\n",
    "        super().__init__()\n",
    "        self.train_dataloader_instance = train_dataloader\n",
    "        self.val_dataloader_instance = val_dataloader\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_dataloader_instance\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_dataloader_instance\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.val_dataloader_instance  # Optional, if you want to test on the validation set as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b08f0-2188-400b-b30e-88997983f659",
   "metadata": {},
   "source": [
    "## The Model's Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "726efd51-8791-4abc-9204-b5571ed1a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture of Experts with dynamic input size support via GAP.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_experts=4, expert_dim=512):\n",
    "        super(MoE, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "        self.gate = nn.Linear(input_dim, num_experts)\n",
    "        self.experts = nn.ModuleList([nn.Linear(input_dim, expert_dim) for _ in range(num_experts)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is expected to be (B, C), already pooled\n",
    "        gate_weights = F.softmax(self.gate(x), dim=-1)\n",
    "        expert_outputs = [expert(x) for expert in self.experts]\n",
    "        output = sum(gate_weights[:, i].unsqueeze(-1) * expert_outputs[i] for i in range(self.num_experts))\n",
    "        return output\n",
    "\n",
    "class UNetMoE(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, num_experts=4, expert_dim=512):\n",
    "        super(UNetMoE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(2)\n",
    "        )\n",
    "\n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv3d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(32, out_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        # Will be set dynamically after seeing one input\n",
    "        self.moe = None\n",
    "        self.num_experts = num_experts\n",
    "        self.expert_dim = expert_dim\n",
    "\n",
    "        self.deep_supervision = nn.Conv3d(128, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder[0:2](x)\n",
    "        enc2 = self.encoder[2:](enc1)\n",
    "        mid = self.middle(enc2)\n",
    "        dec = self.decoder[0:2](mid)\n",
    "        dec_out = self.decoder[2:](dec)\n",
    "\n",
    "        # Global pooling before MoE\n",
    "        pooled = F.adaptive_avg_pool3d(dec, 1).view(x.size(0), -1)\n",
    "\n",
    "        # Init MoE if not yet initialized\n",
    "        if self.moe is None:\n",
    "            self.moe = MoE(input_dim=pooled.size(1), num_experts=self.num_experts, expert_dim=self.expert_dim).to(x.device)\n",
    "\n",
    "        moe_out = self.moe(pooled)\n",
    "        deep_supervision_out = self.deep_supervision(mid)\n",
    "\n",
    "        return dec_out, deep_supervision_out, moe_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d7a31-ebcf-4781-aefe-9a2ff343c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for testing (TO REMOVE)\n",
    "model = UNetMoE(in_channels=3, out_channels=1)\n",
    "dummy_input = torch.randn(2, 3, 128, 128, 128)  # e.g., batch size 2, 3 modalities\n",
    "dec_out, deep_out, moe_out = model(dummy_input)\n",
    "print(dec_out.shape, deep_out.shape, moe_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3c2123-75a5-45fa-ab0b-6887c61831ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Summary of the Architecture\n",
    "from torchsummary import summary\n",
    "\n",
    "# Set the device and the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNetMoE(in_channels=3, out_channels=1, num_experts=4, expert_dim=512).to(device)\n",
    "\n",
    "# Get the summary of the model (use an example input size, e.g., 128x128x128 for 3D)\n",
    "summary(model, (3, 128, 128, 128))  # (channels, depth, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff77208-1cc9-4c68-8aa7-eaf9a3def39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the graphical visualization\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Dummy input\n",
    "dummy_input = torch.randn(1, 3, 128, 128, 128).cuda()  # Adjust for batch size and input shape\n",
    "\n",
    "# Forward pass through the model\n",
    "output, _, _ = model(dummy_input)\n",
    "\n",
    "# Visualize the graph\n",
    "make_dot(output, params=dict(model.named_parameters())).render(\"model_architecture\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dcd022-a711-4931-b46c-336f8565d581",
   "metadata": {},
   "source": [
    "## Visualized Model Architecture\n",
    "\n",
    "![Model Architecture Plot](./model_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be907f2a-2600-407f-8649-42965a1fb3d1",
   "metadata": {},
   "source": [
    "## The Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d3471-ee79-41ff-b176-929864412593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.segmentation import DiceScore\n",
    "\n",
    "class MSLeasionSegmentationModel(pl.LightningModule):\n",
    "    def __init__(self, model, lr=1e-4):\n",
    "        super(MSLeasionSegmentationModel, self).__init__()\n",
    "        \n",
    "        # Initialize the model (nnU-Net + MoE)\n",
    "        self.model = model\n",
    "        \n",
    "        # Learning rate for optimizer\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Set up Dice Score metric for evaluation\n",
    "        self.dice_metric = DiceScore(num_classes=2)  # Binary segmentation: lesion or background\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass (output, deep supervision, MoE output)\n",
    "        output, deep_supervision, moe_output = self.model(x)\n",
    "        return output, deep_supervision, moe_output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Extract input data and target segmentation masks from batch\n",
    "        x = batch['input_tensor']  # Shape: (B, 3, D, H, W)\n",
    "        y = batch['segmentation_mask']  # Shape: (B, 1, D, H, W)\n",
    "        \n",
    "        # Get model output\n",
    "        output, _, _ = self(x)\n",
    "        \n",
    "        # Compute loss (using binary cross entropy for binary segmentation)\n",
    "        loss = F.binary_cross_entropy_with_logits(output, y)\n",
    "        \n",
    "        # Log the loss\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Extract input data and target segmentation masks from batch\n",
    "        x = batch['input_tensor']  # Shape: (B, 3, D, H, W)\n",
    "        y = batch['segmentation_mask']  # Shape: (B, 1, D, H, W)\n",
    "        \n",
    "        # Get model output\n",
    "        output, _, _ = self(x)\n",
    "        \n",
    "        # Calculate the Dice score\n",
    "        dice_score = self.dice_metric(output.sigmoid(), y)\n",
    "        \n",
    "        # Log validation metrics\n",
    "        self.log('val_dice', dice_score, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return dice_score\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Extract input data and target segmentation masks from batch\n",
    "        x = batch['input_tensor']  # Shape: (B, 3, D, H, W)\n",
    "        y = batch['segmentation_mask']  # Shape: (B, 1, D, H, W)\n",
    "        \n",
    "        # Get model output\n",
    "        output, _, _ = self(x)\n",
    "        \n",
    "        # Calculate the Dice score\n",
    "        dice_score = self.dice_metric(output.sigmoid(), y)\n",
    "        \n",
    "        # Log test metrics\n",
    "        self.log('test_dice', dice_score, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return dice_score\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Set up Adam optimizer with the specified learning rate\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        # Optionally, add a scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "        \n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_dice'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b884f-771e-4852-be17-23973479e5a5",
   "metadata": {},
   "source": [
    "## The Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330d4d97-5326-4dcf-b27e-2f3a57de10f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a free account with Weights and Biases, and create a new project in order to obtain an API Key for the training\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the env variable\n",
    "api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "# Login to wandb\n",
    "if api_key:\n",
    "    os.environ[\"WANDB_API_KEY\"] = api_key\n",
    "    wandb.login()\n",
    "else:\n",
    "    print(\"❌ WANDB_API_KEY not found in .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd78050f-09dd-4251-a8b1-468756f2f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Weights and Biases logger\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project='MSLesSeg-4-ICPR',     # Change to your actual project name\n",
    "    name='nnUNet_MoE_run1', # A specific run name\n",
    "    log_model=True          # Optional: log model checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc45e04-32b8-4824-a118-01950a2d1976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the multiprocessing start to 'spawn' instead of 'fork' due to CUDA issues with the Dataloader\n",
    "import multiprocessing\n",
    "import torch\n",
    "\n",
    "# Set the start method for multiprocessing\n",
    "multiprocessing.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c82afa2-56b6-4202-8aba-365d4206b4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "\n",
    "# Initialize the model\n",
    "model = MSLeasionSegmentationModel(model=UNetMoE(in_channels=3, out_channels=1, num_experts=4, expert_dim=512))\n",
    "\n",
    "# Set up callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_dice',  # Monitor the validation Dice score\n",
    "    dirpath='checkpoints/',\n",
    "    filename='best_checkpoint',\n",
    "    save_top_k=1,\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_dice',  # Early stopping based on the validation Dice score\n",
    "    patience=10,\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "# Intialize the Trainer\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",        # or \"cpu\" or \"auto\"\n",
    "    devices=1,                # Number of GPUs to use\n",
    "    max_epochs=100,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    logger=wandb_logger,\n",
    ")\n",
    "\n",
    "# Instantiate the PyTorch LighningDataModule to run the training\n",
    "data_module = MSLesionSegmentationDataModule(train_loader, val_loader)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee0a5d-61d4-4561-a731-b24483798f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "trainer.test(model, test_dataloader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6833331b-df4e-485b-8b26-e617ad583746",
   "metadata": {},
   "source": [
    "## Visualizing the Learned Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2febf704-6408-4a85-8d9c-19b88747f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to extract features from the model\n",
    "def extract_features(model, dataloader, device='cuda'):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, target = data['image'].to(device), data['mask'].to(device)  # Replace 'image' and 'mask' keys as per your dataset\n",
    "            \n",
    "            # Forward pass through the model\n",
    "            output = model(inputs)  # This is where we extract the feature; adjust according to your model's architecture\n",
    "            \n",
    "            # Extract features from MoE (or any other layer you want to visualize)\n",
    "            # Here I assume output is the final feature map after the MoE layer\n",
    "            feature_map = output.view(output.size(0), -1)  # Flatten the features to (batch_size, features)\n",
    "            features.append(feature_map.cpu().numpy())\n",
    "            labels.append(target.cpu().numpy())  # Add the target (or ground truth) labels for color coding in t-SNE\n",
    "\n",
    "    features = np.concatenate(features, axis=0)  # Combine all the feature maps\n",
    "    labels = np.concatenate(labels, axis=0)  # Combine all the labels\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "# Function to apply t-SNE on features\n",
    "def plot_tsne(features, labels):\n",
    "    # Standardize the features (optional but recommended for t-SNE)\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(features)\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=labels, cmap='jet', alpha=0.5)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('t-SNE Visualization of Learned Representations')\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage\n",
    "train_dataloader = # Your train DataLoader here\n",
    "\n",
    "# Extract features and labels from the model\n",
    "features, labels = extract_features(model, train_dataloader, device='cuda')\n",
    "\n",
    "# Plot t-SNE visualization\n",
    "plot_tsne(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4089cdf9-90ee-4700-8ec5-2c246a69d848",
   "metadata": {},
   "source": [
    "## Post Hoc Model Explainability - GradCAM++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee0469-f55a-4c46-9c72-a7ba80ad41b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from captum.attr import LayerGradCam, LayerGradCamPlusPlus\n",
    "from captum.attr import GuidedBackprop\n",
    "\n",
    "# Function to visualize GradCAM++ output\n",
    "def visualize_gradcam_plus_plus(model, input_tensor, target_class=None, layer_name='decoder', device='cuda'):\n",
    "    # Move input tensor to the appropriate device\n",
    "    input_tensor = input_tensor.to(device)\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Create GradCAM++ object for the specified layer\n",
    "    gradcam_pp = LayerGradCamPlusPlus(model, model.decoder[2])  # Assuming 'decoder' is your final layer, adjust accordingly.\n",
    "    \n",
    "    # Apply GradCAM++ to input tensor\n",
    "    attributions = gradcam_pp.attribute(input_tensor, target=target_class)\n",
    "    \n",
    "    # Convert attributions to numpy\n",
    "    attributions = attributions.cpu().detach().numpy()\n",
    "    \n",
    "    # Normalize the heatmap\n",
    "    heatmap = np.sum(attributions[0], axis=0)  # Summing over channels for a single-channel heatmap\n",
    "    heatmap = np.maximum(heatmap, 0)  # ReLU to ignore negative values\n",
    "    heatmap = cv2.resize(heatmap, (input_tensor.shape[2], input_tensor.shape[3]))  # Resize to input size\n",
    "    heatmap = cv2.normalize(heatmap, None, 0, 1, cv2.NORM_MINMAX)  # Normalize the heatmap to [0, 1]\n",
    "    \n",
    "    # Convert the original image to numpy and rescale\n",
    "    input_image = input_tensor[0].cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    input_image = cv2.resize(input_image, (input_tensor.shape[2], input_tensor.shape[3]))  # Resize to input size\n",
    "    \n",
    "    # Create a colormap for the heatmap\n",
    "    colormap = plt.get_cmap('jet')\n",
    "    colored_heatmap = colormap(heatmap)  # Apply colormap\n",
    "    \n",
    "    # Overlay the heatmap on top of the original image\n",
    "    superimposed_img = np.uint8(input_image * 255)  # Convert original image back to [0, 255] range\n",
    "    superimposed_img = cv2.addWeighted(superimposed_img, 0.7, np.uint8(colored_heatmap[:, :, :3] * 255), 0.3, 0)\n",
    "    \n",
    "    # Plot the result\n",
    "    plt.imshow(superimposed_img)\n",
    "    plt.title('GradCAM++ Heatmap')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage (assuming `model` is your trained model and `input_tensor` is an input image)\n",
    "input_tensor = torch.randn(1, 3, 128, 128).to(device)  # Example input, use your actual input tensor\n",
    "target_class = None  # You can provide a target class or leave as None for model's top predicted class\n",
    "\n",
    "visualize_gradcam_plus_plus(model, input_tensor, target_class=target_class, layer_name='decoder', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb9090-3260-49ce-be05-a03c2d122e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

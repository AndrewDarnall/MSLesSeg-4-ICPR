{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "919db79e-a3bd-4896-89a1-1d9e0017be10",
   "metadata": {},
   "source": [
    "# Multiple Sclerosis Lesion Segmentation Dataset Overview\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f36701b-9820-49dd-8553-ca671481b49c",
   "metadata": {},
   "source": [
    "\n",
    "- Introduction to the challenge {evaluation metrics, comparrison to other datasets} (with link to website [https://iplab.dmi.unict.it/mfs/ms-les-seg/] and paper)\n",
    "- Introduction to the Dataset (creation, pre-processing, contents and distribution data with visualizations)\n",
    "\n",
    "Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4026f2-e84e-4427-8ab1-eff9d1b54d98",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What have winning strategies been so far?\n",
    "\n",
    "1) Ensamble methods\n",
    "2) Different Registration Spaces\n",
    "3) Selecting only the specific data from the image that can better find the segmentation mask\n",
    "4) Using only ONE modality (FLAIR) and performing Data Augmentations on it + Deep CNN Architecture\n",
    "5) nnU-Net for automatic configuration to the dataset (Interesting)\n",
    "6) Usage of Mamba-based LightM-UNet\n",
    "7) Focal Loss + DICE Loss\n",
    "\n",
    "---\n",
    "\n",
    "## Personal Goals\n",
    "\n",
    "1) Parameter Efficiency\n",
    "2) (XAI) Explainable Architecture\n",
    "3) Segmentation Accuracy (Mean DICE >= 60)\n",
    "4) [Optional] --> Single Model (No Ensambles)\n",
    "\n",
    "---\n",
    "\n",
    "## Personal Model Proposition\n",
    "\n",
    "1) Use ALL modalities\n",
    "2) Preprocess with (Distance Based Labelling, Multi-Sized Labelling ~ Could aid the MoE variant)\n",
    "3) nnU-Net + MoE (+ Deep Supervision ?)\n",
    "\n",
    "### Optional\n",
    "\n",
    "- Bring the model visualization to Extended Reality Domain ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b3a0a-2f43-44f9-a898-28f1fa46b75a",
   "metadata": {},
   "source": [
    "# The Experiments\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293bc032-fddb-44a7-8db3-cd8bdf0315e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from skimage.transform import resize\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13613113-a191-4399-9b42-9f6df49bc6e9",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing\n",
    "\n",
    "- Expand more on the performed preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2027badd-35e2-42ac-b423-e85e9e99d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nifti(file_path):\n",
    "    return nib.load(file_path).get_fdata()\n",
    "\n",
    "def normalize_intensity(img):\n",
    "    # Normalize each modality independently (z-score)\n",
    "    img = img.astype(np.float32)\n",
    "    img = (img - np.mean(img)) / (np.std(img) + 1e-5)\n",
    "    return img\n",
    "\n",
    "def compute_distance_label(mask):\n",
    "    mask = mask.astype(bool)\n",
    "    pos_dist = distance_transform_edt(mask)\n",
    "    neg_dist = distance_transform_edt(~mask)\n",
    "    dist_map = pos_dist - neg_dist  # Signed distance transform\n",
    "    return dist_map\n",
    "\n",
    "def generate_multi_size_masks(mask, scales=[1.0, 0.5, 0.25]):\n",
    "    multi_res = []\n",
    "    for scale in scales:\n",
    "        if scale == 1.0:\n",
    "            multi_res.append(mask)\n",
    "        else:\n",
    "            resized = resize(mask, output_shape=tuple(int(s * scale) for s in mask.shape),\n",
    "                             order=1, preserve_range=True, anti_aliasing=True)\n",
    "            multi_res.append(resized.astype(mask.dtype))\n",
    "    return multi_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3515529-ff9f-4f47-935b-e09a357f22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_case(input_dir, output_dir, case_id):\n",
    "    flair = normalize_intensity(load_nifti(input_dir / f\"{case_id}_flair.nii.gz\"))\n",
    "    t1 = normalize_intensity(load_nifti(input_dir / f\"{case_id}_t1.nii.gz\"))\n",
    "    t2 = normalize_intensity(load_nifti(input_dir / f\"{case_id}_t2.nii.gz\"))\n",
    "    seg = load_nifti(input_dir / f\"{case_id}_seg.nii.gz\").astype(np.uint8)\n",
    "\n",
    "    # Stack input modalities into a tensor (C, D, H, W)\n",
    "    stacked = np.stack([flair, t1, t2], axis=0)\n",
    "    # input_tensor = torch.tensor(stacked, dtype=torch.float32).cuda()\n",
    "    # Use CPU Tensors instead\n",
    "    input_tensor = torch.tensor(stacked, dtype=torch.float32)\n",
    "    \n",
    "    # Compute distance map from segmentation mask\n",
    "    distance_map = compute_distance_label(seg)\n",
    "    # distance_tensor = torch.tensor(distance_map, dtype=torch.float32).unsqueeze(0).cuda()\n",
    "    # Use CPU Tensors instead\n",
    "    distance_tensor = torch.tensor(distance_map, dtype=torch.float32).unsqueeze(0)\n",
    "    \n",
    "    # Generate multi-size label masks\n",
    "    multi_size_masks = generate_multi_size_masks(seg)\n",
    "    # multi_size_tensors = [torch.tensor(m, dtype=torch.uint8).unsqueeze(0).cuda() for m in multi_size_masks]\n",
    "    # Use CPU Tensors instead\n",
    "    multi_size_tensors = [torch.tensor(m, dtype=torch.uint8).unsqueeze(0) for m in multi_size_masks]\n",
    "\n",
    "    # Load it back into a PyTorch Tensor for storing\n",
    "    seg_tensor = torch.tensor(seg, dtype=torch.uint8)\n",
    "    # Add the batch dimension in order to make it compatible with the other Tensor sizes\n",
    "    seg_tensor = seg_tensor.unsqueeze(0)\n",
    "\n",
    "    \n",
    "    # Save all\n",
    "    output_case_dir = output_dir / case_id\n",
    "    output_case_dir.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(input_tensor, output_case_dir / \"input_tensor.pt\")\n",
    "    torch.save(distance_tensor, output_case_dir / \"distance_map.pt\")\n",
    "    torch.save(seg_tensor, output_case_dir / \"seg_mask.pt\")\n",
    "    for i, mask in enumerate(multi_size_tensors):\n",
    "        torch.save(mask, output_case_dir / f\"multi_size_mask_{i}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac6e5ec-ad39-42e9-8a62-c1140398e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_preprocessing(root_path, output_path):\n",
    "    input_path = Path(root_path)\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    # Get all case directories\n",
    "    all_case_dirs = [d for d in input_path.iterdir() if d.is_dir()]\n",
    "    print(f\"Found {len(all_case_dirs)} cases.\")\n",
    "\n",
    "    for case_dir in tqdm(all_case_dirs):\n",
    "        case_id = case_dir.name  # e.g., MSLS_000\n",
    "        output_case_path = output_path / case_id  # Define output path for each case\n",
    "\n",
    "        # Check if the case has already been processed (e.g., output directory or file exists)\n",
    "        if output_case_path.exists():\n",
    "            print(f\"✅ Skipping {case_id}, already processed.\")\n",
    "            continue  # Skip processing if the case already exists\n",
    "\n",
    "        try:\n",
    "            preprocess_case(case_dir, output_path, case_id)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed on {case_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e6a0eb7-7da6-400d-9446-97773e286c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 93 cases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 93/93 [00:00<00:00, 22839.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Skipping MSLS_019, already processed.\n",
      "✅ Skipping MSLS_039, already processed.\n",
      "✅ Skipping MSLS_059, already processed.\n",
      "✅ Skipping MSLS_000, already processed.\n",
      "✅ Skipping MSLS_001, already processed.\n",
      "✅ Skipping MSLS_002, already processed.\n",
      "✅ Skipping MSLS_003, already processed.\n",
      "✅ Skipping MSLS_004, already processed.\n",
      "✅ Skipping MSLS_005, already processed.\n",
      "✅ Skipping MSLS_006, already processed.\n",
      "✅ Skipping MSLS_007, already processed.\n",
      "✅ Skipping MSLS_008, already processed.\n",
      "✅ Skipping MSLS_009, already processed.\n",
      "✅ Skipping MSLS_010, already processed.\n",
      "✅ Skipping MSLS_011, already processed.\n",
      "✅ Skipping MSLS_012, already processed.\n",
      "✅ Skipping MSLS_013, already processed.\n",
      "✅ Skipping MSLS_014, already processed.\n",
      "✅ Skipping MSLS_015, already processed.\n",
      "✅ Skipping MSLS_016, already processed.\n",
      "✅ Skipping MSLS_017, already processed.\n",
      "✅ Skipping MSLS_018, already processed.\n",
      "✅ Skipping MSLS_020, already processed.\n",
      "✅ Skipping MSLS_021, already processed.\n",
      "✅ Skipping MSLS_022, already processed.\n",
      "✅ Skipping MSLS_023, already processed.\n",
      "✅ Skipping MSLS_024, already processed.\n",
      "✅ Skipping MSLS_025, already processed.\n",
      "✅ Skipping MSLS_026, already processed.\n",
      "✅ Skipping MSLS_027, already processed.\n",
      "✅ Skipping MSLS_028, already processed.\n",
      "✅ Skipping MSLS_029, already processed.\n",
      "✅ Skipping MSLS_030, already processed.\n",
      "✅ Skipping MSLS_031, already processed.\n",
      "✅ Skipping MSLS_032, already processed.\n",
      "✅ Skipping MSLS_033, already processed.\n",
      "✅ Skipping MSLS_034, already processed.\n",
      "✅ Skipping MSLS_035, already processed.\n",
      "✅ Skipping MSLS_036, already processed.\n",
      "✅ Skipping MSLS_037, already processed.\n",
      "✅ Skipping MSLS_038, already processed.\n",
      "✅ Skipping MSLS_040, already processed.\n",
      "✅ Skipping MSLS_041, already processed.\n",
      "✅ Skipping MSLS_042, already processed.\n",
      "✅ Skipping MSLS_043, already processed.\n",
      "✅ Skipping MSLS_044, already processed.\n",
      "✅ Skipping MSLS_045, already processed.\n",
      "✅ Skipping MSLS_046, already processed.\n",
      "✅ Skipping MSLS_047, already processed.\n",
      "✅ Skipping MSLS_048, already processed.\n",
      "✅ Skipping MSLS_049, already processed.\n",
      "✅ Skipping MSLS_050, already processed.\n",
      "✅ Skipping MSLS_051, already processed.\n",
      "✅ Skipping MSLS_052, already processed.\n",
      "✅ Skipping MSLS_053, already processed.\n",
      "✅ Skipping MSLS_054, already processed.\n",
      "✅ Skipping MSLS_055, already processed.\n",
      "✅ Skipping MSLS_056, already processed.\n",
      "✅ Skipping MSLS_057, already processed.\n",
      "✅ Skipping MSLS_058, already processed.\n",
      "✅ Skipping MSLS_060, already processed.\n",
      "✅ Skipping MSLS_061, already processed.\n",
      "✅ Skipping MSLS_062, already processed.\n",
      "✅ Skipping MSLS_063, already processed.\n",
      "✅ Skipping MSLS_064, already processed.\n",
      "✅ Skipping MSLS_065, already processed.\n",
      "✅ Skipping MSLS_066, already processed.\n",
      "✅ Skipping MSLS_067, already processed.\n",
      "✅ Skipping MSLS_068, already processed.\n",
      "✅ Skipping MSLS_069, already processed.\n",
      "✅ Skipping MSLS_070, already processed.\n",
      "✅ Skipping MSLS_071, already processed.\n",
      "✅ Skipping MSLS_072, already processed.\n",
      "✅ Skipping MSLS_073, already processed.\n",
      "✅ Skipping MSLS_074, already processed.\n",
      "✅ Skipping MSLS_075, already processed.\n",
      "✅ Skipping MSLS_076, already processed.\n",
      "✅ Skipping MSLS_077, already processed.\n",
      "✅ Skipping MSLS_078, already processed.\n",
      "✅ Skipping MSLS_079, already processed.\n",
      "✅ Skipping MSLS_080, already processed.\n",
      "✅ Skipping MSLS_081, already processed.\n",
      "✅ Skipping MSLS_082, already processed.\n",
      "✅ Skipping MSLS_083, already processed.\n",
      "✅ Skipping MSLS_084, already processed.\n",
      "✅ Skipping MSLS_085, already processed.\n",
      "✅ Skipping MSLS_086, already processed.\n",
      "✅ Skipping MSLS_087, already processed.\n",
      "✅ Skipping MSLS_088, already processed.\n",
      "✅ Skipping MSLS_089, already processed.\n",
      "✅ Skipping MSLS_090, already processed.\n",
      "✅ Skipping MSLS_091, already processed.\n",
      "✅ Skipping MSLS_092, already processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the preprocessing on the training set\n",
    "RAW_DATA_PATH = \"../data/01-Pre-Processed-Data/train\"\n",
    "OUTPUT_PATH = \"../data/02-Tensor-Data/train\"\n",
    "\n",
    "run_preprocessing(RAW_DATA_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc06f78a-1ed2-4e4b-90f7-391b10748569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 cases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 22/22 [00:00<00:00, 16965.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Skipping MSLS_093, already processed.\n",
      "✅ Skipping MSLS_094, already processed.\n",
      "✅ Skipping MSLS_095, already processed.\n",
      "✅ Skipping MSLS_096, already processed.\n",
      "✅ Skipping MSLS_097, already processed.\n",
      "✅ Skipping MSLS_098, already processed.\n",
      "✅ Skipping MSLS_099, already processed.\n",
      "✅ Skipping MSLS_100, already processed.\n",
      "✅ Skipping MSLS_101, already processed.\n",
      "✅ Skipping MSLS_102, already processed.\n",
      "✅ Skipping MSLS_103, already processed.\n",
      "✅ Skipping MSLS_104, already processed.\n",
      "✅ Skipping MSLS_105, already processed.\n",
      "✅ Skipping MSLS_106, already processed.\n",
      "✅ Skipping MSLS_107, already processed.\n",
      "✅ Skipping MSLS_108, already processed.\n",
      "✅ Skipping MSLS_109, already processed.\n",
      "✅ Skipping MSLS_110, already processed.\n",
      "✅ Skipping MSLS_111, already processed.\n",
      "✅ Skipping MSLS_112, already processed.\n",
      "✅ Skipping MSLS_113, already processed.\n",
      "✅ Skipping MSLS_114, already processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the preprocessing on the test set (with gt mask)\n",
    "RAW_DATA_PATH = \"../data/01-Pre-Processed-Data/test/test_MASK\"\n",
    "OUTPUT_PATH = \"../data/02-Tensor-Data/test/test_MASK\"\n",
    "\n",
    "run_preprocessing(RAW_DATA_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33b1eae6-3dee-4f09-aaf6-1bcfc5da9e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variation for the test set WITHOUT the gt seg mask\n",
    "def preprocess_case_no_seg_mask(input_dir, output_dir, case_id):\n",
    "    flair = normalize_intensity(load_nifti(input_dir / f\"{case_id}_flair.nii.gz\"))\n",
    "    t1 = normalize_intensity(load_nifti(input_dir / f\"{case_id}_t1.nii.gz\"))\n",
    "    t2 = normalize_intensity(load_nifti(input_dir / f\"{case_id}_t2.nii.gz\"))\n",
    "\n",
    "    # Stack input modalities into a tensor (C, D, H, W)\n",
    "    stacked = np.stack([flair, t1, t2], axis=0)\n",
    "    # input_tensor = torch.tensor(stacked, dtype=torch.float32).cuda()\n",
    "    # Use CPU Tensors instead\n",
    "    input_tensor = torch.tensor(stacked, dtype=torch.float32)\n",
    "    \n",
    "    # Cannot compute the distance maps and multi scale masks due to lack of segmentation mask\n",
    "    \n",
    "    # Save all\n",
    "    output_case_dir = output_dir / case_id\n",
    "    output_case_dir.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(input_tensor, output_case_dir / \"input_tensor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "003c7bb6-5162-456c-8893-f286d27e1873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Variation for the test set WITHOUT the gt seg mask\n",
    "def run_preprocess_no_seg_mask(root_path, output_path):\n",
    "    input_path = Path(root_path)\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    # Get all case directories\n",
    "    all_case_dirs = [d for d in input_path.iterdir() if d.is_dir()]\n",
    "    print(f\"Found {len(all_case_dirs)} cases.\")\n",
    "\n",
    "    for case_dir in tqdm(all_case_dirs):\n",
    "        case_id = case_dir.name\n",
    "        output_case_path = output_path / case_id  # Define output path for each case\n",
    "\n",
    "        # Check if the case has already been processed (e.g., output directory or file exists)\n",
    "        if output_case_path.exists():\n",
    "            print(f\"✅ Skipping {case_id}, already processed.\")\n",
    "            continue  # Skip processing if the case already exists\n",
    "\n",
    "        try:\n",
    "            preprocess_case_no_seg_mask(case_dir, output_path, case_id)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Skipping {case_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dc58dd7-0257-4954-905d-404f8975dd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 cases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 22/22 [00:00<00:00, 12659.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Skipping MSLS_093, already processed.\n",
      "✅ Skipping MSLS_094, already processed.\n",
      "✅ Skipping MSLS_095, already processed.\n",
      "✅ Skipping MSLS_096, already processed.\n",
      "✅ Skipping MSLS_097, already processed.\n",
      "✅ Skipping MSLS_098, already processed.\n",
      "✅ Skipping MSLS_099, already processed.\n",
      "✅ Skipping MSLS_100, already processed.\n",
      "✅ Skipping MSLS_101, already processed.\n",
      "✅ Skipping MSLS_102, already processed.\n",
      "✅ Skipping MSLS_103, already processed.\n",
      "✅ Skipping MSLS_104, already processed.\n",
      "✅ Skipping MSLS_105, already processed.\n",
      "✅ Skipping MSLS_106, already processed.\n",
      "✅ Skipping MSLS_107, already processed.\n",
      "✅ Skipping MSLS_108, already processed.\n",
      "✅ Skipping MSLS_109, already processed.\n",
      "✅ Skipping MSLS_110, already processed.\n",
      "✅ Skipping MSLS_111, already processed.\n",
      "✅ Skipping MSLS_112, already processed.\n",
      "✅ Skipping MSLS_113, already processed.\n",
      "✅ Skipping MSLS_114, already processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the preprocessing on the test set (no gt mask)\n",
    "RAW_DATA_PATH = \"../data/01-Pre-Processed-Data/test/test\"\n",
    "OUTPUT_PATH = \"../data/02-Tensor-Data/test/test\"\n",
    "\n",
    "run_preprocess_no_seg_mask(RAW_DATA_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2f3830-e80c-47b9-9f3f-3b1268619f14",
   "metadata": {},
   "source": [
    "## Build the Dataset and Dataloaders for the MSLesSeg preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "813cbfe2-346f-48f1-b350-c0ef69e9ae0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Check the initial memory consuption (GPU) of the project\n",
    "import torch\n",
    "\n",
    "# Check initial GPU memory usage\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59da4863-42f3-4d2c-a675-c8cdc1eeeace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MSLesionDataset(Dataset):\n",
    "    def __init__(self, root_dir, include_labels=True, sample_ids=None, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        all_samples = sorted([d for d in self.root_dir.iterdir() if d.is_dir()])\n",
    "        self.sample_dirs = [d for d in all_samples if not sample_ids or d.name in sample_ids]\n",
    "        self.include_labels = include_labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def pad_to_match(self, tensor, reference_shape):\n",
    "        \"\"\"Pads a tensor to match a 4D (C, D, H, W) reference shape.\"\"\"\n",
    "        current_shape = tensor.shape\n",
    "        pad_sizes = []\n",
    "        for i in range(3, 0, -1):  # W, H, D\n",
    "            diff = reference_shape[i] - current_shape[i]\n",
    "            if diff < 0:\n",
    "                raise ValueError(f\"Tensor dimension {i} is larger than reference: {current_shape[i]} > {reference_shape[i]}\")\n",
    "            pad_sizes.extend([0, diff])\n",
    "        return F.pad(tensor, pad_sizes, mode='constant', value=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            case_dir = self.sample_dirs[idx]\n",
    "    \n",
    "            # === Load FLAIR, T1, T2 ===\n",
    "            input_tensor = torch.load(case_dir / \"input_tensor.pt\").float()  # shape: [3, D, H, W]\n",
    "            if input_tensor.dim() == 3:\n",
    "                input_tensor = input_tensor.unsqueeze(0)\n",
    "            reference_shape = input_tensor.shape  # (C, D, H, W)\n",
    "    \n",
    "            # === Load distance map ===\n",
    "            distance_map = torch.load(case_dir / \"distance_map.pt\").float()\n",
    "            if distance_map.dim() == 3:\n",
    "                distance_map = distance_map.unsqueeze(0)\n",
    "            distance_map = self.pad_to_match(distance_map, reference_shape)\n",
    "    \n",
    "            # === Load multi-scale masks ===\n",
    "            multi_size_masks = []\n",
    "            for i in range(3):\n",
    "                path = case_dir / f\"multi_size_mask_{i}.pt\"\n",
    "                mask = torch.load(path).float() if path.exists() else torch.zeros_like(distance_map)\n",
    "                if mask.dim() == 3:\n",
    "                    mask = mask.unsqueeze(0)\n",
    "                multi_size_masks.append(self.pad_to_match(mask, reference_shape))\n",
    "            multi_masks_cat = torch.cat(multi_size_masks, dim=0)  # [3, D, H, W]\n",
    "    \n",
    "            # === Final input: [7, D, H, W] ===\n",
    "            final_input = torch.cat([input_tensor, distance_map, multi_masks_cat], dim=0)\n",
    "    \n",
    "            if self.transform:\n",
    "                final_input = self.transform(final_input)\n",
    "    \n",
    "            # === Optional target: segmentation mask ===\n",
    "            if self.include_labels:\n",
    "                seg_mask_path = case_dir / \"seg_mask.pt\"\n",
    "                seg_mask = torch.load(seg_mask_path).float()\n",
    "                if seg_mask.dim() == 3:\n",
    "                    seg_mask = seg_mask.unsqueeze(0)\n",
    "                seg_mask = self.pad_to_match(seg_mask, reference_shape)\n",
    "                return final_input, seg_mask\n",
    "    \n",
    "            return final_input, str(case_dir.name)\n",
    "        except Exception as e:\n",
    "            print(f\"Dataset raised an exception:\\t{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02dcd821-2267-490c-a3cd-afa761eac4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03106a16-273a-4298-b82f-e15990258754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_dataloader(data_root, batch_size=2, num_workers=4, shuffle=True, include_labels=True, sample_ids=None, transform=None):\n",
    "    \"\"\"\n",
    "    Returns a PyTorch DataLoader for the MSLesionDataset.\n",
    "\n",
    "    Args:\n",
    "        data_root (str or Path): Root path containing sample subdirectories.\n",
    "        batch_size (int): Batch size.\n",
    "        num_workers (int): Number of worker threads.\n",
    "        shuffle (bool): Whether to shuffle the dataset.\n",
    "        include_labels (bool): If True, return segmentation masks (for training).\n",
    "        sample_ids (list of str): Optional subset of sample names.\n",
    "        transform (callable): Optional transform to apply to the input tensor.\n",
    "    \"\"\"\n",
    "    dataset = MSLesionDataset(\n",
    "        root_dir=data_root,\n",
    "        include_labels=include_labels,\n",
    "        sample_ids=sample_ids,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5e4b760-d9c4-4bc1-b231-b8f08f48331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of some (non training) parameters\n",
    "NUM_WORKERS_TRAIN = 0\n",
    "NUM_WORKERS_VAL = 0\n",
    "NUM_WORKERS_TEST = 0\n",
    "\n",
    "BATCH_SIZE_TRAIN = 2\n",
    "BATCH_SIZE_VAL = 2\n",
    "BATCH_SIZE_TEST = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26a1d71b-abca-4933-8ec7-81302f763067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a49cd1c-7775-40f0-91ec-aae0a176be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# === Config ===\n",
    "TRAIN_TENSOR_DATA_PATH = Path(\"../data/02-Tensor-Data/train\")\n",
    "BATCH_SIZE_TRAIN = 2\n",
    "BATCH_SIZE_VAL = 2\n",
    "NUM_WORKERS_TRAIN = 0\n",
    "NUM_WORKERS_VAL = 0\n",
    "\n",
    "# === Get all valid sample folders ===\n",
    "all_cases = sorted([d for d in os.listdir(TRAIN_TENSOR_DATA_PATH) if os.path.isdir(TRAIN_TENSOR_DATA_PATH / d)])\n",
    "\n",
    "# === Split for 80-20 train/val ===\n",
    "random.seed(42)\n",
    "random.shuffle(all_cases)\n",
    "split = int(0.8 * len(all_cases))\n",
    "train_ids = all_cases[:split]\n",
    "val_ids = all_cases[split:]\n",
    "\n",
    "# === Loaders ===\n",
    "train_loader = get_dataloader(\n",
    "    data_root=TRAIN_TENSOR_DATA_PATH,\n",
    "    sample_ids=train_ids,\n",
    "    batch_size=BATCH_SIZE_TRAIN,\n",
    "    num_workers=NUM_WORKERS_TRAIN,\n",
    "    shuffle=True,\n",
    "    include_labels=True\n",
    ")\n",
    "\n",
    "val_loader = get_dataloader(\n",
    "    data_root=TRAIN_TENSOR_DATA_PATH,\n",
    "    sample_ids=val_ids,\n",
    "    batch_size=BATCH_SIZE_VAL,\n",
    "    num_workers=NUM_WORKERS_VAL,\n",
    "    shuffle=False,\n",
    "    include_labels=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9d350be-96b0-4456-bfe9-2176cdc52570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ca4c7fb-ef72-48f3-9583-19503337a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TENSOR_DATA_PATH_NOMASK = Path(\"../data/02-Tensor-Data/test/test\")\n",
    "TEST_TENSOR_DATA_PATH_MASK = Path(\"../data/02-Tensor-Data/test/test_MASK\")\n",
    "\n",
    "# Test set WITHOUT labels (for inference or prediction)\n",
    "test_loader_no_labels = get_dataloader(\n",
    "    data_root=TEST_TENSOR_DATA_PATH_NOMASK,\n",
    "    include_labels=False,\n",
    "    batch_size=BATCH_SIZE_TEST,\n",
    "    num_workers=NUM_WORKERS_TEST,\n",
    "    shuffle=False  # No shuffling for inference\n",
    ")\n",
    "\n",
    "# Test set WITH labels (for final evaluation or performance metrics)\n",
    "test_loader_with_labels = get_dataloader(\n",
    "    data_root=TEST_TENSOR_DATA_PATH_MASK,\n",
    "    include_labels=True,\n",
    "    batch_size=BATCH_SIZE_TEST,\n",
    "    num_workers=NUM_WORKERS_TEST,\n",
    "    shuffle=False  # No shuffling for evaluation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a3370c6-c8fa-4871-9093-f5e13a8d9002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "def get_k_fold_loaders(data_root, k=5, batch_size=BATCH_SIZE_TRAIN, num_workers=NUM_WORKERS_TRAIN, include_labels=True):\n",
    "    # Get all case IDs\n",
    "    all_cases = sorted(os.listdir(data_root))\n",
    "    \n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_loaders = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(all_cases):\n",
    "        train_ids = [all_cases[i] for i in train_idx]\n",
    "        val_ids = [all_cases[i] for i in val_idx]\n",
    "\n",
    "        # Updated to pass `include_labels`\n",
    "        train_loader = get_dataloader(data_root, sample_ids=train_ids, batch_size=batch_size, num_workers=num_workers, include_labels=include_labels)\n",
    "        val_loader = get_dataloader(data_root, sample_ids=val_ids, batch_size=batch_size, num_workers=num_workers, include_labels=include_labels, shuffle=False)\n",
    "\n",
    "        fold_loaders.append((train_loader, val_loader))\n",
    "\n",
    "    return fold_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a1847cf-501d-4233-8600-08376ed05e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get K-Fold DataLoaders\n",
    "fold_loaders = get_k_fold_loaders(TRAIN_TENSOR_DATA_PATH, k=5, batch_size=2, num_workers=0)\n",
    "\n",
    "# Access the first fold\n",
    "train_loader, val_loader = fold_loaders[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f0311-9b80-4bf0-b80d-996692a83815",
   "metadata": {},
   "source": [
    "### PyTorch Lightning DataModule\n",
    "\n",
    "This particular version of PyTorch Lightning, and in general from version 2.x onward require a ***LightningDataModule*** instead of passing the dataloaders directly to the ***.fit()*** method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac9f0867-1234-4a5d-b1e1-bd8802d56a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningDataModule\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class MSLesionSegmentationDataModule(LightningDataModule):\n",
    "    def __init__(self, data_root, train_split=0.8, batch_size=2, num_workers=4, include_labels=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_root: Path to the root directory containing the dataset.\n",
    "            train_split: Fraction of data to use for training (default 80%).\n",
    "            batch_size: Batch size for the dataloader.\n",
    "            num_workers: Number of workers for the dataloader.\n",
    "            include_labels: Whether to include segmentation masks during training/validation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_root = data_root\n",
    "        self.train_split = train_split\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.include_labels = include_labels\n",
    "\n",
    "        self.train_dataloader_instance = None\n",
    "        self.val_dataloader_instance = None\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_dataloader_instance\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_dataloader_instance\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.val_dataloader_instance  # Optional, if you want to test on the validation set as well.\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"Prepare any datasets if needed, such as downloading or preprocessing.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"Assign train/val datasets for use in dataloaders.\"\"\"\n",
    "        # List all sample directories\n",
    "        all_samples = sorted([d for d in Path(self.data_root).iterdir() if d.is_dir()])\n",
    "\n",
    "        # Shuffle the sample list before splitting\n",
    "        random.seed(42)  # To ensure reproducibility\n",
    "        random.shuffle(all_samples)\n",
    "\n",
    "        # Split the dataset into training and validation sets\n",
    "        split_idx = int(len(all_samples) * self.train_split)\n",
    "        train_samples = all_samples[:split_idx]\n",
    "        val_samples = all_samples[split_idx:]\n",
    "\n",
    "        # Create the train and val dataloaders, passing the `include_labels` parameter\n",
    "        self.train_dataloader_instance = get_dataloader(\n",
    "            self.data_root,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            sample_ids=[d.name for d in train_samples],\n",
    "            include_labels=self.include_labels,  # Pass this argument\n",
    "        )\n",
    "\n",
    "        self.val_dataloader_instance = get_dataloader(\n",
    "            self.data_root,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            sample_ids=[d.name for d in val_samples],\n",
    "            include_labels=self.include_labels,  # Pass this argument\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e8bcef4-4bd3-429a-9722-5ac78eeea124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b08f0-2188-400b-b30e-88997983f659",
   "metadata": {},
   "source": [
    "## The Model's Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "726efd51-8791-4abc-9204-b5571ed1a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture of Experts with dynamic input size support via GAP.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_experts=4, expert_dim=512):\n",
    "        super(MoE, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.gate = nn.Linear(input_dim, num_experts)\n",
    "        self.experts = nn.ModuleList([nn.Linear(input_dim, expert_dim) for _ in range(num_experts)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is expected to be (B, C), already pooled\n",
    "        gate_weights = F.softmax(self.gate(x), dim=-1)\n",
    "        expert_outputs = [expert(x) for expert in self.experts]\n",
    "        output = sum(gate_weights[:, i].unsqueeze(-1) * expert_outputs[i] for i in range(self.num_experts))\n",
    "        return output\n",
    "\n",
    "class UNetMoE(nn.Module):\n",
    "    def __init__(self, in_channels=7, out_channels=1, num_experts=4, expert_dim=512):\n",
    "        super(UNetMoE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.enc_conv1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.pool = nn.MaxPool3d(2)\n",
    "\n",
    "        # Middle layers\n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Decoder with upsampling\n",
    "        self.up = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec_conv = nn.Sequential(\n",
    "            nn.Conv3d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(32, out_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        # Mixture of Experts (lazy init)\n",
    "        self.moe = None\n",
    "        self.num_experts = num_experts\n",
    "        self.expert_dim = expert_dim\n",
    "\n",
    "        # Deep supervision\n",
    "        self.deep_supervision = nn.Conv3d(128, out_channels, kernel_size=1)\n",
    "\n",
    "        # Upsample deep supervision output to match the final output size\n",
    "        self.upsample_deep_supervision = nn.ConvTranspose3d(out_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc = self.enc_conv1(x)   # [B, 64, D, H, W]\n",
    "        pooled = self.pool(enc)   # [B, 64, D/2, H/2, W/2]\n",
    "\n",
    "        # Middle\n",
    "        mid = self.middle(pooled) # [B, 128, D/2, H/2, W/2]\n",
    "\n",
    "        # Decoder\n",
    "        upsampled = self.up(mid)  # [B, 64, D, H, W]\n",
    "        dec = self.dec_conv(upsampled)  # [B, 1, D, H, W]\n",
    "\n",
    "        # Global Average Pooling before MoE\n",
    "        gap = F.adaptive_avg_pool3d(upsampled, 1).view(x.size(0), -1)\n",
    "\n",
    "        # Initialize MoE lazily\n",
    "        if self.moe is None:\n",
    "            self.moe = MoE(input_dim=gap.size(1), num_experts=self.num_experts, expert_dim=self.expert_dim).to(x.device)\n",
    "\n",
    "        moe_out = self.moe(gap)\n",
    "\n",
    "        # Deep supervision\n",
    "        deep_supervision_out = self.deep_supervision(mid)  # [B, 1, D/2, H/2, W/2]\n",
    "\n",
    "        # Upsample deep supervision output to match final output size (128x128x128)\n",
    "        deep_supervision_out_upsampled = self.upsample_deep_supervision(deep_supervision_out)  # [B, 1, D, H, W]\n",
    "\n",
    "        return dec, deep_supervision_out_upsampled, moe_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60727e14-353b-4fd3-9e3e-938472763e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "406d7a31-ebcf-4781-aefe-9a2ff343c4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: torch.Size([2, 1, 128, 128, 128])\n",
      "Deep supervision output shape: torch.Size([2, 1, 128, 128, 128])\n",
      "MoE output shape: torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "# Example usage for testing (TO REMOVE)\n",
    "model = UNetMoE(in_channels=7, out_channels=1)  # 7 modalities as input\n",
    "dummy_input = torch.randn(2, 7, 128, 128, 128)  # batch size 2, 7 input channels\n",
    "dec_out, deep_out, moe_out = model(dummy_input)\n",
    "\n",
    "print(\"Decoder output shape:\", dec_out.shape)             # Expected: [2, 1, H, W, D]\n",
    "print(\"Deep supervision output shape:\", deep_out.shape)   # Expected: [2, 1, H, W, D]\n",
    "print(\"MoE output shape:\", moe_out.shape)                 # Expected: [2, expert_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80d7e20b-62c4-493a-b457-2fcea6133015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa3c2123-75a5-45fa-ab0b-6887c61831ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1    [-1, 32, 128, 128, 128]           6,080\n",
      "              ReLU-2    [-1, 32, 128, 128, 128]               0\n",
      "            Conv3d-3    [-1, 64, 128, 128, 128]          55,360\n",
      "              ReLU-4    [-1, 64, 128, 128, 128]               0\n",
      "         MaxPool3d-5       [-1, 64, 64, 64, 64]               0\n",
      "            Conv3d-6      [-1, 128, 64, 64, 64]         221,312\n",
      "              ReLU-7      [-1, 128, 64, 64, 64]               0\n",
      "            Conv3d-8      [-1, 128, 64, 64, 64]         442,496\n",
      "              ReLU-9      [-1, 128, 64, 64, 64]               0\n",
      "  ConvTranspose3d-10    [-1, 64, 128, 128, 128]          65,600\n",
      "           Conv3d-11    [-1, 64, 128, 128, 128]         110,656\n",
      "             ReLU-12    [-1, 64, 128, 128, 128]               0\n",
      "           Conv3d-13    [-1, 32, 128, 128, 128]          55,328\n",
      "             ReLU-14    [-1, 32, 128, 128, 128]               0\n",
      "           Conv3d-15     [-1, 1, 128, 128, 128]              33\n",
      "           Conv3d-16        [-1, 1, 64, 64, 64]             129\n",
      "  ConvTranspose3d-17     [-1, 1, 128, 128, 128]               9\n",
      "================================================================\n",
      "Total params: 957,003\n",
      "Trainable params: 957,003\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 56.00\n",
      "Forward/backward pass size (MB): 8354.00\n",
      "Params size (MB): 3.65\n",
      "Estimated Total Size (MB): 8413.65\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# The Summary of the Architecture\n",
    "from torchsummary import summary\n",
    "\n",
    "# Set the device and the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNetMoE(in_channels=7, out_channels=1, num_experts=4, expert_dim=512).to(device)\n",
    "\n",
    "# Get the summary of the model (for 3D input: channels, depth, height, width)\n",
    "summary(model, (7, 128, 128, 128))  # 7 input channels now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "686f871e-cfea-440a-afd9-c950b441427c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 12.29 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ff77208-1cc9-4c68-8aa7-eaf9a3def39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the graphical visualization\n",
    "from torchviz import make_dot\n",
    "import gc\n",
    "\n",
    "# Dummy input with correct number of channels (7), batch size = 1\n",
    "dummy_input = torch.randn(1, 7, 128, 128, 128).cuda()\n",
    "\n",
    "# Explicitly instruct the model to not save the gradients to avoid computation graph creation\n",
    "with torch.no_grad():\n",
    "    # Forward pass through the model\n",
    "    output, _, _ = model(dummy_input)\n",
    "\n",
    "    # Visualize the graph and save as PNG\n",
    "    make_dot(output, params=dict(model.named_parameters())).render(\"model_architecture\", format=\"png\")\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Clear non-essential GPU memory\n",
    "del dummy_input\n",
    "del output\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87ecec9a-397d-492f-a6a6-0762cd22dfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 12.29 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bbba010-2e2d-4aa1-9c7f-86d110084e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the GPU memory used for the dummy input and re-initialize the model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNetMoE(in_channels=7, out_channels=1, num_experts=4, expert_dim=512).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52a73d93-c9ae-41ba-9bb8-b297067a69cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 11.78 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dcd022-a711-4931-b46c-336f8565d581",
   "metadata": {},
   "source": [
    "## Visualized Model Architecture\n",
    "\n",
    "![Model Architecture Plot](./model_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be907f2a-2600-407f-8649-42965a1fb3d1",
   "metadata": {},
   "source": [
    "## The Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b2d3471-ee79-41ff-b176-929864412593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.segmentation import DiceScore\n",
    "\n",
    "class MSLesionSegmentationModel(pl.LightningModule):\n",
    "    def __init__(self, model, lr=1e-4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "        # DiceScore for binary segmentation\n",
    "        self.dice_metric = DiceScore(num_classes=2, average='micro')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch  # x: [B, 7, D, H, W], y: [B, 1, D, H, W]\n",
    "\n",
    "        output, deep_output, _ = self(x)\n",
    "\n",
    "        loss_main = F.binary_cross_entropy_with_logits(output, y)\n",
    "        loss_deep = F.binary_cross_entropy_with_logits(deep_output, y)\n",
    "        loss = loss_main + 0.4 * loss_deep\n",
    "\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output, _, _ = self(x)\n",
    "\n",
    "        preds = torch.sigmoid(output)\n",
    "        dice = self.dice_metric(preds, y.int())\n",
    "\n",
    "        self.log('val_dice', dice, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return dice\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output, _, _ = self(x)\n",
    "\n",
    "        preds = torch.sigmoid(output)\n",
    "        dice = self.dice_metric(preds, y.int())\n",
    "\n",
    "        self.log('test_dice', dice, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return dice\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_dice'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b884f-771e-4852-be17-23973479e5a5",
   "metadata": {},
   "source": [
    "## The Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "330d4d97-5326-4dcf-b27e-2f3a57de10f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdrnnrw00m10c351s\u001b[0m (\u001b[33mfpv-perceivelab-unict\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "# Register a free account with Weights and Biases, and create a new project in order to obtain an API Key for the training\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the env variable\n",
    "api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "# Login to wandb\n",
    "if api_key:\n",
    "    os.environ[\"WANDB_API_KEY\"] = api_key\n",
    "    wandb.login()\n",
    "else:\n",
    "    print(\"❌ WANDB_API_KEY not found in .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd78050f-09dd-4251-a8b1-468756f2f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Weights and Biases logger\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project='MSLesSeg-4-ICPR',     # Change to your actual project name\n",
    "    name='nnUNet_MoE_run1', # A specific run name\n",
    "    log_model=True          # Optional: log model checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2cc45e04-32b8-4824-a118-01950a2d1976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the multiprocessing start to 'spawn' instead of 'fork' due to CUDA issues with the Dataloader\n",
    "import multiprocessing\n",
    "import torch\n",
    "\n",
    "# Set the start method for multiprocessing\n",
    "multiprocessing.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43d7e0a3-b3d2-4688-bf74-66b0b8054b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 11.78 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f7969d2-7bdf-4b36-bd8e-db0e5413527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the profiler to keep track of GPU overhead\n",
    "from pytorch_lightning.profilers import PyTorchProfiler\n",
    "\n",
    "# Slightly more sophisticated profiler\n",
    "profiler = PyTorchProfiler(\n",
    "    on_trace_ready=lambda prof: print(prof.key_averages().table(\n",
    "        sort_by=\"self_cuda_memory_usage\", row_limit=15  # Change as needed\n",
    "    )),\n",
    "    profile_memory=True,\n",
    "    record_shapes=True,\n",
    "    with_stack=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60716b23-a05b-4ca7-8c5c-611575ea6a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 11.78 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "318e0b5b-a55c-4dd2-8aa0-a2c43dab9569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 957003\n",
      "Estimated model size: 3.65 MB\n"
     ]
    }
   ],
   "source": [
    "# A computation to estimate the GPU memory consumption of the model\n",
    "\n",
    "# Print the total number of parameters in your model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "\n",
    "# Calculate the approximate memory size of the model (in MB)\n",
    "model_size_MB = total_params * 4 / (1024 ** 2)  # 4 bytes per parameter for float32\n",
    "print(f\"Estimated model size: {model_size_MB:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e89832cf-b12f-4996-92f5-4078eeb524a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17ec947e-62a9-4280-9b98-76de9eb29f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Run garbage collection to clean up unused objects\n",
    "gc.collect()\n",
    "\n",
    "# Empty the PyTorch cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c82afa2-56b6-4202-8aba-365d4206b4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drew/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250506_004230-hjvk1nu0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR/runs/hjvk1nu0' target=\"_blank\">nnUNet_MoE_run1</a></strong> to <a href='https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR' target=\"_blank\">https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR/runs/hjvk1nu0' target=\"_blank\">https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR/runs/hjvk1nu0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drew/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /mnt/HDD-Drive/01-Large-GitHub-Projects/MSLesSeg-4-ICPR/notebooks/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type      | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | model       | UNetMoE   | 957 K  | train\n",
      "1 | dice_metric | DiceScore | 0      | train\n",
      "--------------------------------------------------\n",
      "957 K     Trainable params\n",
      "0         Non-trainable params\n",
      "957 K     Total params\n",
      "3.828     Total estimated model params size (MB)\n",
      "22        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f971ce6179496c80577f3f645653db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drew/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/drew/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/drew/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd91287c7c9942bc9bd5a5af7a891508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a59da5b1bf4213b0da58846a5d2a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46e57d7ade04a81bc52156ac99de5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f10aef0acc4facb88318f058ccac74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d7ed9ed8924f3a9d050798671f277b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578f39d40d624f87a7de8c0004289680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225415001e3e4a0087bd5a5fc1d70cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36b5f59321a4099851e7bd21c0f7be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97388995a0b48d1b732454a50c837e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19291b72026f455da032a55538045d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6748f40db6bd455083a7d129ceac305c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c9538ff0b042c5aa602e846abb12cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0dda6b440c845bf994c36c1afc6eb07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca76483c9d3a4d25a8650c2e933d4dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8873393aef95477d9a5519d2fefc4d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6091fdbdfdfe498a84d788d35e5d632f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81556df99f9a4f81aa7e5b0e49547c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12342ba6d94c4d56bc4dea2ae5b15476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b57e5c53b4f48c9a9aa7c3fbc8dc898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954a3a6a76c84052b8b8142065d96626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb69b94666cc443faee7de8f463c4480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609c4967d14c41c39f5721704583f4c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da343ae8afd41f48b133d36f38896b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4913cd14264b7295f22bea5a4ef604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f3f92385714181923e5fb2e391ecea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b3d3967b154ac4a092fb3cbc027de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa3dfd908a24490a0ff9a8bf9280ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "\n",
    "# Initialize the model with 7 input channels\n",
    "model = MSLesionSegmentationModel(\n",
    "    model=UNetMoE(in_channels=7, out_channels=1, num_experts=4, expert_dim=512)\n",
    ")\n",
    "\n",
    "# Set up checkpointing to save best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_dice',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='best_checkpoint',\n",
    "    save_top_k=1,\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_dice',\n",
    "    patience=10,\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "# Path to training data\n",
    "data_root = TRAIN_TENSOR_DATA_PATH  # Replace with actual path if needed\n",
    "\n",
    "# Initialize the data module\n",
    "data_module = MSLesionSegmentationDataModule(\n",
    "    data_root=data_root,\n",
    "    train_split=0.8,\n",
    "    batch_size=1,\n",
    "    num_workers=0,  # Can be tuned depending on your CPU\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",       # Or \"auto\" if unsure\n",
    "    devices=1,\n",
    "    precision=16,\n",
    "    max_epochs=100,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    logger=wandb_logger,\n",
    "    # profiler=\"simple\"  # Optional, if performance profiling is needed\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee0a5d-61d4-4561-a731-b24483798f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "trainer.test(model, test_dataloader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6833331b-df4e-485b-8b26-e617ad583746",
   "metadata": {},
   "source": [
    "## Visualizing the Learned Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2febf704-6408-4a85-8d9c-19b88747f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to extract features from the model\n",
    "def extract_features(model, dataloader, device='cuda'):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, target = data['image'].to(device), data['mask'].to(device)  # Replace 'image' and 'mask' keys as per your dataset\n",
    "            \n",
    "            # Forward pass through the model\n",
    "            output = model(inputs)  # This is where we extract the feature; adjust according to your model's architecture\n",
    "            \n",
    "            # Extract features from MoE (or any other layer you want to visualize)\n",
    "            # Here I assume output is the final feature map after the MoE layer\n",
    "            feature_map = output.view(output.size(0), -1)  # Flatten the features to (batch_size, features)\n",
    "            features.append(feature_map.cpu().numpy())\n",
    "            labels.append(target.cpu().numpy())  # Add the target (or ground truth) labels for color coding in t-SNE\n",
    "\n",
    "    features = np.concatenate(features, axis=0)  # Combine all the feature maps\n",
    "    labels = np.concatenate(labels, axis=0)  # Combine all the labels\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "# Function to apply t-SNE on features\n",
    "def plot_tsne(features, labels):\n",
    "    # Standardize the features (optional but recommended for t-SNE)\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(features)\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=labels, cmap='jet', alpha=0.5)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('t-SNE Visualization of Learned Representations')\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage\n",
    "train_dataloader = # Your train DataLoader here\n",
    "\n",
    "# Extract features and labels from the model\n",
    "features, labels = extract_features(model, train_dataloader, device='cuda')\n",
    "\n",
    "# Plot t-SNE visualization\n",
    "plot_tsne(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4089cdf9-90ee-4700-8ec5-2c246a69d848",
   "metadata": {},
   "source": [
    "## Post Hoc Model Explainability - GradCAM++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee0469-f55a-4c46-9c72-a7ba80ad41b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from captum.attr import LayerGradCam, LayerGradCamPlusPlus\n",
    "from captum.attr import GuidedBackprop\n",
    "\n",
    "# Function to visualize GradCAM++ output\n",
    "def visualize_gradcam_plus_plus(model, input_tensor, target_class=None, layer_name='decoder', device='cuda'):\n",
    "    # Move input tensor to the appropriate device\n",
    "    input_tensor = input_tensor.to(device)\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Create GradCAM++ object for the specified layer\n",
    "    gradcam_pp = LayerGradCamPlusPlus(model, model.decoder[2])  # Assuming 'decoder' is your final layer, adjust accordingly.\n",
    "    \n",
    "    # Apply GradCAM++ to input tensor\n",
    "    attributions = gradcam_pp.attribute(input_tensor, target=target_class)\n",
    "    \n",
    "    # Convert attributions to numpy\n",
    "    attributions = attributions.cpu().detach().numpy()\n",
    "    \n",
    "    # Normalize the heatmap\n",
    "    heatmap = np.sum(attributions[0], axis=0)  # Summing over channels for a single-channel heatmap\n",
    "    heatmap = np.maximum(heatmap, 0)  # ReLU to ignore negative values\n",
    "    heatmap = cv2.resize(heatmap, (input_tensor.shape[2], input_tensor.shape[3]))  # Resize to input size\n",
    "    heatmap = cv2.normalize(heatmap, None, 0, 1, cv2.NORM_MINMAX)  # Normalize the heatmap to [0, 1]\n",
    "    \n",
    "    # Convert the original image to numpy and rescale\n",
    "    input_image = input_tensor[0].cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    input_image = cv2.resize(input_image, (input_tensor.shape[2], input_tensor.shape[3]))  # Resize to input size\n",
    "    \n",
    "    # Create a colormap for the heatmap\n",
    "    colormap = plt.get_cmap('jet')\n",
    "    colored_heatmap = colormap(heatmap)  # Apply colormap\n",
    "    \n",
    "    # Overlay the heatmap on top of the original image\n",
    "    superimposed_img = np.uint8(input_image * 255)  # Convert original image back to [0, 255] range\n",
    "    superimposed_img = cv2.addWeighted(superimposed_img, 0.7, np.uint8(colored_heatmap[:, :, :3] * 255), 0.3, 0)\n",
    "    \n",
    "    # Plot the result\n",
    "    plt.imshow(superimposed_img)\n",
    "    plt.title('GradCAM++ Heatmap')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage (assuming `model` is your trained model and `input_tensor` is an input image)\n",
    "input_tensor = torch.randn(1, 3, 128, 128).to(device)  # Example input, use your actual input tensor\n",
    "target_class = None  # You can provide a target class or leave as None for model's top predicted class\n",
    "\n",
    "visualize_gradcam_plus_plus(model, input_tensor, target_class=target_class, layer_name='decoder', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb9090-3260-49ce-be05-a03c2d122e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size of the stored (pre-processed) Tensors\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def inspect_pt_file(file_path):\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(f\"File does not exist: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    data = torch.load(file_path, map_location='cpu')\n",
    "\n",
    "    print(f\"\\nLoaded file: {file_path}\")\n",
    "    \n",
    "    if isinstance(data, torch.Tensor):\n",
    "        print(f\"Single Tensor - Shape: {data.shape}\")\n",
    "    \n",
    "    elif isinstance(data, dict):\n",
    "        print(\"Dictionary of tensors:\")\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                print(f\"  {key}: shape = {value.shape}\")\n",
    "            else:\n",
    "                print(f\"  {key}: type = {type(value)}\")\n",
    "    \n",
    "    elif isinstance(data, (list, tuple)):\n",
    "        print(f\"{type(data).__name__} of tensors:\")\n",
    "        for idx, item in enumerate(data):\n",
    "            if isinstance(item, torch.Tensor):\n",
    "                print(f\"  [{idx}]: shape = {item.shape}\")\n",
    "            else:\n",
    "                print(f\"  [{idx}]: type = {type(item)}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Unknown type loaded: {type(data)}\")\n",
    "        print(f\"Unkown type shape: {data.shape}\")\n",
    "\n",
    "INPUT_PATH_PREFIX = \"../data/02-Tensor-Data/train/MSLS_000/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af84775-fb63-4bfe-9fda-a1b238a7a20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Tensor Shape (Stacked Modalities)\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"input_tensor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753172b8-22b4-4290-8104-f0edfad737ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance Map Shape\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"distance_map.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0192a99-65c0-491a-97d2-d0b3fc847c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Size Mask - 0\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"multi_size_mask_0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f505b915-04d4-437b-bcf7-e0760cac75fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Size Mask - 1\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"multi_size_mask_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fe78ce-84b5-4936-a774-88c2f457b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Size Mask - 2\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"multi_size_mask_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feeedd4-a2d9-4e3d-900d-d3ca5a68d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation Mask\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"seg_mask.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858f58bf-be14-45b3-9f56-44f6d8719a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15b66f1a-394d-4b5b-ab06-ea2774295e3c",
   "metadata": {},
   "source": [
    "# U-Net 4 Multiple Sclerosis Lesion Segmentation - ICPR Challenge\n",
    "### Authors: Andrew R. Darnall, Giovanni Spadaro @ UniCT\n",
    "---\n",
    "\n",
    "## üéØ Competition Objective: MS Lesion Segmentation\n",
    "\n",
    "The central goal of this competition is the **automatic segmentation of Multiple Sclerosis (MS) lesions** using **multi-modal MRI data** and **deep learning algorithms**.\n",
    "\n",
    "### üß™ Provided Data\n",
    "Participants were given:\n",
    "- **MRI scans** in three modalities:\n",
    "  - **FLAIR**\n",
    "  - **T1-weighted (T1-w)**\n",
    "  - **T2-weighted (T2-w)**\n",
    "- **Ground-truth segmentation masks**, which are:\n",
    "  - **Binary masks**:  \n",
    "    - **White pixels** ‚Üí MS lesion regions  \n",
    "    - **Black pixels** ‚Üí Background\n",
    "\n",
    "### üß† Task Description\n",
    "- Participants could use **any or all modalities**, along with the ground-truth labels, to:\n",
    "  - Develop **deep learning-based models** for **automatic lesion segmentation**\n",
    "- MS lesions appear as **irregular clusters of pixels** with **high variability in size and shape**\n",
    "- These lesions are often **difficult to detect** via visual inspection, requiring **expert-level interpretation**\n",
    "\n",
    "The ultimate goal is to create **fully automated segmentation pipelines** that can robustly identify and delineate MS lesions from raw MRI data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919db79e-a3bd-4896-89a1-1d9e0017be10",
   "metadata": {},
   "source": [
    "## üß† MSLesSeg Dataset Overview\n",
    "\n",
    "As part of this competition, participants were provided with the **MSLesSeg Dataset** ‚Äî a **comprehensively annotated, multi-modal MRI dataset** designed for advancing **lesion segmentation** research in medical imaging.\n",
    "\n",
    "### üìä Dataset Composition\n",
    "- **Total Patients:** 75 (48 women, 27 men)  \n",
    "- **Age Range:** 18‚Äì59 years (Mean: 37 ¬± 10.3 years)  \n",
    "- **Longitudinal Timepoints:**  \n",
    "  - 50 patients with 1 timepoint  \n",
    "  - 15 patients with 2 timepoints  \n",
    "  - 5 patients with 3 timepoints  \n",
    "  - 5 patients with 4 timepoints  \n",
    "- **Time Interval Between Scans:** ~1.27 ¬± 0.62 years  \n",
    "- **Total MRI Series:** 115\n",
    "\n",
    "### üß¨ Imaging Modalities\n",
    "Each timepoint includes **three core MRI modalities**:\n",
    "- **T1-weighted (T1-w)**\n",
    "- **T2-weighted (T2-w)**\n",
    "- **FLAIR (Fluid-Attenuated Inversion Recovery)**\n",
    "\n",
    "### üßë‚Äç‚öïÔ∏è Expert Annotation\n",
    "- Lesions were **manually annotated** by clinical experts.\n",
    "- **FLAIR sequences** were the primary reference for lesion labeling.\n",
    "- **T1-w and T2-w** scans supported **multi-contrast lesion characterization**.\n",
    "\n",
    "### üß™ Dataset Splits\n",
    "- **Training Set:** 53 scans  \n",
    "- **Test Set:** 22 scans  \n",
    "\n",
    "### ‚úÖ Ethical Compliance\n",
    "- **Ethical approval** was obtained from the corresponding Hospital Ethics Committee.\n",
    "- **Informed consent** was acquired from all participating patients.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b3a0a-2f43-44f9-a898-28f1fa46b75a",
   "metadata": {},
   "source": [
    "# The Experiment\n",
    "\n",
    "Below is the code used for the:\n",
    "\n",
    "1) Preprocessing of the ***Brain MRI*** scans\n",
    "2) Definition of Dataset, Dataloader and LihgtningDataModule classes\n",
    "3) ***U-Net*** architecture\n",
    "4) ***PyTorch Lightning*** Trainer\n",
    "5) Training & Evaluation\n",
    "6) Model Exaplainability with the post-hoc method ***GradCam++***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13613113-a191-4399-9b42-9f6df49bc6e9",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Preprocessing & Annotation Workflow\n",
    "\n",
    "The MSLesSeg dataset underwent a **comprehensive preprocessing pipeline** and **expert-driven manual annotation** to ensure **standardization** and **label quality** for downstream MS lesion segmentation tasks.\n",
    "\n",
    "### üßº Preprocessing Pipeline\n",
    "1. **Anonymization** of all MRI scans to protect patient privacy.\n",
    "2. **DICOM to NIfTI conversion**, leveraging NIfTI's wide adoption in neuroimaging.\n",
    "3. **Co-registration to the MNI152 1mm¬≥ isotropic template** using **FLIRT** (FMRIB‚Äôs Linear Image Registration Tool), ensuring all scans are aligned to a **common anatomical space**.\n",
    "4. **Brain extraction** via **BET** (Brain Extraction Tool) to remove non-brain tissues and isolate relevant structures.\n",
    "\n",
    "This pipeline guarantees that all images are **standardized** and **aligned**, which is critical for **automated MS lesion segmentation algorithms**.\n",
    "\n",
    "---\n",
    "\n",
    "### üñãÔ∏è Ground-Truth Annotation Protocol\n",
    "- Lesions were **manually segmented** on the **FLAIR modality** for each patient and timepoint.\n",
    "- **T1-w and T2-w** modalities were used to **cross-validate ambiguous cases**.\n",
    "- Annotation was conducted by a **trained junior rater**, under supervision of:\n",
    "  - A **senior neuroradiologist**\n",
    "  - A **senior neurologist**\n",
    "- Annotation sessions included:\n",
    "  - Multiple **training meetings** to establish a **consistent segmentation strategy**\n",
    "  - Use of **JIM9** ‚Äî a high-end tool for **medical image segmentation and analysis**\n",
    "  - Regular **expert validation checkpoints** to ensure consistency and accuracy\n",
    "\n",
    "The final masks, reviewed and approved by senior experts, are considered the **gold-standard ground truth**.\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ Key Annotation Highlights\n",
    "- **Independent segmentation** for each patient/timepoint to avoid bias\n",
    "- Conducted on **FLAIR scans registered to MNI space**\n",
    "- **Validated ground-truth masks** ready for training and evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293bc032-fddb-44a7-8db3-cd8bdf0315e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from skimage.transform import resize\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2027badd-35e2-42ac-b423-e85e9e99d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load the nifti files\n",
    "def load_nifti(file_path):\n",
    "    return nib.load(file_path).get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3515529-ff9f-4f47-935b-e09a357f22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_case(input_dir, output_dir, case_id):\n",
    "    flair = load_nifti(input_dir / f\"{case_id}_flair.nii.gz\")\n",
    "    t1 = load_nifti(input_dir / f\"{case_id}_t1.nii.gz\")\n",
    "    t2 = load_nifti(input_dir / f\"{case_id}_t2.nii.gz\")\n",
    "    seg = load_nifti(input_dir / f\"{case_id}_seg.nii.gz\").astype(np.uint8)\n",
    "\n",
    "    # Stack input modalities into a tensor (C, D, H, W)\n",
    "    stacked = np.stack([flair, t1, t2], axis=0)\n",
    "    # Use CPU Tensors instead\n",
    "    input_tensor = torch.tensor(stacked, dtype=torch.float32)\n",
    "    \n",
    "    # Load it back into a PyTorch Tensor for storing\n",
    "    seg_tensor = torch.tensor(seg, dtype=torch.uint8)\n",
    "    # Add the batch dimension in order to make it compatible with the other Tensor sizes\n",
    "    seg_tensor = seg_tensor.unsqueeze(0)\n",
    "\n",
    "    \n",
    "    # Save all\n",
    "    output_case_dir = output_dir / case_id\n",
    "    output_case_dir.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(input_tensor, output_case_dir / \"input_tensor.pt\")\n",
    "    torch.save(seg_tensor, output_case_dir / \"seg_mask.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac6e5ec-ad39-42e9-8a62-c1140398e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_preprocessing(root_path, output_path):\n",
    "    input_path = Path(root_path)\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    # Get all case directories\n",
    "    all_case_dirs = [d for d in input_path.iterdir() if d.is_dir()]\n",
    "    print(f\"Found {len(all_case_dirs)} cases.\")\n",
    "\n",
    "    for case_dir in tqdm(all_case_dirs):\n",
    "        case_id = case_dir.name  # e.g., MSLS_000\n",
    "        output_case_path = output_path / case_id  # Define output path for each case\n",
    "\n",
    "        # Check if the case has already been processed (e.g., output directory or file exists)\n",
    "        if output_case_path.exists():\n",
    "            print(f\"‚úÖ Skipping {case_id}, already processed.\")\n",
    "            continue  # Skip processing if the case already exists\n",
    "\n",
    "        try:\n",
    "            preprocess_case(case_dir, output_path, case_id)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed on {case_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e6a0eb7-7da6-400d-9446-97773e286c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 93 cases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:00<00:00, 2630.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Skipping MSLS_019, already processed.\n",
      "‚úÖ Skipping MSLS_039, already processed.\n",
      "‚úÖ Skipping MSLS_059, already processed.\n",
      "‚úÖ Skipping MSLS_000, already processed.\n",
      "‚úÖ Skipping MSLS_001, already processed.\n",
      "‚úÖ Skipping MSLS_002, already processed.\n",
      "‚úÖ Skipping MSLS_003, already processed.\n",
      "‚úÖ Skipping MSLS_004, already processed.\n",
      "‚úÖ Skipping MSLS_005, already processed.\n",
      "‚úÖ Skipping MSLS_006, already processed.\n",
      "‚úÖ Skipping MSLS_007, already processed.\n",
      "‚úÖ Skipping MSLS_008, already processed.\n",
      "‚úÖ Skipping MSLS_009, already processed.\n",
      "‚úÖ Skipping MSLS_010, already processed.\n",
      "‚úÖ Skipping MSLS_011, already processed.\n",
      "‚úÖ Skipping MSLS_012, already processed.\n",
      "‚úÖ Skipping MSLS_013, already processed.\n",
      "‚úÖ Skipping MSLS_014, already processed.\n",
      "‚úÖ Skipping MSLS_015, already processed.\n",
      "‚úÖ Skipping MSLS_016, already processed.\n",
      "‚úÖ Skipping MSLS_017, already processed.\n",
      "‚úÖ Skipping MSLS_018, already processed.\n",
      "‚úÖ Skipping MSLS_020, already processed.\n",
      "‚úÖ Skipping MSLS_021, already processed.\n",
      "‚úÖ Skipping MSLS_022, already processed.\n",
      "‚úÖ Skipping MSLS_023, already processed.\n",
      "‚úÖ Skipping MSLS_024, already processed.\n",
      "‚úÖ Skipping MSLS_025, already processed.\n",
      "‚úÖ Skipping MSLS_026, already processed.\n",
      "‚úÖ Skipping MSLS_027, already processed.\n",
      "‚úÖ Skipping MSLS_028, already processed.\n",
      "‚úÖ Skipping MSLS_029, already processed.\n",
      "‚úÖ Skipping MSLS_030, already processed.\n",
      "‚úÖ Skipping MSLS_031, already processed.\n",
      "‚úÖ Skipping MSLS_032, already processed.\n",
      "‚úÖ Skipping MSLS_033, already processed.\n",
      "‚úÖ Skipping MSLS_034, already processed.\n",
      "‚úÖ Skipping MSLS_035, already processed.\n",
      "‚úÖ Skipping MSLS_036, already processed.\n",
      "‚úÖ Skipping MSLS_037, already processed.\n",
      "‚úÖ Skipping MSLS_038, already processed.\n",
      "‚úÖ Skipping MSLS_040, already processed.\n",
      "‚úÖ Skipping MSLS_041, already processed.\n",
      "‚úÖ Skipping MSLS_042, already processed.\n",
      "‚úÖ Skipping MSLS_043, already processed.\n",
      "‚úÖ Skipping MSLS_044, already processed.\n",
      "‚úÖ Skipping MSLS_045, already processed.\n",
      "‚úÖ Skipping MSLS_046, already processed.\n",
      "‚úÖ Skipping MSLS_047, already processed.\n",
      "‚úÖ Skipping MSLS_048, already processed.\n",
      "‚úÖ Skipping MSLS_049, already processed.\n",
      "‚úÖ Skipping MSLS_050, already processed.\n",
      "‚úÖ Skipping MSLS_051, already processed.\n",
      "‚úÖ Skipping MSLS_052, already processed.\n",
      "‚úÖ Skipping MSLS_053, already processed.\n",
      "‚úÖ Skipping MSLS_054, already processed.\n",
      "‚úÖ Skipping MSLS_055, already processed.\n",
      "‚úÖ Skipping MSLS_056, already processed.\n",
      "‚úÖ Skipping MSLS_057, already processed.\n",
      "‚úÖ Skipping MSLS_058, already processed.\n",
      "‚úÖ Skipping MSLS_060, already processed.\n",
      "‚úÖ Skipping MSLS_061, already processed.\n",
      "‚úÖ Skipping MSLS_062, already processed.\n",
      "‚úÖ Skipping MSLS_063, already processed.\n",
      "‚úÖ Skipping MSLS_064, already processed.\n",
      "‚úÖ Skipping MSLS_065, already processed.\n",
      "‚úÖ Skipping MSLS_066, already processed.\n",
      "‚úÖ Skipping MSLS_067, already processed.\n",
      "‚úÖ Skipping MSLS_068, already processed.\n",
      "‚úÖ Skipping MSLS_069, already processed.\n",
      "‚úÖ Skipping MSLS_070, already processed.\n",
      "‚úÖ Skipping MSLS_071, already processed.\n",
      "‚úÖ Skipping MSLS_072, already processed.\n",
      "‚úÖ Skipping MSLS_073, already processed.\n",
      "‚úÖ Skipping MSLS_074, already processed.\n",
      "‚úÖ Skipping MSLS_075, already processed.\n",
      "‚úÖ Skipping MSLS_076, already processed.\n",
      "‚úÖ Skipping MSLS_077, already processed.\n",
      "‚úÖ Skipping MSLS_078, already processed.\n",
      "‚úÖ Skipping MSLS_079, already processed.\n",
      "‚úÖ Skipping MSLS_080, already processed.\n",
      "‚úÖ Skipping MSLS_081, already processed.\n",
      "‚úÖ Skipping MSLS_082, already processed.\n",
      "‚úÖ Skipping MSLS_083, already processed.\n",
      "‚úÖ Skipping MSLS_084, already processed.\n",
      "‚úÖ Skipping MSLS_085, already processed.\n",
      "‚úÖ Skipping MSLS_086, already processed.\n",
      "‚úÖ Skipping MSLS_087, already processed.\n",
      "‚úÖ Skipping MSLS_088, already processed.\n",
      "‚úÖ Skipping MSLS_089, already processed.\n",
      "‚úÖ Skipping MSLS_090, already processed.\n",
      "‚úÖ Skipping MSLS_091, already processed.\n",
      "‚úÖ Skipping MSLS_092, already processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the preprocessing on the training set\n",
    "RAW_DATA_PATH = \"../data/01-Pre-Processed-Data/train\"\n",
    "OUTPUT_PATH = \"../data/02-Tensor-Data/train\"\n",
    "\n",
    "run_preprocessing(RAW_DATA_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc06f78a-1ed2-4e4b-90f7-391b10748569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 cases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:00<00:00, 899.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Skipping MSLS_093, already processed.\n",
      "‚úÖ Skipping MSLS_094, already processed.\n",
      "‚úÖ Skipping MSLS_095, already processed.\n",
      "‚úÖ Skipping MSLS_096, already processed.\n",
      "‚úÖ Skipping MSLS_097, already processed.\n",
      "‚úÖ Skipping MSLS_098, already processed.\n",
      "‚úÖ Skipping MSLS_099, already processed.\n",
      "‚úÖ Skipping MSLS_100, already processed.\n",
      "‚úÖ Skipping MSLS_101, already processed.\n",
      "‚úÖ Skipping MSLS_102, already processed.\n",
      "‚úÖ Skipping MSLS_103, already processed.\n",
      "‚úÖ Skipping MSLS_104, already processed.\n",
      "‚úÖ Skipping MSLS_105, already processed.\n",
      "‚úÖ Skipping MSLS_106, already processed.\n",
      "‚úÖ Skipping MSLS_107, already processed.\n",
      "‚úÖ Skipping MSLS_108, already processed.\n",
      "‚úÖ Skipping MSLS_109, already processed.\n",
      "‚úÖ Skipping MSLS_110, already processed.\n",
      "‚úÖ Skipping MSLS_111, already processed.\n",
      "‚úÖ Skipping MSLS_112, already processed.\n",
      "‚úÖ Skipping MSLS_113, already processed.\n",
      "‚úÖ Skipping MSLS_114, already processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the preprocessing on the test set\n",
    "RAW_DATA_PATH = \"../data/01-Pre-Processed-Data/test/test_MASK\"\n",
    "OUTPUT_PATH = \"../data/02-Tensor-Data/test\"\n",
    "\n",
    "run_preprocessing(RAW_DATA_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2f3830-e80c-47b9-9f3f-3b1268619f14",
   "metadata": {},
   "source": [
    "## Build the Dataset and Dataloaders for the MSLesSeg preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "813cbfe2-346f-48f1-b350-c0ef69e9ae0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Check the initial memory consuption (GPU) of the project\n",
    "import torch\n",
    "\n",
    "# Check initial GPU memory usage\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59da4863-42f3-4d2c-a675-c8cdc1eeeace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSLesSeg Tensor Dataset class\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MSLesSegDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.patient_dirs = self._get_patient_dirs()\n",
    "\n",
    "    def _get_patient_dirs(self):\n",
    "        \"\"\"\n",
    "        Helper function to search for all patient directories within train/test directories.\n",
    "        \"\"\"\n",
    "        patient_dirs = []\n",
    "        \n",
    "        # Get all patient directories within the root_dir (either train or test)\n",
    "        for patient_dir in os.listdir(self.root_dir):\n",
    "            patient_path = os.path.join(self.root_dir, patient_dir)\n",
    "            \n",
    "            # Make sure it's a directory\n",
    "            if os.path.isdir(patient_path):\n",
    "                # Check if both 'input_tensor.pt' and 'seg_mask.pt' exist\n",
    "                input_tensor_path = os.path.join(patient_path, 'input_tensor.pt')\n",
    "                seg_mask_path = os.path.join(patient_path, 'seg_mask.pt')\n",
    "                \n",
    "                if os.path.exists(input_tensor_path) and os.path.exists(seg_mask_path):\n",
    "                    patient_dirs.append(patient_path)\n",
    "        \n",
    "        return patient_dirs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "        \n",
    "        Returns:\n",
    "            dict: {'input': tensor, 'mask': tensor} for the requested sample.\n",
    "        \"\"\"\n",
    "        patient_dir = self.patient_dirs[idx]\n",
    "        \n",
    "        # Load the input tensor and segmentation mask\n",
    "        input_tensor = torch.load(os.path.join(patient_dir, 'input_tensor.pt'))  # Shape: [3, 182, 218, 182]\n",
    "        seg_mask = torch.load(os.path.join(patient_dir, 'seg_mask.pt'))  # Shape: [1, 182, 218, 182]\n",
    "        \n",
    "        return input_tensor, seg_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f0311-9b80-4bf0-b80d-996692a83815",
   "metadata": {},
   "source": [
    "### PyTorch Lightning DataModule\n",
    "\n",
    "This particular version of PyTorch Lightning, and in general from version 2.x onward require a ***LightningDataModule*** instead of passing the dataloaders directly to the ***.fit()*** method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03106a16-273a-4298-b82f-e15990258754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSLesSeg (PyTorch) LightningDataModule definition\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class MSLesSegDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, root_data_dir, batch_size, val_split, num_workers):\n",
    "        \"\"\"\n",
    "        root_data_dir: Is the path to the train and test data\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_dir = root_data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.val_split = val_split\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Load full training dataset\n",
    "        full_dataset = MSLesSegDataset(root_dir=os.path.join(self.data_dir, 'train'))\n",
    "\n",
    "        # Split into train and val\n",
    "        val_size = int(len(full_dataset) * self.val_split)\n",
    "        train_size = len(full_dataset) - val_size\n",
    "        self.train_dataset, self.val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "        # Load test dataset (if it exists)\n",
    "        test_dir = os.path.join(self.data_dir, 'test')\n",
    "        if os.path.exists(test_dir):\n",
    "            self.test_dataset = MSLesSegDataset(root_dir=test_dir)\n",
    "        else:\n",
    "            self.test_dataset = None\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if self.test_dataset:\n",
    "            return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b08f0-2188-400b-b30e-88997983f659",
   "metadata": {},
   "source": [
    "## The Model's Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "303c976d-7038-4798-91fb-99a94c810caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The U-Net architecture is based on the tutorial offered by the author of the architecture\n",
    "# Link --> https://github.com/bnsreenu/python_for_image_processing_APEER/blob/master/tutorial122_3D_Unet.ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu1 = nn.GELU()\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu2 = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x\n",
    "\n",
    "# Downsampling path Conv block followed by maxpooling.\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down, self).__init__()\n",
    "        self.conv_block = ConvBlock(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_output = self.conv_block(x)\n",
    "        pool_output = self.pool(conv_output)\n",
    "        return conv_output, pool_output\n",
    "\n",
    "# Upsampling path: Skip features gets input from encoder for concatenation\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Up, self).__init__()\n",
    "        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv_block = ConvBlock(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, skip_features):\n",
    "        x = self.conv_transpose(x)\n",
    "        x = torch.cat((x, skip_features), dim=1)\n",
    "        x = self.conv_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48ea20e2-8ad7-4693-ab48-3d442038dedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to pad to a multiple of 16 (because of the 4 down and up convolutions)\n",
    "def pad_to_multiple(x, multiple=16):\n",
    "    _, _, h, w, d = x.shape\n",
    "    pad_h = (multiple - h % multiple) % multiple\n",
    "    pad_w = (multiple - w % multiple) % multiple\n",
    "    pad_d = (multiple - d % multiple) % multiple\n",
    "\n",
    "    pad = [0, pad_d, 0, pad_w, 0, pad_h]  # D, W, H\n",
    "    return F.pad(x, pad, mode='constant', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbdc3277-c204-4867-baea-7abf416655df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center cropping helper function to obtain the desired shape for the prediction with the segmentation mask\n",
    "def center_crop(x, target_shape):\n",
    "    _, h, w, d = x.shape\n",
    "    th, tw, td = target_shape\n",
    "    h1 = (h - th) // 2\n",
    "    w1 = (w - tw) // 2\n",
    "    d1 = (d - td) // 2\n",
    "    return x[:, h1:h1+th, w1:w1+tw, d1:d1+td]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c67520ee-53db-45f6-a184-3ff3761bb80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D U-Net Architecture\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    The U-Net model will accept input Tensors of shape: [B, C, H, W, D]\n",
    "    Which based on the used MSLesSeg dataset will be [B, 3, 182, 218, 182]\n",
    "    The input Tensor is nothing more than the stacked [FLAIR, T1w, T2w] modalities\n",
    "\n",
    "    The output will be the segmentation mask of shape [B, 1, 182, 218, 182]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        self.down_1 = Down(in_channels, 64)\n",
    "        self.down_2 = Down(64, 128)\n",
    "        self.down_3 = Down(128, 256)\n",
    "        self.down_4 = Down(256, 512)\n",
    "\n",
    "        self.bottleneck = ConvBlock(512, 1024)\n",
    "\n",
    "        self.up_1 = Up(1024, 512)\n",
    "        self.up_2 = Up(512, 256)\n",
    "        self.up_3 = Up(256, 128)\n",
    "        self.up_4 = Up(128, 64)\n",
    "\n",
    "        self.classifier = nn.Conv3d(64, 1, kernel_size=1, padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Pad to multiple\n",
    "        x = pad_to_multiple(x)\n",
    "        \n",
    "        # Downsampling path\n",
    "        conv1, pool1 = self.down_1(x)\n",
    "        conv2, pool2 = self.down_2(pool1)\n",
    "        conv3, pool3 = self.down_3(pool2)\n",
    "        conv4, pool4 = self.down_4(pool3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(pool4)\n",
    "\n",
    "        # Upsampling path\n",
    "        upconv1 = self.up_1(bottleneck, conv4)\n",
    "        upconv2 = self.up_2(upconv1, conv3)\n",
    "        upconv3 = self.up_3(upconv2, conv2)\n",
    "        upconv4 = self.up_4(upconv3, conv1)\n",
    "\n",
    "        output = self.sigmoid(self.classifier(upconv4))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fd1805d-2f4e-4057-9d61-2ac0755cbe69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "UNet                                     [1, 1, 192, 224, 192]     --\n",
       "‚îú‚îÄDown: 1-1                              [1, 64, 192, 224, 192]    --\n",
       "‚îÇ    ‚îî‚îÄConvBlock: 2-1                    [1, 64, 192, 224, 192]    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-1                  [1, 64, 192, 224, 192]    5,248\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-2             [1, 64, 192, 224, 192]    128\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-3                    [1, 64, 192, 224, 192]    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-4                  [1, 64, 192, 224, 192]    110,656\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-5             [1, 64, 192, 224, 192]    128\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-6                    [1, 64, 192, 224, 192]    --\n",
       "‚îÇ    ‚îî‚îÄMaxPool3d: 2-2                    [1, 64, 96, 112, 96]      --\n",
       "‚îú‚îÄDown: 1-2                              [1, 128, 96, 112, 96]     --\n",
       "‚îÇ    ‚îî‚îÄConvBlock: 2-3                    [1, 128, 96, 112, 96]     --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-7                  [1, 128, 96, 112, 96]     221,312\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-8             [1, 128, 96, 112, 96]     256\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-9                    [1, 128, 96, 112, 96]     --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-10                 [1, 128, 96, 112, 96]     442,496\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-11            [1, 128, 96, 112, 96]     256\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-12                   [1, 128, 96, 112, 96]     --\n",
       "‚îÇ    ‚îî‚îÄMaxPool3d: 2-4                    [1, 128, 48, 56, 48]      --\n",
       "‚îú‚îÄDown: 1-3                              [1, 256, 48, 56, 48]      --\n",
       "‚îÇ    ‚îî‚îÄConvBlock: 2-5                    [1, 256, 48, 56, 48]      --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-13                 [1, 256, 48, 56, 48]      884,992\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-14            [1, 256, 48, 56, 48]      512\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-15                   [1, 256, 48, 56, 48]      --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-16                 [1, 256, 48, 56, 48]      1,769,728\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-17            [1, 256, 48, 56, 48]      512\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-18                   [1, 256, 48, 56, 48]      --\n",
       "‚îÇ    ‚îî‚îÄMaxPool3d: 2-6                    [1, 256, 24, 28, 24]      --\n",
       "‚îú‚îÄDown: 1-4                              [1, 512, 24, 28, 24]      --\n",
       "‚îÇ    ‚îî‚îÄConvBlock: 2-7                    [1, 512, 24, 28, 24]      --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-19                 [1, 512, 24, 28, 24]      3,539,456\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-20            [1, 512, 24, 28, 24]      1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-21                   [1, 512, 24, 28, 24]      --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-22                 [1, 512, 24, 28, 24]      7,078,400\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-23            [1, 512, 24, 28, 24]      1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-24                   [1, 512, 24, 28, 24]      --\n",
       "‚îÇ    ‚îî‚îÄMaxPool3d: 2-8                    [1, 512, 12, 14, 12]      --\n",
       "‚îú‚îÄConvBlock: 1-5                         [1, 1024, 12, 14, 12]     --\n",
       "‚îÇ    ‚îî‚îÄConv3d: 2-9                       [1, 1024, 12, 14, 12]     14,156,800\n",
       "‚îÇ    ‚îî‚îÄBatchNorm3d: 2-10                 [1, 1024, 12, 14, 12]     2,048\n",
       "‚îÇ    ‚îî‚îÄGELU: 2-11                        [1, 1024, 12, 14, 12]     --\n",
       "‚îÇ    ‚îî‚îÄConv3d: 2-12                      [1, 1024, 12, 14, 12]     28,312,576\n",
       "‚îÇ    ‚îî‚îÄBatchNorm3d: 2-13                 [1, 1024, 12, 14, 12]     2,048\n",
       "‚îÇ    ‚îî‚îÄGELU: 2-14                        [1, 1024, 12, 14, 12]     --\n",
       "‚îú‚îÄUp: 1-6                                [1, 512, 24, 28, 24]      --\n",
       "‚îÇ    ‚îî‚îÄConvTranspose3d: 2-15             [1, 512, 24, 28, 24]      4,194,816\n",
       "‚îÇ    ‚îî‚îÄConvBlock: 2-16                   [1, 512, 24, 28, 24]      --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-25                 [1, 512, 24, 28, 24]      14,156,288\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-26            [1, 512, 24, 28, 24]      1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-27                   [1, 512, 24, 28, 24]      --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-28                 [1, 512, 24, 28, 24]      7,078,400\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-29            [1, 512, 24, 28, 24]      1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-30                   [1, 512, 24, 28, 24]      --\n",
       "‚îú‚îÄUp: 1-7                                [1, 256, 48, 56, 48]      --\n",
       "‚îÇ    ‚îî‚îÄConvTranspose3d: 2-17             [1, 256, 48, 56, 48]      1,048,832\n",
       "‚îÇ    ‚îî‚îÄConvBlock: 2-18                   [1, 256, 48, 56, 48]      --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-31                 [1, 256, 48, 56, 48]      3,539,200\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-32            [1, 256, 48, 56, 48]      512\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-33                   [1, 256, 48, 56, 48]      --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-34                 [1, 256, 48, 56, 48]      1,769,728\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-35            [1, 256, 48, 56, 48]      512\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-36                   [1, 256, 48, 56, 48]      --\n",
       "‚îú‚îÄUp: 1-8                                [1, 128, 96, 112, 96]     --\n",
       "‚îÇ    ‚îî‚îÄConvTranspose3d: 2-19             [1, 128, 96, 112, 96]     262,272\n",
       "‚îÇ    ‚îî‚îÄConvBlock: 2-20                   [1, 128, 96, 112, 96]     --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-37                 [1, 128, 96, 112, 96]     884,864\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-38            [1, 128, 96, 112, 96]     256\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-39                   [1, 128, 96, 112, 96]     --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-40                 [1, 128, 96, 112, 96]     442,496\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-41            [1, 128, 96, 112, 96]     256\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-42                   [1, 128, 96, 112, 96]     --\n",
       "‚îú‚îÄUp: 1-9                                [1, 64, 192, 224, 192]    --\n",
       "‚îÇ    ‚îî‚îÄConvTranspose3d: 2-21             [1, 64, 192, 224, 192]    65,600\n",
       "‚îÇ    ‚îî‚îÄConvBlock: 2-22                   [1, 64, 192, 224, 192]    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-43                 [1, 64, 192, 224, 192]    221,248\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-44            [1, 64, 192, 224, 192]    128\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-45                   [1, 64, 192, 224, 192]    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv3d: 3-46                 [1, 64, 192, 224, 192]    110,656\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm3d: 3-47            [1, 64, 192, 224, 192]    128\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄGELU: 3-48                   [1, 64, 192, 224, 192]    --\n",
       "‚îú‚îÄConv3d: 1-10                           [1, 1, 192, 224, 192]     65\n",
       "‚îú‚îÄSigmoid: 1-11                          [1, 1, 192, 224, 192]     --\n",
       "==========================================================================================\n",
       "Total params: 90,307,905\n",
       "Trainable params: 90,307,905\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.TERABYTES): 8.40\n",
       "==========================================================================================\n",
       "Input size (MB): 86.65\n",
       "Forward/backward pass size (MB): 50668.24\n",
       "Params size (MB): 361.23\n",
       "Estimated Total Size (MB): 51116.12\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the model summary\n",
    "from torchinfo import summary\n",
    "\n",
    "# Assuming your model is already defined (UNet or similar)\n",
    "model = UNet(in_channels=3)\n",
    "\n",
    "# Example input size: (Batch Size, Channels, Height, Width, Depth)\n",
    "input_tensor = torch.randn(1, 3, 182, 218, 182)  # Modify based on your use case\n",
    "\n",
    "# Use summary from torchinfo to display the model summary\n",
    "summary(model, input_data=input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60727e14-353b-4fd3-9e3e-938472763e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint to see where the GPU memory overhead is\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be907f2a-2600-407f-8649-42965a1fb3d1",
   "metadata": {},
   "source": [
    "## The Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec1451e1-9756-49f3-a595-7fda87796588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we implement the DiceLoss\n",
    "def dice_loss_3d(pred, target, smooth=1):\n",
    "    \"\"\"\n",
    "    Computes Dice Loss for 3D segmentation tasks.\n",
    "    Args:\n",
    "    pred: Tensor of predictions (batch_size, C, D, H, W).\n",
    "    target: Tensor of ground truth (batch_size, C, D, H, W).\n",
    "    smooth: Smoothing factor.\n",
    "    Returns:\n",
    "    Scalar Dice Loss.\n",
    "    \"\"\"\n",
    "    pred = F.softmax(pred, dim=1)\n",
    "    num_classes = pred.shape[1]\n",
    "    dice = 0\n",
    "    for c in range(num_classes):\n",
    "        pred_c = pred[:, c]\n",
    "        target_c = target[:, c]\n",
    "        intersection = (pred_c * target_c).sum(dim=(2, 3, 4))\n",
    "        union = pred_c.sum(dim=(2, 3, 4)) + target_c.sum(dim=(2, 3, 4))\n",
    "        dice += (2. * intersection + smooth) / (union + smooth)\n",
    "        \n",
    "    return 1 - dice.mean() / num_classes\n",
    "\n",
    "# Then we implement the Mean Dice\n",
    "def mean_dice(pred, target, smooth=1):\n",
    "    \"\"\"\n",
    "    Computes Dice Loss for 3D segmentation tasks.\n",
    "    Args:\n",
    "    pred: Tensor of predictions (batch_size, C, D, H, W).\n",
    "    target: Tensor of ground truth (batch_size, C, D, H, W).\n",
    "    smooth: Smoothing factor.\n",
    "    Returns:\n",
    "    Scalar Dice Loss.\n",
    "    \"\"\"\n",
    "    pred = F.softmax(pred, dim=1)\n",
    "    num_classes = pred.shape[1]\n",
    "    dice = 0\n",
    "    for c in range(num_classes):\n",
    "        pred_c = pred[:, c]\n",
    "        target_c = target[:, c]\n",
    "        intersection = (pred_c * target_c).sum(dim=(2, 3, 4))\n",
    "        union = pred_c.sum(dim=(2, 3, 4)) + target_c.sum(dim=(2, 3, 4))\n",
    "        dice += (2. * intersection + smooth) / (union + smooth)\n",
    "        \n",
    "    return dice.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6b5560d-a488-4a2f-89bf-b8e4f5a2eb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Dice Score and Dice Loss\n",
    "def dice_coefficient(y_pred, y_true, smooth=1e-5):\n",
    "    y_true_flat = y_true.contiguous().view(y_true.shape[0], -1)\n",
    "    y_pred_flat = y_pred.contiguous().view(y_pred.shape[0], -1)\n",
    "    \n",
    "    intersection = (y_true_flat * y_pred_flat).sum(dim=1)\n",
    "    union = y_true_flat.sum(dim=1) + y_pred_flat.sum(dim=1)\n",
    "    \n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return dice.mean()\n",
    "\n",
    "def dice_coefficient_loss(y_pred, y_true):\n",
    "    return 1 - dice_coefficient(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b2d3471-ee79-41ff-b176-929864412593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning Trainer for the U-Net model\n",
    "import torch.optim as optim\n",
    "\n",
    "class MSLesionSegmentationModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, model, checkpoint_dir, lr=1e-4):\n",
    "        super(MSLesionSegmentationModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Separate the input Tensor from the Segmentation Mask\n",
    "        input_tensor, gt = batch\n",
    "        # Forward Pass on the model\n",
    "        y_pred = self(input_tensor)\n",
    "        # Crop y_pred shape to match the seg mask gt\n",
    "        y_pred = center_crop(y_pred, (182, 218, 182))\n",
    "        # Compute DiceLoss\n",
    "        loss = dice_coefficient_loss(y_pred, gt)\n",
    "\n",
    "        # log the train loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Separate the input Tensor from the Segmentation Mask\n",
    "        input_tensor, gt = batch\n",
    "        # Forward pass on the model\n",
    "        y_pred = self(input_tensor)[0]\n",
    "        # Crop y_pred shape to match the seg mask gt\n",
    "        y_pred = center_crop(y_pred, (182, 218, 182))\n",
    "        # Compute DiceLoss\n",
    "        loss = dice_coefficient_loss(y_pred, gt)\n",
    "        # Compute the Mean Dice Score\n",
    "        dice_score = dice_coefficient(y_pred, gt)\n",
    "        # log the val loss\n",
    "        self.log(\"val_loss\", loss)\n",
    "        # log the Mean Dice Score\n",
    "        self.log(\"dice_score\", dice_score)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Separate the input Tensor from the Segmentation Mask\n",
    "        input_tensor, gt = batch\n",
    "        # Forward pass on the model\n",
    "        y_pred = self(input_tensor)\n",
    "        # Crop y_pred shape to match the seg mask gt\n",
    "        y_pred = center_crop(y_pred, (182, 218, 182))\n",
    "        # Compute DiceLoss\n",
    "        loss = dice_coefficient_loss(y_pred, gt)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "        \n",
    "    def on_train_epoch_end(self):\n",
    "        # Save the model's checkpoint at the end of the epoch\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "        save_path = os.path.join(self.save_dir, \"unet_best_model.pth\")\n",
    "        \n",
    "        # Prepare the checkpoint dict\n",
    "        checkpoint = {\n",
    "            'epoch': self.current_epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,\n",
    "            'learning_rate': self.lr,\n",
    "        }\n",
    "\n",
    "        # Save and flush to disk\n",
    "        with open(save_path, 'wb') as f:\n",
    "            torch.save(checkpoint, f)\n",
    "            f.flush()\n",
    "            os.fsync(f.fileno())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b884f-771e-4852-be17-23973479e5a5",
   "metadata": {},
   "source": [
    "## The Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "330d4d97-5326-4dcf-b27e-2f3a57de10f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdrnnrw00m10c351s\u001b[0m (\u001b[33mfpv-perceivelab-unict\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "# Register a free account with Weights and Biases, and create a new project in order to obtain an API Key for the training\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the env variable\n",
    "api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "# Login to wandb\n",
    "if api_key:\n",
    "    os.environ[\"WANDB_API_KEY\"] = api_key\n",
    "    wandb.login()\n",
    "else:\n",
    "    print(\"‚ùå WANDB_API_KEY not found in .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd78050f-09dd-4251-a8b1-468756f2f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Weights and Biases logger\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project='MSLesSeg-4-ICPR',     # Change to your actual project name\n",
    "    name='UNet_cpu_run_1', # A specific run name\n",
    "    log_model=True          # Optional: log model checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1804c28e-27c2-490d-bf6f-83be924e2674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the constant for the maximum number of epochs for the training\n",
    "MAX_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe02bc36-efab-409d-9ff4-780ebf605e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of Dataloader hyperparameters (Batch size, seed and num workers)\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "TRAIN_NUM_WORKERS = 0\n",
    "TEST_NUM_WORKERS = 0\n",
    "\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "VAL_BATCH_SIZE = 1\n",
    "TEST_BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "094c3a1c-11bf-456a-a77b-8a0eb8698b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/drew/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    }
   ],
   "source": [
    "# Create the Trainer object\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    accelerator=\"gpu\",\n",
    "    precision=32,\n",
    "    logger=wandb_logger,\n",
    "    benchmark=True,  # optimize CUDA kernels for performance\n",
    "    detect_anomaly=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05a9d033-88de-417f-ad1a-1be7b3b3c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data module\n",
    "data_module = MSLesSegDataModule(\n",
    "    root_data_dir=\"../data/02-Tensor-Data/\",\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    val_split=VAL_SPLIT,\n",
    "    num_workers=TRAIN_NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80715b4e-731b-496e-85f4-1f3f31f7c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the PyTorch Lightning Module (wrapper for the PyTorch nn.Module ~ Architecture)\n",
    "lightning_model = MSLesionSegmentationModel(\n",
    "    model=UNet(in_channels=3),\n",
    "    checkpoint_dir=\"./model_checkpoints\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "038c95be-0282-431c-b17f-75932d56e760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Check the initial memory consuption (GPU) of the project\n",
    "import torch\n",
    "\n",
    "# Check initial GPU memory usage\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024 ** 2  # in MB\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "457b2ccd-6a49-4caa-ba6a-4259aedbc5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CUDA) Input tensor size in MB: 82.64 MB\n"
     ]
    }
   ],
   "source": [
    "# Estimated VRAM footprint per CUDA Tensor of shape [B, C, H, W, D]\n",
    "def get_tensor_vram_mb(tensor):\n",
    "    return tensor.element_size() * tensor.nelement() / (1024 ** 2)\n",
    "\n",
    "input_tensor = torch.randn(1, 3, 182, 218, 182).cuda()\n",
    "print(f\"(CUDA) Input tensor size in MB: {get_tensor_vram_mb(input_tensor):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "885cca8b-90e4-420d-8785-36a1c2839436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CUDA) Model parameters size: 344.50 MB\n"
     ]
    }
   ],
   "source": [
    "# Estimated VRAM footprint for the CUDA model\n",
    "def get_model_param_vram_mb(model):\n",
    "    return sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)\n",
    "\n",
    "model = UNet(in_channels=3).cuda()\n",
    "print(f\"(CUDA) Model parameters size: {get_model_param_vram_mb(model):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf24dbb-10f4-4a5a-955d-84f46cf566f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250508_224345-r421kvxu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR/runs/r421kvxu' target=\"_blank\">UNet_cpu_run_1</a></strong> to <a href='https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR' target=\"_blank\">https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR/runs/r421kvxu' target=\"_blank\">https://wandb.ai/fpv-perceivelab-unict/MSLesSeg-4-ICPR/runs/r421kvxu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type | Params | Mode \n",
      "---------------------------------------\n",
      "0 | model | UNet | 90.3 M | train\n",
      "---------------------------------------\n",
      "90.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "90.3 M    Total params\n",
      "361.232   Total estimated model params size (MB)\n",
      "82        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ca7d2848714c09a876e0de629aaad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drew/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/drew/miniconda3/envs/mslesseg4icpr/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5557e01c512a4eedac973e26b45f8773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train (fit) the model\n",
    "trainer.fit(lightning_model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3886c494-2f08-4fb4-868e-ce9e4866b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model's checkpoint and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee0a5d-61d4-4561-a731-b24483798f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "trainer.test(model, test_dataloader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6833331b-df4e-485b-8b26-e617ad583746",
   "metadata": {},
   "source": [
    "## Visualizing the Learned Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4089cdf9-90ee-4700-8ec5-2c246a69d848",
   "metadata": {},
   "source": [
    "## Post Hoc Model Explainability - GradCAM++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb9090-3260-49ce-be05-a03c2d122e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size of the stored (pre-processed) Tensors\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def inspect_pt_file(file_path):\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(f\"File does not exist: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    data = torch.load(file_path, map_location='cpu')\n",
    "\n",
    "    print(f\"\\nLoaded file: {file_path}\")\n",
    "    \n",
    "    if isinstance(data, torch.Tensor):\n",
    "        print(f\"Single Tensor - Shape: {data.shape}\")\n",
    "    \n",
    "    elif isinstance(data, dict):\n",
    "        print(\"Dictionary of tensors:\")\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                print(f\"  {key}: shape = {value.shape}\")\n",
    "            else:\n",
    "                print(f\"  {key}: type = {type(value)}\")\n",
    "    \n",
    "    elif isinstance(data, (list, tuple)):\n",
    "        print(f\"{type(data).__name__} of tensors:\")\n",
    "        for idx, item in enumerate(data):\n",
    "            if isinstance(item, torch.Tensor):\n",
    "                print(f\"  [{idx}]: shape = {item.shape}\")\n",
    "            else:\n",
    "                print(f\"  [{idx}]: type = {type(item)}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Unknown type loaded: {type(data)}\")\n",
    "        print(f\"Unkown type shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45d428c-86fa-4c27-b37b-da49338e2bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH_PREFIX = \"../data/02-Tensor-Data/train/MSLS_000/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af84775-fb63-4bfe-9fda-a1b238a7a20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Tensor Shape (Stacked Modalities)\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"input_tensor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feeedd4-a2d9-4e3d-900d-d3ca5a68d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation Mask\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"seg_mask.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858f58bf-be14-45b3-9f56-44f6d8719a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH_PREFIX = \"../data/02-Tensor-Data/train/MSLS_010/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d59b454-f46d-4220-9263-9da064ed08b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Tensor Shape (Stacked Modalities)\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"input_tensor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc47eb8c-e1ab-45b5-b8fc-e8b29ce136f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation Mask\n",
    "inspect_pt_file(INPUT_PATH_PREFIX + \"seg_mask.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7696a6-4413-405e-b1aa-734afea539de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
